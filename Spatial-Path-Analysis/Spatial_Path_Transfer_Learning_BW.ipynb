{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/Hand-Gesture-Recognition/blob/dev/Spatial-Path-Analysis/Spatial_Path_Transfer_Learning_BW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "tamil-chrome",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tamil-chrome",
        "outputId": "175826c9-d342-4172-c2de-13784839791a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import time\n",
        "import joblib\n",
        "import shutil\n",
        "import tarfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, confusion_matrix\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, GlobalAvgPool2D, Dropout, Flatten, concatenate\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "MfafORnwZkyT",
      "metadata": {
        "id": "MfafORnwZkyT"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "casual-stack",
      "metadata": {
        "id": "casual-stack"
      },
      "outputs": [],
      "source": [
        "# -------- TEST USER ----------- #\n",
        "\n",
        "TEST_USER      = '010'\n",
        "DATASET_ID     = '1p0CSRb9gax0sKqdyzOYVt-BXvZ4GtrBv'\n",
        "\n",
        "# BASE_DIR       = '../Dataset/'\n",
        "\n",
        "# Google Drive\n",
        "BASE_DIR       = '.'\n",
        "DATA_DIR       = 'Sensor-Data/'\n",
        "BW_IMG_DIR     = 'BW-Spatial-Path-Images/'\n",
        "RGB_IMG_DIR    = 'RGB-Spatial-Path-Images/'\n",
        "CHANNELS_DIR   = 'Channels/'\n",
        "IMG_SIZE       = (3, 3) # INCHES\n",
        "\n",
        "IMG_DIR        = 'BW-Spatial-Path-Images/'\n",
        "LOG_DIR        = 'Logs/'\n",
        "\n",
        "USERS          = ['001', '002', '003', '004', '005', '006', '007', '008', '009',\n",
        "                  '010', '011', '012', '013', '014', '015', '016', '017', '018',\n",
        "                  '019', '020', '021', '022', '023', '024', '025']\n",
        "\n",
        "# ------------------------------- Only Dynalic Gestures ------------------------------ #\n",
        "GESTURES       = ['j', 'z', 'bad', 'deaf', 'fine', 'good', 'goodbye', 'hello', 'hungry',\n",
        "                  'me', 'no', 'please', 'sorry', 'thankyou', 'yes', 'you']\n",
        "\n",
        "PLANES         = ['XY', 'YZ', 'ZX']\n",
        "\n",
        "BATCH_SIZE     = 32\n",
        "IMG_LEN        = 224\n",
        "IMG_SIZE       = (IMG_LEN, IMG_LEN)\n",
        "\n",
        "# ------------- FOR THE GREATER GOOD :) ------------- #\n",
        "DATASET_LEN    = 4000\n",
        "TRAIN_LEN      = 3840\n",
        "TEST_LEN       = 160\n",
        "\n",
        "EPOCHS         = 50\n",
        "LEARNING_RATE  = 0.001\n",
        "DECAY          = 0.0\n",
        "\n",
        "DT             = 0.01\n",
        "SHAPES         = 100\n",
        "CUT_OFF        = 3.0\n",
        "ORDER          = 4\n",
        "FS             = 100\n",
        "\n",
        "WINDOW_LEN     = 150\n",
        "\n",
        "CONFIG         = '_L_7_S_160x160_E_7'\n",
        "CHANNELS_GROUP = 'DYNAMIC_ACC_ONLY_'\n",
        "\n",
        "XY_WEIGHTS     = np.array([0.91, 0.75, 0.61, 0.63, 0.51, 0.66, 0.81, 0.65, 0.65, 0.31,\n",
        "                           0.66, 0.29, 0.34, 0.64, 0.64, 0.31])\n",
        "YZ_WEIGHTS     = np.array([0.73, 0.71, 0.70, 0.79, 0.76, 0.38, 0.80, 0.61, 0.58, 0.73,\n",
        "                           0.49, 0.26, 0.26, 0.52, 0.59, 0.54])\n",
        "ZX_WEIGHTS     = np.array([0.33, 0.66, 0.51, 0.54, 0.37, 0.51, 0.71, 0.30, 0.75, 0.41,\n",
        "                           0.40, 0.27, 0.24, 0.61, 0.36, 0.49])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e65322ba",
      "metadata": {
        "id": "e65322ba"
      },
      "outputs": [],
      "source": [
        "class LowPassFilter(object): \n",
        "    def butter_lowpass(cutoff, fs, order):\n",
        "        nyq = 0.5 * fs\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "        return b, a\n",
        "\n",
        "    def apply(data, cutoff=CUT_OFF, fs=FS, order=ORDER):\n",
        "        b, a = LowPassFilter.butter_lowpass(cutoff, fs, order=order)\n",
        "        y = lfilter(b, a, data)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ksdY7MgcqHNr",
      "metadata": {
        "id": "ksdY7MgcqHNr"
      },
      "outputs": [],
      "source": [
        "#--------------------- Download util for Google Drive ------------------- #\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "        \n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "        \n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(fid, destination):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(destination)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "        \n",
        "    print('creating data directory ... ', end='')\n",
        "    os.mkdir(destination)\n",
        "    print('√')\n",
        "    \n",
        "    print('downloading dataset from the repository ... ', end='')\n",
        "    filename = os.path.join(destination, 'dataset.tar.xz')\n",
        "    try:\n",
        "        download_file_from_google_drive(fid, filename)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "        \n",
        "    print('extracting the dataset ... ', end='')\n",
        "    try:\n",
        "        tar = tarfile.open(filename)\n",
        "        tar.extractall(destination)\n",
        "        tar.close()\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "MtE4QJm-qOZY",
      "metadata": {
        "id": "MtE4QJm-qOZY"
      },
      "outputs": [],
      "source": [
        "# # ------- Comment This if already downloaded -------- #\n",
        "\n",
        "# destination = os.path.join(BASE_DIR, DATA_DIR)\n",
        "# download_data(DATASET_ID, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f9d84aa1",
      "metadata": {
        "id": "f9d84aa1"
      },
      "outputs": [],
      "source": [
        "def clean_dir(path):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "    \n",
        "    print('creating ' + path + ' directory ... ', end='')\n",
        "    os.mkdir(path)\n",
        "    print('√')\n",
        "\n",
        "def extract_channels():\n",
        "    channels_dir = os.path.join(BASE_DIR, CHANNELS_DIR)\n",
        "    clean_dir(channels_dir)\n",
        "        \n",
        "    for user in USERS:\n",
        "    # for gesture in GESTURES:\n",
        "        print('Processing data for user ' + user, end=' ')\n",
        "        # print(f\"processing data for gesture {gesture} \", end=\"...\" )\n",
        "        \n",
        "        X = []\n",
        "        y = []\n",
        "        first_time = True\n",
        "        \n",
        "        for gesture in GESTURES:\n",
        "        # for user in USERS:\n",
        "              \n",
        "            user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
        "            gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
        "\n",
        "            dataset = pd.read_csv(gesture_dir)\n",
        "\n",
        "            dataset['flex_1'] = dataset['flex_1'].rolling(3).median()\n",
        "            dataset['flex_2'] = dataset['flex_2'].rolling(3).median()\n",
        "            dataset['flex_3'] = dataset['flex_3'].rolling(3).median()\n",
        "            dataset['flex_4'] = dataset['flex_4'].rolling(3).median()\n",
        "            dataset['flex_5'] = dataset['flex_5'].rolling(3).median()\n",
        "\n",
        "            dataset.fillna(0, inplace=True)\n",
        "\n",
        "            # flex = ['flex_1', 'flex_2', 'flex_3', 'flex_4', 'flex_5']\n",
        "            # max_flex = dataset[flex].max(axis=1)\n",
        "            # max_flex.replace(0, 1, inplace=True)\n",
        "            # dataset[flex] = dataset[flex].divide(max_flex, axis=0)\n",
        "            \n",
        "            flx1 = dataset['flex_1'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx2 = dataset['flex_2'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx3 = dataset['flex_3'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx4 = dataset['flex_4'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx5 = dataset['flex_5'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            accx = dataset['ACCx'].to_numpy()\n",
        "            accy = dataset['ACCy'].to_numpy()\n",
        "            accz = dataset['ACCz'].to_numpy()\n",
        "            \n",
        "            accx = LowPassFilter.apply(accx).reshape(-1, WINDOW_LEN)\n",
        "            accy = LowPassFilter.apply(accy).reshape(-1, WINDOW_LEN)\n",
        "            accz = LowPassFilter.apply(accz).reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            gyrx = dataset['GYRx'].to_numpy()\n",
        "            gyry = dataset['GYRy'].to_numpy()\n",
        "            gyrz = dataset['GYRz'].to_numpy()\n",
        "            \n",
        "            gyrx = LowPassFilter.apply(gyrx).reshape(-1, WINDOW_LEN)\n",
        "            gyry = LowPassFilter.apply(gyry).reshape(-1, WINDOW_LEN)\n",
        "            gyrz = LowPassFilter.apply(gyrz).reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            accm = np.sqrt(accx ** 2 + accy ** 2 + accz ** 2)\n",
        "            gyrm = np.sqrt(gyrx ** 2 + gyry ** 2 + gyrz ** 2)\n",
        "            \n",
        "            g_idx = GESTURES.index(gesture)\n",
        "            labels = np.ones((accx.shape[0], 1)) * g_idx\n",
        "            \n",
        "            channels = np.stack([\n",
        "                flx1, flx2, flx3, flx4, flx5,\n",
        "                accx, accy, accz\n",
        "            ], axis=-1)\n",
        "            \n",
        "            if first_time == True:\n",
        "                X = channels\n",
        "                y = labels\n",
        "                first_time = False\n",
        "            else:\n",
        "                X = np.append(X, channels, axis=0)\n",
        "                y = np.append(y, labels, axis=0)\n",
        "            \n",
        "        \n",
        "        x_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_X.joblib')\n",
        "        y_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_y.joblib')\n",
        "        joblib.dump(X, x_path)\n",
        "        joblib.dump(y, y_path)\n",
        "        \n",
        "        print('√')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "QmReRMF-qQcl",
      "metadata": {
        "id": "QmReRMF-qQcl"
      },
      "outputs": [],
      "source": [
        "# ------------- Spatial Path Image Generation ----------- #\n",
        "\n",
        "def clean_dir(path):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "    \n",
        "    print('creating ' + path + ' directory ... ', end='')\n",
        "    os.mkdir(path)\n",
        "    print('√')\n",
        "    \n",
        "# ----------- Spatial Path Vector Calculation ----------- #\n",
        "\n",
        "def get_displacement(acc):\n",
        "    v = np.zeros(acc.shape)\n",
        "    d = np.zeros(acc.shape)\n",
        "    for i in range(acc.shape[0] - 1):\n",
        "        v[i + 1] = v[i] + acc[i] * DT\n",
        "        d[i + 1] = v[i] * DT + 0.5 * acc[i] * DT * DT\n",
        "        \n",
        "    return d\n",
        "\n",
        "def write_image(x, y, path):\n",
        "    fig, ax = plt.subplots(frameon=True, figsize=(3, 3))\n",
        "    ax.axis('off')\n",
        "    plt.scatter(x, y, s=SHAPES, c='black')\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "def generate_bw_images():\n",
        "    image_dir = os.path.join(BASE_DIR, BW_IMG_DIR)\n",
        "    clean_dir(image_dir)\n",
        "    \n",
        "    for plane in PLANES:\n",
        "        print('processing spatial path images for ' + plane + ' plane ... ', end='')\n",
        "        plane_dir = os.path.join(image_dir, plane)\n",
        "        os.mkdir(plane_dir)\n",
        "        \n",
        "        # for gesture in GESTURES:\n",
        "        #     os.mkdir(os.path.join(plane_dir, gesture))\n",
        "\n",
        "        for user in USERS:\n",
        "            os.mkdir(os.path.join(plane_dir, user))\n",
        "    \n",
        "            # for user in USERS:\n",
        "            for gesture in GESTURES:\n",
        "                os.mkdir(os.path.join(plane_dir, user, gesture))\n",
        "                user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
        "                gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
        "                \n",
        "                accx = pd.read_csv(gesture_dir)['ACCx'].to_numpy()\n",
        "                accy = pd.read_csv(gesture_dir)['ACCy'].to_numpy()\n",
        "                accz = pd.read_csv(gesture_dir)['ACCz'].to_numpy()\n",
        "\n",
        "                x = get_displacement(accx).reshape(-1, 150)\n",
        "                y = get_displacement(accy).reshape(-1, 150)\n",
        "                z = get_displacement(accz).reshape(-1, 150)\n",
        "\n",
        "                count = 0\n",
        "                for i in range(x.shape[0]):\n",
        "                    # image_name = 'u' + user + '_g' + '{:0>2d}'.format(GESTURES.index(gesture)) + \\\n",
        "                    #              '_s' + '{:0>7d}'.format(count) + '_p' + plane + '.jpg'\n",
        "                    image_name = \"{:0>7d}\".format(count) + \".jpg\"\n",
        "                    path = os.path.join(plane_dir, user, gesture, image_name)\n",
        "                    \n",
        "                    if plane == 'XY':\n",
        "                        write_image(x[i, :], y[i, :], path)\n",
        "                    elif plane == 'YZ':\n",
        "                        write_image(y[i, :], z[i, :], path)\n",
        "                    else:\n",
        "                        write_image(z[i, :], x[i, :], path)\n",
        "\n",
        "                    count = count + 1\n",
        "            \n",
        "        print('√')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "07jhA8ukqScv",
      "metadata": {
        "id": "07jhA8ukqScv"
      },
      "outputs": [],
      "source": [
        "# extract_channels()\n",
        "# generate_bw_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "stylish-banner",
      "metadata": {
        "id": "stylish-banner"
      },
      "outputs": [],
      "source": [
        "def load_data(plane, test_user):\n",
        "    X_train = np.zeros((TRAIN_LEN, IMG_LEN, IMG_LEN, 3), dtype='uint8')\n",
        "    X_test = np.zeros((TEST_LEN, IMG_LEN, IMG_LEN, 3), dtype='uint8')\n",
        "    y_train = np.zeros((TRAIN_LEN, 1), dtype='uint8')\n",
        "    y_test = np.zeros((TEST_LEN, 1), dtype='uint8')\n",
        "    \n",
        "    train_count = 0\n",
        "    test_count = 0\n",
        "        \n",
        "    # for gesture in GESTURES:\n",
        "    for user in USERS:\n",
        "        print('loading data for user ' + user + ' on the ' + plane + ' plane ... ', end='')\n",
        "        user_dir = os.path.join(BASE_DIR, IMG_DIR, plane, user)\n",
        "\n",
        "        for gesture in GESTURES:\n",
        "            gesture_dir = os.path.join(user_dir, gesture)\n",
        "\n",
        "            # for filename in os.listdir(path):\n",
        "\n",
        "            for count in range(10):\n",
        "                image_name = \"{:0>7d}\".format(count) + \".jpg\"\n",
        "                img = cv2.imread(os.path.join(gesture_dir, image_name))\n",
        "                resized = cv2.resize(img, IMG_SIZE)\n",
        "                # resized = np.expand_dims(resized, axis=-1)\n",
        "                # label = int(filename[6:8])\n",
        "                if user != test_user:\n",
        "                    X_train[train_count, :] = resized\n",
        "                    y_train[train_count, 0] = GESTURES.index(gesture)\n",
        "                    train_count = train_count + 1\n",
        "                else:\n",
        "                    X_test[test_count, :] = resized\n",
        "                    y_test[test_count, 0] = GESTURES.index(gesture)\n",
        "                    test_count = test_count + 1\n",
        "                \n",
        "        print('√')\n",
        "        \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def load_and_save_data(plane):\n",
        "    X = np.zeros((DATASET_LEN, IMG_LEN, IMG_LEN, 1))\n",
        "    y = np.zeros((DATASET_LEN, 1))\n",
        "    \n",
        "    train_count = 0\n",
        "    test_count  = TRAIN_LEN\n",
        "        \n",
        "    for gesture in GESTURES:\n",
        "        print('loading data for ' + gesture + ' gesture on the ' + plane + ' plane ... ', end='')\n",
        "        path = os.path.join(BASE_DIR, IMG_DIR, plane, gesture)\n",
        "        for filename in os.listdir(path):\n",
        "            img = cv2.imread(os.path.join(path, filename), cv2.IMREAD_GRAYSCALE)\n",
        "            resized = cv2.resize(img, IMG_SIZE)\n",
        "            if filename[1:4] != TEST_USER:\n",
        "                X[train_count, :] = resized\n",
        "                y[train_count, 0] = GESTURES.index(gesture)\n",
        "                train_count = train_count + 1\n",
        "            else:\n",
        "                X[test_count, :] = resized\n",
        "                y[test_count, 0] = GESTURES.index(gesture)\n",
        "                test_count = test_count + 1\n",
        "                \n",
        "        print('√')\n",
        "\n",
        "    joblib.dump(X, BASE_DIR + 'X_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    joblib.dump(y, BASE_DIR + 'Y_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "\n",
        "def load_data_from_joblib(plane):\n",
        "    print('Loading data for ' + plane + ' plane ... ', end='')\n",
        "    X = joblib.load(BASE_DIR + 'X_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    y = joblib.load(BASE_DIR + 'Y_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    test_user = int(TEST_USER)\n",
        "    X_train = X[:TRAIN_LEN, :, :, :]\n",
        "    y_train = y[:TRAIN_LEN, :]\n",
        "    X_test = X[TRAIN_LEN:, :, :, :]\n",
        "    y_test = y[TRAIN_LEN:, :]\n",
        "\n",
        "    print('√')\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "extra-disclosure",
      "metadata": {
        "id": "extra-disclosure"
      },
      "outputs": [],
      "source": [
        "# X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data('XY')\n",
        "# X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data('YZ')\n",
        "# X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data('ZX')\n",
        "\n",
        "# Save to Google  Drive\n",
        "# load_and_save_data('XY')X_train_xy, y_train_xy = shuffle(X_train_xy, y_train_xy)\n",
        "# load_and_save_data('YZ')\n",
        "# load_and_save_data('ZX')\n",
        "\n",
        "# Load from Google Drive\n",
        "# X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data_from_joblib('XY')\n",
        "# X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data_from_joblib('YZ')\n",
        "# X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data_from_joblib('ZX')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "conditional-advisory",
      "metadata": {
        "id": "conditional-advisory"
      },
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "internal-arkansas",
      "metadata": {
        "id": "internal-arkansas",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6261ddc6-348a-4fcf-d2b5-e669472b46f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "\n",
        "# ... Change Model\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "strange-encoding",
      "metadata": {
        "id": "strange-encoding"
      },
      "outputs": [],
      "source": [
        "global_average_layer = GlobalAvgPool2D()\n",
        "prediction_layer = Dense(len(GESTURES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6e049e9a",
      "metadata": {
        "id": "6e049e9a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers.pooling import MaxPooling2D\n",
        "\n",
        "def get_conv_block_1D():\n",
        "    input = Input(shape=(150, 1))\n",
        "    x = BatchNormalization()(input)\n",
        "    x = Conv1D(filters=8, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(50, activation='relu')(x)\n",
        "\n",
        "    return input, x\n",
        "\n",
        "def get_conv_block_2D():\n",
        "    input = tf.keras.layers.Input(shape=IMG_SHAPE)\n",
        "    # x = data_augmentation(input)\n",
        "    x = preprocess_input(input)\n",
        "    x = base_model(x, training=False)\n",
        "    x = global_average_layer(x)\n",
        "\n",
        "    # x = layers.Conv2D(16, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),)(\n",
        "    #     input\n",
        "    # )\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.MaxPooling2D()(x)\n",
        "    # x = layers.Conv2D(32, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),)(\n",
        "    #     x\n",
        "    # )\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.MaxPooling2D()(x)\n",
        "    # x = layers.Conv2D(\n",
        "    #     64, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),\n",
        "    # )(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.Flatten()(x)\n",
        "\n",
        "    return input, x\n",
        "\n",
        "\n",
        "def get_stacked_model():\n",
        "    inputs = []\n",
        "    CNNs = []\n",
        "\n",
        "    for i in range(5):\n",
        "        input_i, CNN_i = get_conv_block_1D()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    for i in range(3):\n",
        "        input_i, CNN_i = get_conv_block_2D()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    x = concatenate(CNNs, axis=-1)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    output = Dense(16, activation=\"softmax\")(x)\n",
        "    model = Model(inputs, output)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
        "    model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "WTWKoIlYyzKW",
      "metadata": {
        "id": "WTWKoIlYyzKW",
        "outputId": "082c8ac0-afff-4468-818b-7e3bdc3cc749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 150, 1)      4           ['input_2[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 150, 1)      4           ['input_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 150, 1)      4           ['input_4[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 150, 1)      4           ['input_5[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 150, 1)      4           ['input_6[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 148, 8)       32          ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 148, 8)       32          ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 148, 8)       32          ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 148, 8)       32          ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 148, 8)       32          ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 146, 16)      400         ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 146, 16)      400         ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 146, 16)      400         ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 146, 16)      400         ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 146, 16)      400         ['conv1d_16[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 73, 16)       0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_13[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 71, 16)       784         ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 71, 16)       784         ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_6[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_8[0][0]']        \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_9 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 69, 16)       784         ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 69, 16)       784         ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 69, 16)       784         ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 69, 16)       784         ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 69, 16)       784         ['conv1d_18[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.truediv (TFOpLambda)   (None, 224, 224, 3)  0           ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.truediv_1 (TFOpLambda)  (None, 224, 224, 3)  0          ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.truediv_2 (TFOpLambda)  (None, 224, 224, 3)  0          ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_11[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_15[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_19[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLambda)  (None, 224, 224, 3)  0           ['tf.math.truediv[0][0]']        \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLambda  (None, 224, 224, 3)  0          ['tf.math.truediv_1[0][0]']      \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.subtract_2 (TFOpLambda  (None, 224, 224, 3)  0          ['tf.math.truediv_2[0][0]']      \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 544)          0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 544)          0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 544)          0           ['max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 544)          0           ['max_pooling1d_7[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 544)          0           ['max_pooling1d_9[0][0]']        \n",
            "                                                                                                  \n",
            " mobilenetv2_1.00_224 (Function  (None, 7, 7, 1280)  2257984     ['tf.math.subtract[0][0]',       \n",
            " al)                                                              'tf.math.subtract_1[0][0]',     \n",
            "                                                                  'tf.math.subtract_2[0][0]']     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 50)           27250       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 50)           27250       ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 50)           27250       ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 50)           27250       ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 50)           27250       ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 1280)        0           ['mobilenetv2_1.00_224[0][0]',   \n",
            " alAveragePooling2D)                                              'mobilenetv2_1.00_224[1][0]',   \n",
            "                                                                  'mobilenetv2_1.00_224[2][0]']   \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 4090)         0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_2[0][0]',                \n",
            "                                                                  'dense_3[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'dense_5[0][0]',                \n",
            "                                                                  'global_average_pooling2d[0][0]'\n",
            "                                                                 , 'global_average_pooling2d[1][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_average_pooling2d[2][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 4090)         0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          523648      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 16)           2064        ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,929,966\n",
            "Trainable params: 671,972\n",
            "Non-trainable params: 2,257,994\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_stacked_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "upper-tobago",
      "metadata": {
        "id": "upper-tobago"
      },
      "outputs": [],
      "source": [
        "# def get_model():\n",
        "#     inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
        "#     #     x = data_augmentation(inputs)\n",
        "#     x = preprocess_input(inputs)\n",
        "#     x = base_model(x, training=False)\n",
        "#     x = global_average_layer(x)\n",
        "#     x = tf.keras.layers.Dropout(0.2)(x)\n",
        "#     outputs = prediction_layer(x)\n",
        "#     model = tf.keras.Model(inputs, outputs)\n",
        "#     model.compile(\n",
        "#         optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE, decay=DECAY),\n",
        "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#         metrics=[\"accuracy\"],\n",
        "#     )\n",
        "#     return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b3a457f4",
      "metadata": {
        "id": "b3a457f4",
        "outputId": "ca5cf7f4-63ae-409f-fda0-c4888555e17c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing results for user 001... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 54s 558ms/step - loss: 2.6271 - accuracy: 0.1951 - val_loss: 1.8657 - val_accuracy: 0.5125\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 30s 498ms/step - loss: 1.5981 - accuracy: 0.4961 - val_loss: 0.8882 - val_accuracy: 0.7625\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.9147 - accuracy: 0.7237 - val_loss: 0.4820 - val_accuracy: 0.8687\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.6161 - accuracy: 0.8081 - val_loss: 0.3682 - val_accuracy: 0.8625\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.4563 - accuracy: 0.8552 - val_loss: 0.2549 - val_accuracy: 0.9500\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.3566 - accuracy: 0.8805 - val_loss: 0.1931 - val_accuracy: 0.9625\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.3147 - accuracy: 0.9023 - val_loss: 0.1599 - val_accuracy: 0.9625\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.2663 - accuracy: 0.9128 - val_loss: 0.1657 - val_accuracy: 0.9563\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.2542 - accuracy: 0.9182 - val_loss: 0.1130 - val_accuracy: 0.9812\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.2077 - accuracy: 0.9326 - val_loss: 0.0912 - val_accuracy: 0.9937\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1859 - accuracy: 0.9451 - val_loss: 0.0895 - val_accuracy: 0.9875\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1610 - accuracy: 0.9482 - val_loss: 0.1002 - val_accuracy: 0.9750\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1534 - accuracy: 0.9521 - val_loss: 0.0619 - val_accuracy: 0.9937\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.1417 - accuracy: 0.9536 - val_loss: 0.0801 - val_accuracy: 0.9875\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1307 - accuracy: 0.9578 - val_loss: 0.0570 - val_accuracy: 0.9875\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.1209 - accuracy: 0.9615 - val_loss: 0.0844 - val_accuracy: 0.9812\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1142 - accuracy: 0.9648 - val_loss: 0.0844 - val_accuracy: 0.9688\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1102 - accuracy: 0.9635 - val_loss: 0.0600 - val_accuracy: 0.9937\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0993 - accuracy: 0.9724 - val_loss: 0.0541 - val_accuracy: 0.9875\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0942 - accuracy: 0.9703 - val_loss: 0.0733 - val_accuracy: 0.9875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0874 - accuracy: 0.9732 - val_loss: 0.0585 - val_accuracy: 0.9875\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0834 - accuracy: 0.9755 - val_loss: 0.0650 - val_accuracy: 0.9875\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0822 - accuracy: 0.9747 - val_loss: 0.0541 - val_accuracy: 0.9937\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0711 - accuracy: 0.9781 - val_loss: 0.0555 - val_accuracy: 0.9937\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0741 - accuracy: 0.9753 - val_loss: 0.0550 - val_accuracy: 0.9937\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0702 - accuracy: 0.9779 - val_loss: 0.0480 - val_accuracy: 0.9875\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0608 - accuracy: 0.9828 - val_loss: 0.0645 - val_accuracy: 0.9875\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0636 - accuracy: 0.9826 - val_loss: 0.0543 - val_accuracy: 0.9937\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0576 - accuracy: 0.9836 - val_loss: 0.0503 - val_accuracy: 0.9937\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0577 - accuracy: 0.9823 - val_loss: 0.0443 - val_accuracy: 0.9937\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0530 - accuracy: 0.9833 - val_loss: 0.0424 - val_accuracy: 0.9937\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0482 - accuracy: 0.9857 - val_loss: 0.0510 - val_accuracy: 0.9875\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0504 - accuracy: 0.9867 - val_loss: 0.0414 - val_accuracy: 0.9875\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0477 - accuracy: 0.9870 - val_loss: 0.0543 - val_accuracy: 0.9875\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0498 - accuracy: 0.9854 - val_loss: 0.0375 - val_accuracy: 0.9937\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0447 - accuracy: 0.9859 - val_loss: 0.0341 - val_accuracy: 0.9937\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0394 - accuracy: 0.9896 - val_loss: 0.0424 - val_accuracy: 0.9937\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0441 - accuracy: 0.9862 - val_loss: 0.0359 - val_accuracy: 0.9937\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0400 - accuracy: 0.9878 - val_loss: 0.0348 - val_accuracy: 0.9937\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0398 - accuracy: 0.9878 - val_loss: 0.0370 - val_accuracy: 0.9937\n",
            "3/3 [==============================] - 1s 345ms/step - loss: 0.0912 - accuracy: 0.9937\n",
            "99.37 %\n",
            "Processing results for user 002... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 43s 544ms/step - loss: 2.7053 - accuracy: 0.1734 - val_loss: 2.0586 - val_accuracy: 0.3875\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 1.7509 - accuracy: 0.4594 - val_loss: 1.2236 - val_accuracy: 0.6750\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 1.0489 - accuracy: 0.6794 - val_loss: 0.7476 - val_accuracy: 0.7750\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.7072 - accuracy: 0.7815 - val_loss: 0.4830 - val_accuracy: 0.8813\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.5011 - accuracy: 0.8427 - val_loss: 0.3695 - val_accuracy: 0.8813\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.4049 - accuracy: 0.8758 - val_loss: 0.3564 - val_accuracy: 0.8750\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.3263 - accuracy: 0.9013 - val_loss: 0.3247 - val_accuracy: 0.8875\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 30s 498ms/step - loss: 0.2891 - accuracy: 0.9068 - val_loss: 0.2804 - val_accuracy: 0.9000\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.2416 - accuracy: 0.9263 - val_loss: 0.2533 - val_accuracy: 0.9000\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.2067 - accuracy: 0.9388 - val_loss: 0.2201 - val_accuracy: 0.9000\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.1931 - accuracy: 0.9391 - val_loss: 0.2429 - val_accuracy: 0.9187\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.1577 - accuracy: 0.9505 - val_loss: 0.2175 - val_accuracy: 0.9187\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.1606 - accuracy: 0.9539 - val_loss: 0.2149 - val_accuracy: 0.9187\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.1476 - accuracy: 0.9536 - val_loss: 0.1953 - val_accuracy: 0.9125\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.1347 - accuracy: 0.9565 - val_loss: 0.2041 - val_accuracy: 0.9250\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.1184 - accuracy: 0.9612 - val_loss: 0.1851 - val_accuracy: 0.9312\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.1138 - accuracy: 0.9669 - val_loss: 0.1725 - val_accuracy: 0.9187\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0941 - accuracy: 0.9742 - val_loss: 0.1812 - val_accuracy: 0.9187\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0966 - accuracy: 0.9724 - val_loss: 0.1734 - val_accuracy: 0.9312\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.1012 - accuracy: 0.9680 - val_loss: 0.1467 - val_accuracy: 0.9312\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0850 - accuracy: 0.9724 - val_loss: 0.1621 - val_accuracy: 0.9250\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0817 - accuracy: 0.9771 - val_loss: 0.1620 - val_accuracy: 0.9250\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0793 - accuracy: 0.9740 - val_loss: 0.1647 - val_accuracy: 0.9250\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0709 - accuracy: 0.9802 - val_loss: 0.1579 - val_accuracy: 0.9250\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0738 - accuracy: 0.9781 - val_loss: 0.1510 - val_accuracy: 0.9312\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0692 - accuracy: 0.9781 - val_loss: 0.1366 - val_accuracy: 0.9375\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0661 - accuracy: 0.9786 - val_loss: 0.1576 - val_accuracy: 0.9250\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0689 - accuracy: 0.9784 - val_loss: 0.1513 - val_accuracy: 0.9438\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0627 - accuracy: 0.9818 - val_loss: 0.1398 - val_accuracy: 0.9500\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0520 - accuracy: 0.9854 - val_loss: 0.1227 - val_accuracy: 0.9438\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0520 - accuracy: 0.9852 - val_loss: 0.1320 - val_accuracy: 0.9500\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0467 - accuracy: 0.9865 - val_loss: 0.1381 - val_accuracy: 0.9500\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0463 - accuracy: 0.9872 - val_loss: 0.1359 - val_accuracy: 0.9375\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0491 - accuracy: 0.9865 - val_loss: 0.1417 - val_accuracy: 0.9438\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0435 - accuracy: 0.9878 - val_loss: 0.1553 - val_accuracy: 0.9438\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0469 - accuracy: 0.9870 - val_loss: 0.1558 - val_accuracy: 0.9500\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0402 - accuracy: 0.9880 - val_loss: 0.1451 - val_accuracy: 0.9438\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 30s 498ms/step - loss: 0.0407 - accuracy: 0.9867 - val_loss: 0.1231 - val_accuracy: 0.9563\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0395 - accuracy: 0.9893 - val_loss: 0.1192 - val_accuracy: 0.9438\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0378 - accuracy: 0.9885 - val_loss: 0.1274 - val_accuracy: 0.9500\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0383 - accuracy: 0.9885 - val_loss: 0.1351 - val_accuracy: 0.9438\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0325 - accuracy: 0.9919 - val_loss: 0.1236 - val_accuracy: 0.9563\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0356 - accuracy: 0.9901 - val_loss: 0.1159 - val_accuracy: 0.9500\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0311 - accuracy: 0.9909 - val_loss: 0.1133 - val_accuracy: 0.9563\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0364 - accuracy: 0.9898 - val_loss: 0.1398 - val_accuracy: 0.9438\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0331 - accuracy: 0.9909 - val_loss: 0.1291 - val_accuracy: 0.9500\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0345 - accuracy: 0.9898 - val_loss: 0.1133 - val_accuracy: 0.9500\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0307 - accuracy: 0.9914 - val_loss: 0.1348 - val_accuracy: 0.9375\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0322 - accuracy: 0.9901 - val_loss: 0.1231 - val_accuracy: 0.9563\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0292 - accuracy: 0.9901 - val_loss: 0.1157 - val_accuracy: 0.9500\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0287 - accuracy: 0.9914 - val_loss: 0.1232 - val_accuracy: 0.9563\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0239 - accuracy: 0.9930 - val_loss: 0.1171 - val_accuracy: 0.9500\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0305 - accuracy: 0.9898 - val_loss: 0.1047 - val_accuracy: 0.9500\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0290 - accuracy: 0.9914 - val_loss: 0.0963 - val_accuracy: 0.9625\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0280 - accuracy: 0.9904 - val_loss: 0.1110 - val_accuracy: 0.9500\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0323 - accuracy: 0.9893 - val_loss: 0.1014 - val_accuracy: 0.9563\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0222 - accuracy: 0.9940 - val_loss: 0.1106 - val_accuracy: 0.9438\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0240 - accuracy: 0.9940 - val_loss: 0.1192 - val_accuracy: 0.9563\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0221 - accuracy: 0.9940 - val_loss: 0.1232 - val_accuracy: 0.9438\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0240 - accuracy: 0.9924 - val_loss: 0.1179 - val_accuracy: 0.9563\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0271 - accuracy: 0.9906 - val_loss: 0.1013 - val_accuracy: 0.9563\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0242 - accuracy: 0.9924 - val_loss: 0.0928 - val_accuracy: 0.9563\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0156 - accuracy: 0.9971 - val_loss: 0.0913 - val_accuracy: 0.9625\n",
            "Epoch 64/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0208 - accuracy: 0.9935 - val_loss: 0.1103 - val_accuracy: 0.9500\n",
            "Epoch 65/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0194 - accuracy: 0.9940 - val_loss: 0.1110 - val_accuracy: 0.9563\n",
            "Epoch 66/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0213 - accuracy: 0.9951 - val_loss: 0.0963 - val_accuracy: 0.9563\n",
            "Epoch 67/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0190 - accuracy: 0.9937 - val_loss: 0.0965 - val_accuracy: 0.9500\n",
            "Epoch 68/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0160 - accuracy: 0.9958 - val_loss: 0.0893 - val_accuracy: 0.9625\n",
            "Epoch 69/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0233 - accuracy: 0.9932 - val_loss: 0.1495 - val_accuracy: 0.9500\n",
            "Epoch 70/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0211 - accuracy: 0.9943 - val_loss: 0.1160 - val_accuracy: 0.9625\n",
            "Epoch 71/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0150 - accuracy: 0.9964 - val_loss: 0.1211 - val_accuracy: 0.9563\n",
            "Epoch 72/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0129 - accuracy: 0.9971 - val_loss: 0.0871 - val_accuracy: 0.9563\n",
            "Epoch 73/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0165 - accuracy: 0.9953 - val_loss: 0.1064 - val_accuracy: 0.9625\n",
            "Epoch 74/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0135 - accuracy: 0.9966 - val_loss: 0.1081 - val_accuracy: 0.9563\n",
            "Epoch 75/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.1329 - val_accuracy: 0.9563\n",
            "Epoch 76/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0171 - accuracy: 0.9953 - val_loss: 0.1120 - val_accuracy: 0.9500\n",
            "Epoch 77/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.1016 - val_accuracy: 0.9625\n",
            "Epoch 78/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0187 - accuracy: 0.9930 - val_loss: 0.1002 - val_accuracy: 0.9625\n",
            "Epoch 79/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0153 - accuracy: 0.9961 - val_loss: 0.0822 - val_accuracy: 0.9563\n",
            "Epoch 80/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0130 - accuracy: 0.9971 - val_loss: 0.0920 - val_accuracy: 0.9625\n",
            "Epoch 81/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0143 - accuracy: 0.9966 - val_loss: 0.0894 - val_accuracy: 0.9625\n",
            "Epoch 82/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0135 - accuracy: 0.9958 - val_loss: 0.0934 - val_accuracy: 0.9563\n",
            "Epoch 83/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0137 - accuracy: 0.9956 - val_loss: 0.1149 - val_accuracy: 0.9500\n",
            "Epoch 84/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0113 - accuracy: 0.9979 - val_loss: 0.0970 - val_accuracy: 0.9625\n",
            "3/3 [==============================] - 1s 346ms/step - loss: 0.0963 - accuracy: 0.9625\n",
            "96.25 %\n",
            "Processing results for user 003... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 42s 543ms/step - loss: 2.6270 - accuracy: 0.1898 - val_loss: 1.7777 - val_accuracy: 0.6375\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 1.6401 - accuracy: 0.4833 - val_loss: 0.8540 - val_accuracy: 0.8188\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.9372 - accuracy: 0.7148 - val_loss: 0.4093 - val_accuracy: 0.9250\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.6317 - accuracy: 0.7961 - val_loss: 0.2712 - val_accuracy: 0.9500\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.4821 - accuracy: 0.8500 - val_loss: 0.2670 - val_accuracy: 0.9187\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.3931 - accuracy: 0.8771 - val_loss: 0.1954 - val_accuracy: 0.9563\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.3331 - accuracy: 0.8966 - val_loss: 0.1934 - val_accuracy: 0.9563\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.2851 - accuracy: 0.9091 - val_loss: 0.1725 - val_accuracy: 0.9563\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.2473 - accuracy: 0.9174 - val_loss: 0.2105 - val_accuracy: 0.9375\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.2147 - accuracy: 0.9299 - val_loss: 0.1784 - val_accuracy: 0.9438\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1867 - accuracy: 0.9419 - val_loss: 0.1585 - val_accuracy: 0.9625\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.1670 - accuracy: 0.9529 - val_loss: 0.1639 - val_accuracy: 0.9500\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.1604 - accuracy: 0.9482 - val_loss: 0.1497 - val_accuracy: 0.9500\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1469 - accuracy: 0.9516 - val_loss: 0.1204 - val_accuracy: 0.9688\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.1339 - accuracy: 0.9578 - val_loss: 0.1167 - val_accuracy: 0.9688\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.1310 - accuracy: 0.9615 - val_loss: 0.0922 - val_accuracy: 0.9688\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.1143 - accuracy: 0.9659 - val_loss: 0.1142 - val_accuracy: 0.9625\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.1080 - accuracy: 0.9635 - val_loss: 0.0989 - val_accuracy: 0.9750\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0892 - accuracy: 0.9747 - val_loss: 0.1121 - val_accuracy: 0.9625\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 29s 489ms/step - loss: 0.1048 - accuracy: 0.9646 - val_loss: 0.0873 - val_accuracy: 0.9688\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0865 - accuracy: 0.9784 - val_loss: 0.1095 - val_accuracy: 0.9625\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0841 - accuracy: 0.9755 - val_loss: 0.0935 - val_accuracy: 0.9688\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0779 - accuracy: 0.9786 - val_loss: 0.0776 - val_accuracy: 0.9750\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0769 - accuracy: 0.9776 - val_loss: 0.0903 - val_accuracy: 0.9750\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 29s 489ms/step - loss: 0.0700 - accuracy: 0.9789 - val_loss: 0.0705 - val_accuracy: 0.9688\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0651 - accuracy: 0.9810 - val_loss: 0.0833 - val_accuracy: 0.9750\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 29s 489ms/step - loss: 0.0707 - accuracy: 0.9781 - val_loss: 0.0613 - val_accuracy: 0.9750\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0658 - accuracy: 0.9797 - val_loss: 0.0609 - val_accuracy: 0.9750\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0659 - accuracy: 0.9781 - val_loss: 0.0561 - val_accuracy: 0.9812\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 29s 489ms/step - loss: 0.0562 - accuracy: 0.9826 - val_loss: 0.0589 - val_accuracy: 0.9750\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0589 - accuracy: 0.9815 - val_loss: 0.0503 - val_accuracy: 0.9750\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0532 - accuracy: 0.9841 - val_loss: 0.0893 - val_accuracy: 0.9625\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0552 - accuracy: 0.9826 - val_loss: 0.0739 - val_accuracy: 0.9688\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0475 - accuracy: 0.9852 - val_loss: 0.0616 - val_accuracy: 0.9750\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0487 - accuracy: 0.9841 - val_loss: 0.0561 - val_accuracy: 0.9812\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0452 - accuracy: 0.9878 - val_loss: 0.0706 - val_accuracy: 0.9750\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0473 - accuracy: 0.9857 - val_loss: 0.0485 - val_accuracy: 0.9875\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0387 - accuracy: 0.9865 - val_loss: 0.0589 - val_accuracy: 0.9750\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0394 - accuracy: 0.9888 - val_loss: 0.0627 - val_accuracy: 0.9750\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0418 - accuracy: 0.9875 - val_loss: 0.0498 - val_accuracy: 0.9812\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0393 - accuracy: 0.9893 - val_loss: 0.0481 - val_accuracy: 0.9812\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0361 - accuracy: 0.9878 - val_loss: 0.0568 - val_accuracy: 0.9750\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0386 - accuracy: 0.9867 - val_loss: 0.0588 - val_accuracy: 0.9750\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0321 - accuracy: 0.9904 - val_loss: 0.0620 - val_accuracy: 0.9750\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0392 - accuracy: 0.9898 - val_loss: 0.0597 - val_accuracy: 0.9688\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0353 - accuracy: 0.9901 - val_loss: 0.0573 - val_accuracy: 0.9750\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0307 - accuracy: 0.9906 - val_loss: 0.0473 - val_accuracy: 0.9750\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0292 - accuracy: 0.9922 - val_loss: 0.0412 - val_accuracy: 0.9875\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0322 - accuracy: 0.9911 - val_loss: 0.0434 - val_accuracy: 0.9750\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0313 - accuracy: 0.9901 - val_loss: 0.0632 - val_accuracy: 0.9688\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0284 - accuracy: 0.9901 - val_loss: 0.0574 - val_accuracy: 0.9750\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0280 - accuracy: 0.9927 - val_loss: 0.0516 - val_accuracy: 0.9688\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.0292 - accuracy: 0.9904 - val_loss: 0.0803 - val_accuracy: 0.9750\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0253 - accuracy: 0.9932 - val_loss: 0.0431 - val_accuracy: 0.9812\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0231 - accuracy: 0.9945 - val_loss: 0.0505 - val_accuracy: 0.9688\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.0194 - accuracy: 0.9940 - val_loss: 0.0381 - val_accuracy: 0.9812\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0227 - accuracy: 0.9937 - val_loss: 0.0422 - val_accuracy: 0.9750\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0226 - accuracy: 0.9953 - val_loss: 0.0471 - val_accuracy: 0.9750\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0229 - accuracy: 0.9945 - val_loss: 0.0623 - val_accuracy: 0.9750\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0218 - accuracy: 0.9943 - val_loss: 0.0343 - val_accuracy: 0.9875\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0245 - accuracy: 0.9930 - val_loss: 0.0383 - val_accuracy: 0.9750\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0254 - accuracy: 0.9909 - val_loss: 0.0366 - val_accuracy: 0.9812\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.0217 - accuracy: 0.9927 - val_loss: 0.0372 - val_accuracy: 0.9812\n",
            "Epoch 64/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.0171 - accuracy: 0.9961 - val_loss: 0.0401 - val_accuracy: 0.9750\n",
            "Epoch 65/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0190 - accuracy: 0.9943 - val_loss: 0.0391 - val_accuracy: 0.9812\n",
            "Epoch 66/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0181 - accuracy: 0.9956 - val_loss: 0.0425 - val_accuracy: 0.9750\n",
            "Epoch 67/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0184 - accuracy: 0.9943 - val_loss: 0.0505 - val_accuracy: 0.9750\n",
            "3/3 [==============================] - 1s 351ms/step - loss: 0.0485 - accuracy: 0.9875\n",
            "98.75 %\n",
            "Processing results for user 004... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 42s 542ms/step - loss: 2.5627 - accuracy: 0.2120 - val_loss: 2.2363 - val_accuracy: 0.2625\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 1.5353 - accuracy: 0.5065 - val_loss: 1.7505 - val_accuracy: 0.4125\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.8498 - accuracy: 0.7312 - val_loss: 1.4593 - val_accuracy: 0.5437\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.5355 - accuracy: 0.8328 - val_loss: 1.3689 - val_accuracy: 0.5750\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.4081 - accuracy: 0.8753 - val_loss: 1.3840 - val_accuracy: 0.6562\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.3217 - accuracy: 0.8979 - val_loss: 1.4746 - val_accuracy: 0.5938\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.2801 - accuracy: 0.9096 - val_loss: 1.4103 - val_accuracy: 0.6875\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.2174 - accuracy: 0.9359 - val_loss: 1.4042 - val_accuracy: 0.6625\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.2006 - accuracy: 0.9346 - val_loss: 1.4578 - val_accuracy: 0.6625\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1671 - accuracy: 0.9484 - val_loss: 1.5451 - val_accuracy: 0.6750\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.1547 - accuracy: 0.9534 - val_loss: 1.5531 - val_accuracy: 0.7000\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.1436 - accuracy: 0.9526 - val_loss: 1.5214 - val_accuracy: 0.6812\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1395 - accuracy: 0.9539 - val_loss: 1.5891 - val_accuracy: 0.7000\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1221 - accuracy: 0.9625 - val_loss: 1.6277 - val_accuracy: 0.6938\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1122 - accuracy: 0.9656 - val_loss: 1.6789 - val_accuracy: 0.7000\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1109 - accuracy: 0.9677 - val_loss: 1.6528 - val_accuracy: 0.7063\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0967 - accuracy: 0.9711 - val_loss: 1.6664 - val_accuracy: 0.7063\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.1004 - accuracy: 0.9680 - val_loss: 1.6185 - val_accuracy: 0.6812\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0816 - accuracy: 0.9771 - val_loss: 1.6412 - val_accuracy: 0.7063\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0756 - accuracy: 0.9760 - val_loss: 1.6053 - val_accuracy: 0.6938\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0737 - accuracy: 0.9773 - val_loss: 1.6126 - val_accuracy: 0.7125\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0747 - accuracy: 0.9768 - val_loss: 1.6722 - val_accuracy: 0.6875\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.0739 - accuracy: 0.9781 - val_loss: 1.5717 - val_accuracy: 0.7063\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0661 - accuracy: 0.9805 - val_loss: 1.6168 - val_accuracy: 0.7125\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0630 - accuracy: 0.9794 - val_loss: 1.6174 - val_accuracy: 0.7063\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0633 - accuracy: 0.9818 - val_loss: 1.6419 - val_accuracy: 0.7250\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.0589 - accuracy: 0.9844 - val_loss: 1.6899 - val_accuracy: 0.7188\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0534 - accuracy: 0.9844 - val_loss: 1.7692 - val_accuracy: 0.7188\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0484 - accuracy: 0.9867 - val_loss: 1.6983 - val_accuracy: 0.7312\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0486 - accuracy: 0.9859 - val_loss: 1.7876 - val_accuracy: 0.7063\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.0415 - accuracy: 0.9898 - val_loss: 1.7075 - val_accuracy: 0.7250\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0422 - accuracy: 0.9867 - val_loss: 1.6331 - val_accuracy: 0.7250\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0411 - accuracy: 0.9883 - val_loss: 1.6924 - val_accuracy: 0.7188\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 29s 490ms/step - loss: 0.0427 - accuracy: 0.9888 - val_loss: 1.6771 - val_accuracy: 0.7063\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0351 - accuracy: 0.9909 - val_loss: 1.8317 - val_accuracy: 0.7000\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0349 - accuracy: 0.9911 - val_loss: 1.7514 - val_accuracy: 0.7125\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0365 - accuracy: 0.9893 - val_loss: 1.6881 - val_accuracy: 0.7125\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0357 - accuracy: 0.9891 - val_loss: 1.7931 - val_accuracy: 0.7063\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0358 - accuracy: 0.9896 - val_loss: 1.7434 - val_accuracy: 0.6938\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0319 - accuracy: 0.9898 - val_loss: 1.7472 - val_accuracy: 0.7063\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0329 - accuracy: 0.9911 - val_loss: 1.6130 - val_accuracy: 0.7125\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0343 - accuracy: 0.9893 - val_loss: 1.8272 - val_accuracy: 0.7000\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.0356 - accuracy: 0.9888 - val_loss: 1.7640 - val_accuracy: 0.7000\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0291 - accuracy: 0.9917 - val_loss: 1.8742 - val_accuracy: 0.6750\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0326 - accuracy: 0.9901 - val_loss: 1.6382 - val_accuracy: 0.7125\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0282 - accuracy: 0.9914 - val_loss: 1.8461 - val_accuracy: 0.6938\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.0269 - accuracy: 0.9909 - val_loss: 1.8229 - val_accuracy: 0.6938\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0268 - accuracy: 0.9927 - val_loss: 1.8369 - val_accuracy: 0.6875\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0219 - accuracy: 0.9953 - val_loss: 1.8167 - val_accuracy: 0.7000\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0199 - accuracy: 0.9945 - val_loss: 1.7134 - val_accuracy: 0.7125\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0230 - accuracy: 0.9927 - val_loss: 1.8203 - val_accuracy: 0.7063\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0254 - accuracy: 0.9927 - val_loss: 1.9803 - val_accuracy: 0.6875\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0213 - accuracy: 0.9930 - val_loss: 1.8844 - val_accuracy: 0.6750\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0238 - accuracy: 0.9943 - val_loss: 1.8940 - val_accuracy: 0.6938\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0251 - accuracy: 0.9935 - val_loss: 1.9778 - val_accuracy: 0.6812\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.0218 - accuracy: 0.9927 - val_loss: 1.8301 - val_accuracy: 0.6938\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 30s 503ms/step - loss: 0.0185 - accuracy: 0.9953 - val_loss: 1.8004 - val_accuracy: 0.6812\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0191 - accuracy: 0.9951 - val_loss: 1.7305 - val_accuracy: 0.7000\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0180 - accuracy: 0.9945 - val_loss: 2.0671 - val_accuracy: 0.6500\n",
            "3/3 [==============================] - 1s 359ms/step - loss: 1.6983 - accuracy: 0.7312\n",
            "73.12 %\n",
            "Processing results for user 005... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 43s 546ms/step - loss: 2.6003 - accuracy: 0.1969 - val_loss: 1.9422 - val_accuracy: 0.4750\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 1.6256 - accuracy: 0.4872 - val_loss: 1.1821 - val_accuracy: 0.6812\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.9539 - accuracy: 0.7052 - val_loss: 0.7879 - val_accuracy: 0.7875\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.6200 - accuracy: 0.8060 - val_loss: 0.5689 - val_accuracy: 0.8438\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.4547 - accuracy: 0.8555 - val_loss: 0.5116 - val_accuracy: 0.8375\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.3565 - accuracy: 0.8943 - val_loss: 0.4303 - val_accuracy: 0.8625\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.3046 - accuracy: 0.9065 - val_loss: 0.4051 - val_accuracy: 0.8625\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.2533 - accuracy: 0.9214 - val_loss: 0.4055 - val_accuracy: 0.8438\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.2197 - accuracy: 0.9312 - val_loss: 0.3399 - val_accuracy: 0.8625\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1967 - accuracy: 0.9396 - val_loss: 0.3304 - val_accuracy: 0.8687\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.1771 - accuracy: 0.9471 - val_loss: 0.3626 - val_accuracy: 0.8938\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1787 - accuracy: 0.9432 - val_loss: 0.3112 - val_accuracy: 0.8750\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 29s 491ms/step - loss: 0.1442 - accuracy: 0.9536 - val_loss: 0.2885 - val_accuracy: 0.8875\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 30s 492ms/step - loss: 0.1367 - accuracy: 0.9565 - val_loss: 0.2884 - val_accuracy: 0.8750\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1368 - accuracy: 0.9560 - val_loss: 0.2852 - val_accuracy: 0.8625\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1067 - accuracy: 0.9693 - val_loss: 0.2694 - val_accuracy: 0.8875\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1085 - accuracy: 0.9654 - val_loss: 0.2722 - val_accuracy: 0.8813\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1082 - accuracy: 0.9672 - val_loss: 0.2547 - val_accuracy: 0.9000\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.1107 - accuracy: 0.9656 - val_loss: 0.2650 - val_accuracy: 0.8875\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0861 - accuracy: 0.9729 - val_loss: 0.2551 - val_accuracy: 0.8875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0829 - accuracy: 0.9750 - val_loss: 0.2580 - val_accuracy: 0.9125\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0869 - accuracy: 0.9740 - val_loss: 0.2398 - val_accuracy: 0.8938\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0721 - accuracy: 0.9799 - val_loss: 0.2503 - val_accuracy: 0.9062\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0664 - accuracy: 0.9799 - val_loss: 0.2358 - val_accuracy: 0.9250\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0683 - accuracy: 0.9807 - val_loss: 0.2360 - val_accuracy: 0.9187\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0640 - accuracy: 0.9797 - val_loss: 0.2551 - val_accuracy: 0.9000\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0656 - accuracy: 0.9807 - val_loss: 0.2660 - val_accuracy: 0.8875\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0622 - accuracy: 0.9815 - val_loss: 0.2293 - val_accuracy: 0.9062\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0544 - accuracy: 0.9820 - val_loss: 0.2468 - val_accuracy: 0.8938\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0470 - accuracy: 0.9870 - val_loss: 0.2467 - val_accuracy: 0.9125\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0514 - accuracy: 0.9867 - val_loss: 0.2332 - val_accuracy: 0.9125\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0453 - accuracy: 0.9857 - val_loss: 0.2571 - val_accuracy: 0.8938\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0462 - accuracy: 0.9865 - val_loss: 0.2471 - val_accuracy: 0.9000\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0441 - accuracy: 0.9885 - val_loss: 0.2686 - val_accuracy: 0.9062\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0374 - accuracy: 0.9904 - val_loss: 0.2138 - val_accuracy: 0.9312\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0370 - accuracy: 0.9904 - val_loss: 0.2460 - val_accuracy: 0.9062\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0418 - accuracy: 0.9867 - val_loss: 0.2157 - val_accuracy: 0.9187\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0398 - accuracy: 0.9872 - val_loss: 0.2197 - val_accuracy: 0.8938\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0432 - accuracy: 0.9878 - val_loss: 0.2817 - val_accuracy: 0.8813\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0417 - accuracy: 0.9862 - val_loss: 0.2441 - val_accuracy: 0.9125\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0356 - accuracy: 0.9901 - val_loss: 0.2369 - val_accuracy: 0.9000\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0305 - accuracy: 0.9919 - val_loss: 0.2446 - val_accuracy: 0.9000\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0312 - accuracy: 0.9909 - val_loss: 0.2118 - val_accuracy: 0.9000\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0331 - accuracy: 0.9927 - val_loss: 0.2350 - val_accuracy: 0.8938\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0339 - accuracy: 0.9914 - val_loss: 0.2409 - val_accuracy: 0.9062\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0339 - accuracy: 0.9883 - val_loss: 0.2045 - val_accuracy: 0.9000\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0314 - accuracy: 0.9896 - val_loss: 0.2500 - val_accuracy: 0.9125\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0276 - accuracy: 0.9919 - val_loss: 0.2327 - val_accuracy: 0.9000\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0303 - accuracy: 0.9919 - val_loss: 0.2328 - val_accuracy: 0.9062\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0315 - accuracy: 0.9901 - val_loss: 0.2203 - val_accuracy: 0.9062\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0269 - accuracy: 0.9911 - val_loss: 0.2192 - val_accuracy: 0.9125\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0251 - accuracy: 0.9937 - val_loss: 0.2384 - val_accuracy: 0.8938\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0293 - accuracy: 0.9922 - val_loss: 0.2814 - val_accuracy: 0.9062\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0229 - accuracy: 0.9943 - val_loss: 0.2606 - val_accuracy: 0.9062\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0232 - accuracy: 0.9919 - val_loss: 0.2892 - val_accuracy: 0.9000\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0222 - accuracy: 0.9943 - val_loss: 0.3234 - val_accuracy: 0.8938\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0207 - accuracy: 0.9958 - val_loss: 0.2563 - val_accuracy: 0.9000\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0201 - accuracy: 0.9951 - val_loss: 0.2704 - val_accuracy: 0.8938\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0196 - accuracy: 0.9951 - val_loss: 0.2507 - val_accuracy: 0.8875\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0207 - accuracy: 0.9943 - val_loss: 0.2784 - val_accuracy: 0.8938\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0214 - accuracy: 0.9943 - val_loss: 0.2333 - val_accuracy: 0.8938\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0225 - accuracy: 0.9927 - val_loss: 0.2699 - val_accuracy: 0.8938\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0208 - accuracy: 0.9943 - val_loss: 0.2479 - val_accuracy: 0.9062\n",
            "Epoch 64/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0193 - accuracy: 0.9948 - val_loss: 0.2734 - val_accuracy: 0.8938\n",
            "Epoch 65/300\n",
            "60/60 [==============================] - 30s 498ms/step - loss: 0.0172 - accuracy: 0.9966 - val_loss: 0.2328 - val_accuracy: 0.9125\n",
            "3/3 [==============================] - 1s 345ms/step - loss: 0.2138 - accuracy: 0.9312\n",
            "93.12 %\n",
            "Processing results for user 006... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 43s 553ms/step - loss: 2.5531 - accuracy: 0.2078 - val_loss: 1.6304 - val_accuracy: 0.7250\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 30s 499ms/step - loss: 1.5508 - accuracy: 0.5052 - val_loss: 0.8705 - val_accuracy: 0.8125\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.8759 - accuracy: 0.7281 - val_loss: 0.7410 - val_accuracy: 0.7688\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 30s 498ms/step - loss: 0.5871 - accuracy: 0.8143 - val_loss: 0.6211 - val_accuracy: 0.7937\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.4336 - accuracy: 0.8687 - val_loss: 0.5582 - val_accuracy: 0.7812\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 30s 500ms/step - loss: 0.3351 - accuracy: 0.8883 - val_loss: 0.5513 - val_accuracy: 0.8188\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 30s 499ms/step - loss: 0.2636 - accuracy: 0.9117 - val_loss: 0.4106 - val_accuracy: 0.8562\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 30s 499ms/step - loss: 0.2314 - accuracy: 0.9216 - val_loss: 0.3567 - val_accuracy: 0.8875\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.2169 - accuracy: 0.9305 - val_loss: 0.4076 - val_accuracy: 0.8562\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 30s 498ms/step - loss: 0.1887 - accuracy: 0.9411 - val_loss: 0.4052 - val_accuracy: 0.8438\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.1674 - accuracy: 0.9479 - val_loss: 0.3832 - val_accuracy: 0.8687\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.1533 - accuracy: 0.9518 - val_loss: 0.4191 - val_accuracy: 0.8438\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 30s 498ms/step - loss: 0.1402 - accuracy: 0.9570 - val_loss: 0.3882 - val_accuracy: 0.8687\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 30s 498ms/step - loss: 0.1278 - accuracy: 0.9604 - val_loss: 0.3507 - val_accuracy: 0.8750\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.1143 - accuracy: 0.9651 - val_loss: 0.3170 - val_accuracy: 0.8813\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 30s 499ms/step - loss: 0.0974 - accuracy: 0.9701 - val_loss: 0.3164 - val_accuracy: 0.9000\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0906 - accuracy: 0.9714 - val_loss: 0.3628 - val_accuracy: 0.8750\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0981 - accuracy: 0.9695 - val_loss: 0.3260 - val_accuracy: 0.9000\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0865 - accuracy: 0.9727 - val_loss: 0.3222 - val_accuracy: 0.8938\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0739 - accuracy: 0.9766 - val_loss: 0.3440 - val_accuracy: 0.8875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 30s 499ms/step - loss: 0.0656 - accuracy: 0.9833 - val_loss: 0.3159 - val_accuracy: 0.9062\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 30s 499ms/step - loss: 0.0744 - accuracy: 0.9768 - val_loss: 0.3340 - val_accuracy: 0.9125\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0706 - accuracy: 0.9776 - val_loss: 0.3024 - val_accuracy: 0.9062\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 30s 500ms/step - loss: 0.0612 - accuracy: 0.9831 - val_loss: 0.2968 - val_accuracy: 0.9187\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0579 - accuracy: 0.9823 - val_loss: 0.3437 - val_accuracy: 0.9062\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0584 - accuracy: 0.9828 - val_loss: 0.3691 - val_accuracy: 0.9000\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0538 - accuracy: 0.9815 - val_loss: 0.3603 - val_accuracy: 0.9000\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0519 - accuracy: 0.9833 - val_loss: 0.3605 - val_accuracy: 0.8938\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 30s 498ms/step - loss: 0.0552 - accuracy: 0.9815 - val_loss: 0.3372 - val_accuracy: 0.9000\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0535 - accuracy: 0.9839 - val_loss: 0.3574 - val_accuracy: 0.8875\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0484 - accuracy: 0.9857 - val_loss: 0.3766 - val_accuracy: 0.8875\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.4183 - val_accuracy: 0.8813\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0461 - accuracy: 0.9862 - val_loss: 0.3437 - val_accuracy: 0.9000\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0443 - accuracy: 0.9870 - val_loss: 0.3649 - val_accuracy: 0.9000\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0394 - accuracy: 0.9880 - val_loss: 0.3506 - val_accuracy: 0.9000\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0362 - accuracy: 0.9883 - val_loss: 0.3352 - val_accuracy: 0.9062\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.0370 - accuracy: 0.9883 - val_loss: 0.3267 - val_accuracy: 0.9062\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 30s 499ms/step - loss: 0.0385 - accuracy: 0.9867 - val_loss: 0.3284 - val_accuracy: 0.9250\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0398 - accuracy: 0.9857 - val_loss: 0.3317 - val_accuracy: 0.8938\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0323 - accuracy: 0.9896 - val_loss: 0.3663 - val_accuracy: 0.8875\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0320 - accuracy: 0.9909 - val_loss: 0.3382 - val_accuracy: 0.9000\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0281 - accuracy: 0.9919 - val_loss: 0.3699 - val_accuracy: 0.9250\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0300 - accuracy: 0.9922 - val_loss: 0.3525 - val_accuracy: 0.8938\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0295 - accuracy: 0.9906 - val_loss: 0.3557 - val_accuracy: 0.9125\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0310 - accuracy: 0.9914 - val_loss: 0.3509 - val_accuracy: 0.9187\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0293 - accuracy: 0.9904 - val_loss: 0.3589 - val_accuracy: 0.9062\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0263 - accuracy: 0.9924 - val_loss: 0.3558 - val_accuracy: 0.8938\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0282 - accuracy: 0.9901 - val_loss: 0.4490 - val_accuracy: 0.8625\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0291 - accuracy: 0.9914 - val_loss: 0.3740 - val_accuracy: 0.9125\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0232 - accuracy: 0.9927 - val_loss: 0.3493 - val_accuracy: 0.9312\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 0.4169 - val_accuracy: 0.8938\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0234 - accuracy: 0.9930 - val_loss: 0.3474 - val_accuracy: 0.9125\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0233 - accuracy: 0.9924 - val_loss: 0.3799 - val_accuracy: 0.9312\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0184 - accuracy: 0.9948 - val_loss: 0.4101 - val_accuracy: 0.9062\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0229 - accuracy: 0.9924 - val_loss: 0.4023 - val_accuracy: 0.9062\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0144 - accuracy: 0.9964 - val_loss: 0.3726 - val_accuracy: 0.9000\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0210 - accuracy: 0.9943 - val_loss: 0.3668 - val_accuracy: 0.9250\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0210 - accuracy: 0.9948 - val_loss: 0.3940 - val_accuracy: 0.9125\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0281 - accuracy: 0.9901 - val_loss: 0.3680 - val_accuracy: 0.9125\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0210 - accuracy: 0.9948 - val_loss: 0.3284 - val_accuracy: 0.9250\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0171 - accuracy: 0.9964 - val_loss: 0.3547 - val_accuracy: 0.9187\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0197 - accuracy: 0.9948 - val_loss: 0.3173 - val_accuracy: 0.9312\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0205 - accuracy: 0.9948 - val_loss: 0.3770 - val_accuracy: 0.8938\n",
            "Epoch 64/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0144 - accuracy: 0.9958 - val_loss: 0.3539 - val_accuracy: 0.9250\n",
            "Epoch 65/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0190 - accuracy: 0.9951 - val_loss: 0.3877 - val_accuracy: 0.9187\n",
            "Epoch 66/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0139 - accuracy: 0.9974 - val_loss: 0.3726 - val_accuracy: 0.9062\n",
            "Epoch 67/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0125 - accuracy: 0.9966 - val_loss: 0.3382 - val_accuracy: 0.8938\n",
            "Epoch 68/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0143 - accuracy: 0.9974 - val_loss: 0.3684 - val_accuracy: 0.9187\n",
            "Epoch 69/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0159 - accuracy: 0.9964 - val_loss: 0.4259 - val_accuracy: 0.9062\n",
            "Epoch 70/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0179 - accuracy: 0.9937 - val_loss: 0.3051 - val_accuracy: 0.9375\n",
            "Epoch 71/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0165 - accuracy: 0.9945 - val_loss: 0.3460 - val_accuracy: 0.9125\n",
            "Epoch 72/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0152 - accuracy: 0.9966 - val_loss: 0.4469 - val_accuracy: 0.9000\n",
            "Epoch 73/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0167 - accuracy: 0.9961 - val_loss: 0.3632 - val_accuracy: 0.9250\n",
            "Epoch 74/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0122 - accuracy: 0.9969 - val_loss: 0.3806 - val_accuracy: 0.9062\n",
            "Epoch 75/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0166 - accuracy: 0.9956 - val_loss: 0.3759 - val_accuracy: 0.9062\n",
            "Epoch 76/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.3864 - val_accuracy: 0.8813\n",
            "Epoch 77/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0176 - accuracy: 0.9948 - val_loss: 0.3674 - val_accuracy: 0.9125\n",
            "Epoch 78/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0143 - accuracy: 0.9961 - val_loss: 0.4044 - val_accuracy: 0.9250\n",
            "Epoch 79/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0139 - accuracy: 0.9964 - val_loss: 0.3951 - val_accuracy: 0.9125\n",
            "Epoch 80/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0101 - accuracy: 0.9974 - val_loss: 0.4371 - val_accuracy: 0.9000\n",
            "Epoch 81/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.3976 - val_accuracy: 0.9187\n",
            "Epoch 82/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0114 - accuracy: 0.9977 - val_loss: 0.3988 - val_accuracy: 0.9187\n",
            "Epoch 83/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0096 - accuracy: 0.9977 - val_loss: 0.3744 - val_accuracy: 0.9187\n",
            "Epoch 84/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0137 - accuracy: 0.9956 - val_loss: 0.3811 - val_accuracy: 0.8938\n",
            "Epoch 85/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0122 - accuracy: 0.9964 - val_loss: 0.3571 - val_accuracy: 0.9062\n",
            "Epoch 86/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0118 - accuracy: 0.9971 - val_loss: 0.3214 - val_accuracy: 0.9187\n",
            "Epoch 87/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.3990 - val_accuracy: 0.9062\n",
            "Epoch 88/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0115 - accuracy: 0.9974 - val_loss: 0.5438 - val_accuracy: 0.8687\n",
            "Epoch 89/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0107 - accuracy: 0.9974 - val_loss: 0.3994 - val_accuracy: 0.9000\n",
            "Epoch 90/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0086 - accuracy: 0.9974 - val_loss: 0.3922 - val_accuracy: 0.9062\n",
            "Epoch 91/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0112 - accuracy: 0.9966 - val_loss: 0.4532 - val_accuracy: 0.9000\n",
            "Epoch 92/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0108 - accuracy: 0.9977 - val_loss: 0.3940 - val_accuracy: 0.9000\n",
            "Epoch 93/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0120 - accuracy: 0.9956 - val_loss: 0.4532 - val_accuracy: 0.9062\n",
            "Epoch 94/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.4617 - val_accuracy: 0.9000\n",
            "Epoch 95/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0078 - accuracy: 0.9984 - val_loss: 0.4159 - val_accuracy: 0.9062\n",
            "Epoch 96/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.4130 - val_accuracy: 0.9000\n",
            "Epoch 97/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0094 - accuracy: 0.9977 - val_loss: 0.3799 - val_accuracy: 0.9250\n",
            "Epoch 98/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.4024 - val_accuracy: 0.9187\n",
            "Epoch 99/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0082 - accuracy: 0.9984 - val_loss: 0.4190 - val_accuracy: 0.9062\n",
            "Epoch 100/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 0.3558 - val_accuracy: 0.9250\n",
            "3/3 [==============================] - 1s 355ms/step - loss: 0.3051 - accuracy: 0.9375\n",
            "93.75 %\n",
            "Processing results for user 007... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 43s 546ms/step - loss: 2.6409 - accuracy: 0.1846 - val_loss: 1.8699 - val_accuracy: 0.5375\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 1.6656 - accuracy: 0.4865 - val_loss: 0.8640 - val_accuracy: 0.8375\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 1.0041 - accuracy: 0.6977 - val_loss: 0.4476 - val_accuracy: 0.8875\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.6437 - accuracy: 0.7927 - val_loss: 0.2979 - val_accuracy: 0.9375\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.4970 - accuracy: 0.8435 - val_loss: 0.2237 - val_accuracy: 0.9563\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.4119 - accuracy: 0.8724 - val_loss: 0.1741 - val_accuracy: 0.9563\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.3347 - accuracy: 0.8971 - val_loss: 0.1434 - val_accuracy: 0.9625\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.2855 - accuracy: 0.9120 - val_loss: 0.1320 - val_accuracy: 0.9750\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.2473 - accuracy: 0.9203 - val_loss: 0.1021 - val_accuracy: 0.9750\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.2008 - accuracy: 0.9359 - val_loss: 0.0987 - val_accuracy: 0.9812\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.1907 - accuracy: 0.9427 - val_loss: 0.0997 - val_accuracy: 0.9812\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1729 - accuracy: 0.9474 - val_loss: 0.0857 - val_accuracy: 0.9750\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.1471 - accuracy: 0.9529 - val_loss: 0.1006 - val_accuracy: 0.9812\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.1479 - accuracy: 0.9526 - val_loss: 0.0797 - val_accuracy: 0.9875\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.1337 - accuracy: 0.9612 - val_loss: 0.0951 - val_accuracy: 0.9812\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1263 - accuracy: 0.9599 - val_loss: 0.0914 - val_accuracy: 0.9812\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.1191 - accuracy: 0.9630 - val_loss: 0.0799 - val_accuracy: 0.9750\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.1086 - accuracy: 0.9646 - val_loss: 0.0754 - val_accuracy: 0.9812\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0966 - accuracy: 0.9732 - val_loss: 0.0838 - val_accuracy: 0.9812\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1018 - accuracy: 0.9719 - val_loss: 0.0705 - val_accuracy: 0.9812\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0895 - accuracy: 0.9724 - val_loss: 0.0871 - val_accuracy: 0.9750\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0758 - accuracy: 0.9763 - val_loss: 0.0791 - val_accuracy: 0.9812\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0781 - accuracy: 0.9745 - val_loss: 0.0797 - val_accuracy: 0.9750\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0748 - accuracy: 0.9768 - val_loss: 0.0880 - val_accuracy: 0.9812\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0698 - accuracy: 0.9779 - val_loss: 0.0593 - val_accuracy: 0.9812\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0681 - accuracy: 0.9794 - val_loss: 0.0670 - val_accuracy: 0.9812\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0664 - accuracy: 0.9784 - val_loss: 0.0627 - val_accuracy: 0.9875\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0676 - accuracy: 0.9786 - val_loss: 0.0816 - val_accuracy: 0.9812\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0558 - accuracy: 0.9831 - val_loss: 0.0624 - val_accuracy: 0.9812\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0585 - accuracy: 0.9805 - val_loss: 0.0789 - val_accuracy: 0.9812\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0516 - accuracy: 0.9859 - val_loss: 0.0554 - val_accuracy: 0.9875\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0498 - accuracy: 0.9857 - val_loss: 0.0750 - val_accuracy: 0.9812\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0513 - accuracy: 0.9857 - val_loss: 0.0683 - val_accuracy: 0.9812\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0480 - accuracy: 0.9849 - val_loss: 0.0676 - val_accuracy: 0.9812\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0446 - accuracy: 0.9852 - val_loss: 0.0721 - val_accuracy: 0.9812\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0417 - accuracy: 0.9870 - val_loss: 0.0566 - val_accuracy: 0.9812\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0485 - accuracy: 0.9859 - val_loss: 0.0548 - val_accuracy: 0.9937\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0387 - accuracy: 0.9888 - val_loss: 0.0555 - val_accuracy: 0.9875\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0381 - accuracy: 0.9891 - val_loss: 0.0528 - val_accuracy: 0.9875\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0374 - accuracy: 0.9885 - val_loss: 0.0563 - val_accuracy: 0.9812\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0371 - accuracy: 0.9906 - val_loss: 0.0641 - val_accuracy: 0.9875\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0354 - accuracy: 0.9898 - val_loss: 0.0607 - val_accuracy: 0.9875\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0362 - accuracy: 0.9875 - val_loss: 0.0531 - val_accuracy: 0.9937\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0331 - accuracy: 0.9898 - val_loss: 0.0777 - val_accuracy: 0.9875\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0311 - accuracy: 0.9917 - val_loss: 0.0663 - val_accuracy: 0.9812\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0301 - accuracy: 0.9911 - val_loss: 0.0535 - val_accuracy: 0.9875\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0267 - accuracy: 0.9930 - val_loss: 0.0528 - val_accuracy: 0.9875\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0298 - accuracy: 0.9911 - val_loss: 0.0505 - val_accuracy: 0.9875\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0328 - accuracy: 0.9906 - val_loss: 0.0594 - val_accuracy: 0.9812\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0281 - accuracy: 0.9922 - val_loss: 0.0724 - val_accuracy: 0.9812\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0316 - accuracy: 0.9909 - val_loss: 0.0611 - val_accuracy: 0.9875\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0267 - accuracy: 0.9906 - val_loss: 0.0684 - val_accuracy: 0.9875\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0293 - accuracy: 0.9911 - val_loss: 0.0662 - val_accuracy: 0.9812\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0242 - accuracy: 0.9922 - val_loss: 0.0793 - val_accuracy: 0.9812\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0304 - accuracy: 0.9911 - val_loss: 0.0853 - val_accuracy: 0.9750\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 29s 492ms/step - loss: 0.0260 - accuracy: 0.9935 - val_loss: 0.0625 - val_accuracy: 0.9812\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0241 - accuracy: 0.9937 - val_loss: 0.0535 - val_accuracy: 0.9875\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.0625 - val_accuracy: 0.9812\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0218 - accuracy: 0.9937 - val_loss: 0.0531 - val_accuracy: 0.9937\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0202 - accuracy: 0.9951 - val_loss: 0.0617 - val_accuracy: 0.9812\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0245 - accuracy: 0.9930 - val_loss: 0.0516 - val_accuracy: 0.9937\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0182 - accuracy: 0.9951 - val_loss: 0.0487 - val_accuracy: 0.9937\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0181 - accuracy: 0.9966 - val_loss: 0.0516 - val_accuracy: 0.9937\n",
            "Epoch 64/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0175 - accuracy: 0.9961 - val_loss: 0.0612 - val_accuracy: 0.9875\n",
            "Epoch 65/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0185 - accuracy: 0.9945 - val_loss: 0.0587 - val_accuracy: 0.9875\n",
            "Epoch 66/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0197 - accuracy: 0.9937 - val_loss: 0.0474 - val_accuracy: 0.9937\n",
            "Epoch 67/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0197 - accuracy: 0.9956 - val_loss: 0.0580 - val_accuracy: 0.9937\n",
            "3/3 [==============================] - 1s 345ms/step - loss: 0.0548 - accuracy: 0.9937\n",
            "99.37 %\n",
            "Processing results for user 008... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 42s 543ms/step - loss: 2.5805 - accuracy: 0.2055 - val_loss: 1.7443 - val_accuracy: 0.6125\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 1.5840 - accuracy: 0.4958 - val_loss: 0.9106 - val_accuracy: 0.8062\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.9092 - accuracy: 0.7156 - val_loss: 0.4779 - val_accuracy: 0.9125\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.6098 - accuracy: 0.8073 - val_loss: 0.3540 - val_accuracy: 0.9062\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.4563 - accuracy: 0.8565 - val_loss: 0.2708 - val_accuracy: 0.9250\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.3788 - accuracy: 0.8880 - val_loss: 0.2550 - val_accuracy: 0.9375\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.3042 - accuracy: 0.9049 - val_loss: 0.2329 - val_accuracy: 0.9438\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.2646 - accuracy: 0.9190 - val_loss: 0.2683 - val_accuracy: 0.9250\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.2241 - accuracy: 0.9286 - val_loss: 0.2056 - val_accuracy: 0.9438\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.1993 - accuracy: 0.9362 - val_loss: 0.2074 - val_accuracy: 0.9375\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 30s 497ms/step - loss: 0.1921 - accuracy: 0.9409 - val_loss: 0.1915 - val_accuracy: 0.9500\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.1592 - accuracy: 0.9495 - val_loss: 0.1958 - val_accuracy: 0.9500\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.1595 - accuracy: 0.9474 - val_loss: 0.1918 - val_accuracy: 0.9500\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1537 - accuracy: 0.9495 - val_loss: 0.2232 - val_accuracy: 0.9375\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.1173 - accuracy: 0.9643 - val_loss: 0.2173 - val_accuracy: 0.9500\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1012 - accuracy: 0.9714 - val_loss: 0.1801 - val_accuracy: 0.9500\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.1107 - accuracy: 0.9659 - val_loss: 0.2057 - val_accuracy: 0.9438\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.1012 - accuracy: 0.9677 - val_loss: 0.2284 - val_accuracy: 0.9375\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0958 - accuracy: 0.9698 - val_loss: 0.2343 - val_accuracy: 0.9312\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 30s 493ms/step - loss: 0.0897 - accuracy: 0.9714 - val_loss: 0.1619 - val_accuracy: 0.9500\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0848 - accuracy: 0.9742 - val_loss: 0.2192 - val_accuracy: 0.9438\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0744 - accuracy: 0.9755 - val_loss: 0.2551 - val_accuracy: 0.9312\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0764 - accuracy: 0.9763 - val_loss: 0.2195 - val_accuracy: 0.9438\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0687 - accuracy: 0.9784 - val_loss: 0.2508 - val_accuracy: 0.9312\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0769 - accuracy: 0.9768 - val_loss: 0.2193 - val_accuracy: 0.9438\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0679 - accuracy: 0.9789 - val_loss: 0.2244 - val_accuracy: 0.9375\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0692 - accuracy: 0.9805 - val_loss: 0.2174 - val_accuracy: 0.9375\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0618 - accuracy: 0.9799 - val_loss: 0.2297 - val_accuracy: 0.9438\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0627 - accuracy: 0.9799 - val_loss: 0.2348 - val_accuracy: 0.9375\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0571 - accuracy: 0.9828 - val_loss: 0.2596 - val_accuracy: 0.9312\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 30s 494ms/step - loss: 0.0496 - accuracy: 0.9867 - val_loss: 0.2083 - val_accuracy: 0.9500\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0517 - accuracy: 0.9836 - val_loss: 0.2773 - val_accuracy: 0.9312\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0468 - accuracy: 0.9857 - val_loss: 0.2612 - val_accuracy: 0.9375\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0441 - accuracy: 0.9875 - val_loss: 0.2046 - val_accuracy: 0.9500\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0508 - accuracy: 0.9852 - val_loss: 0.2423 - val_accuracy: 0.9438\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 30s 496ms/step - loss: 0.0425 - accuracy: 0.9880 - val_loss: 0.2851 - val_accuracy: 0.9375\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0414 - accuracy: 0.9878 - val_loss: 0.2371 - val_accuracy: 0.9375\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0429 - accuracy: 0.9867 - val_loss: 0.2900 - val_accuracy: 0.9375\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0358 - accuracy: 0.9893 - val_loss: 0.2611 - val_accuracy: 0.9438\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 30s 495ms/step - loss: 0.0423 - accuracy: 0.9883 - val_loss: 0.2440 - val_accuracy: 0.9375\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 30s 498ms/step - loss: 0.0321 - accuracy: 0.9904 - val_loss: 0.2479 - val_accuracy: 0.9375\n",
            "3/3 [==============================] - 1s 344ms/step - loss: 0.1915 - accuracy: 0.9500\n",
            "95.00 %\n",
            "------------------------------------\n",
            "Average accuracy 93.59 +/- 8.08\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ACC = []\n",
        "CM = []\n",
        "HISTORY = []\n",
        "logs = ''\n",
        "\n",
        "# ... Change user limit\n",
        "users = USERS[:8]\n",
        "\n",
        "for test_user in users:\n",
        "    print('Processing results for user ' + test_user, end='... \\n')\n",
        "    \n",
        "    X_train = []\n",
        "    X_test = []\n",
        "    y_train = []\n",
        "    y_test = []\n",
        "    \n",
        "    first_time_train = True\n",
        "    first_time_test = True\n",
        "\n",
        "    for user in USERS:\n",
        "        x_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_X.joblib')\n",
        "        y_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_y.joblib')\n",
        "        X = joblib.load(x_path)\n",
        "        y = joblib.load(y_path)\n",
        "\n",
        "        if user == test_user:\n",
        "            if first_time_train == True:\n",
        "                first_time_train = False\n",
        "                X_test = X\n",
        "                y_test = y\n",
        "                \n",
        "            else:\n",
        "                X_test = np.append(X_test, X, axis=0)\n",
        "                y_test = np.append(y_test, y, axis=0)\n",
        "                \n",
        "        else:\n",
        "            if first_time_test == True:\n",
        "                first_time_test = False\n",
        "                X_train = X\n",
        "                y_train = y\n",
        "                \n",
        "            else:\n",
        "                X_train = np.append(X_train, X, axis=0)\n",
        "                y_train = np.append(y_train, y, axis=0)\n",
        "\n",
        "\n",
        "    # X_train, y_train = shuffle(X_train, y_train)\n",
        "\n",
        "    X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data('XY', test_user)\n",
        "    X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data('YZ', test_user)\n",
        "    X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data('ZX', test_user)\n",
        "\n",
        "    X_train_xy, X_train_yz, X_train_zx, X_train, y_train = shuffle(\n",
        "        X_train_xy, X_train_yz, X_train_zx, X_train, y_train\n",
        "    )\n",
        "\n",
        "    X_train_combined = np.split(X_train, 8, axis=-1)[:5] + [X_train_xy, X_train_yz, X_train_zx]\n",
        "    X_test_combined = np.split(X_test, 8, axis=-1)[:5] + [X_test_xy, X_test_yz, X_test_zx]\n",
        "\n",
        "    del X_train_xy, X_test_xy, y_train_xy, y_test_xy\n",
        "    del X_train_yz, X_test_yz, y_train_yz, y_test_yz\n",
        "    del X_train_zx, X_test_zx, y_train_zx, y_test_zx\n",
        "    del X_train, X_test\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    print(len(X_train_combined))\n",
        "\n",
        "    callbacks = [\n",
        "                 tf.keras.callbacks.EarlyStopping(\n",
        "                    monitor='val_accuracy', \n",
        "                    patience=30, \n",
        "                    mode='max', \n",
        "                    restore_best_weights=True\n",
        "                ),\n",
        "    ]\n",
        "\n",
        "    model = get_stacked_model()\n",
        "    history = model.fit(\n",
        "        X_train_combined, \n",
        "        y_train, \n",
        "        validation_data=(X_test_combined, y_test),\n",
        "        epochs=300, \n",
        "        batch_size=64,\n",
        "        callbacks=[callbacks]\n",
        "    )\n",
        "    _, accuracy = model.evaluate(X_test_combined, y_test, batch_size=64)\n",
        "    y_pred = model.predict(X_test_combined)\n",
        "    cm = confusion_matrix(y_test.ravel(), np.argmax(y_pred, axis=-1))\n",
        "\n",
        "    CM.append(cm)\n",
        "    HISTORY.append(history)\n",
        "\n",
        "    accuracy = accuracy * 100\n",
        "    print(f'%.2f %%' %(accuracy))\n",
        "    logs = logs + 'Accuracy for user ' + str(test_user) + '... ' + str(accuracy) + '\\n'\n",
        "    ACC.append(accuracy)\n",
        "\n",
        "    del model, history\n",
        "    gc.collect()\n",
        "    \n",
        "AVG_ACC = np.mean(ACC)\n",
        "STD = np.std(ACC)\n",
        "print('------------------------------------')\n",
        "print(f'Average accuracy %.2f +/- %.2f' %(AVG_ACC, STD))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "data = {\n",
        "    \"cm\": CM,\n",
        "    \"accuracy\": ACC,\n",
        "    \"logs\": logs\n",
        "}\n",
        "\n",
        "joblib.dump(data, \"user1-8.joblib\")"
      ],
      "metadata": {
        "id": "HeOI8hw3zCXz",
        "outputId": "1354c328-673d-4157-ae42-a30dbfea912d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HeOI8hw3zCXz",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['user1-8.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    pass"
      ],
      "metadata": {
        "id": "3N94GWOk9KHE",
        "outputId": "a3f415d6-5993-4f77-bbff-ea419d30ac45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "id": "3N94GWOk9KHE",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b7133701d76c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92fd4333",
      "metadata": {
        "id": "92fd4333"
      },
      "outputs": [],
      "source": [
        "# model = get_stacked_model()\n",
        "# X_train_xy, X_train_yz, X_train_zx, y_train_xy = shuffle(\n",
        "#     X_train_xy, X_train_yz, X_train_zx, y_train_xy\n",
        "# )\n",
        "\n",
        "# history = model.fit(\n",
        "#     [X_train_xy, X_train_yz, X_train_zx],\n",
        "#     y_train_xy,\n",
        "#     validation_data=([X_test_xy, X_test_yz, X_test_zx], y_test_xy),\n",
        "#     batch_size=32,\n",
        "#     epochs=10,\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distinct-appendix",
      "metadata": {
        "id": "distinct-appendix",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# model_xy = get_model()\n",
        "# X_train_xy, y_train_xy = shuffle(X_train_xy, y_train_xy)\n",
        "# history_xy = model_xy.fit(X_train_xy, y_train_xy, validation_data=(X_test_xy, y_test_xy), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "constitutional-genre",
      "metadata": {
        "id": "constitutional-genre"
      },
      "outputs": [],
      "source": [
        "# # prob_xy = tf.keras.Sequential([model_xy, tf.keras.layers.Softmax()])\n",
        "# # y_pred_xy = prob_xy.predict(X_test_xy)\n",
        "# y_pred_xy = model_xy.predict(X_test_xy)\n",
        "# y_pred = np.argmax(y_pred_xy, axis=1)\n",
        "# print(classification_report(y_test_xy.ravel(), y_pred, zero_division=0))\n",
        "# prc_xy = precision_score(y_test_xy.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "emotional-chrome",
      "metadata": {
        "id": "emotional-chrome"
      },
      "outputs": [],
      "source": [
        "# model_yz = get_model()\n",
        "# X_train_yz, y_train_yz = shuffle(X_train_yz, y_train_yz)\n",
        "# history_yz = model_yz.fit(X_train_yz, y_train_yz, validation_data=(X_test_yz, y_test_yz), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "boring-insurance",
      "metadata": {
        "id": "boring-insurance"
      },
      "outputs": [],
      "source": [
        "# # prob_yz = tf.keras.Sequential([model_yz, tf.keras.layers.Softmax()])\n",
        "# # y_pred_yz = prob_yz.predict(X_test_yz)\n",
        "# y_pred_yz = model_yz.predict(X_test_yz)\n",
        "# y_pred = np.argmax(y_pred_yz, axis=1)\n",
        "# print(classification_report(y_test_yz.ravel(), y_pred, zero_division=0))\n",
        "# prc_yz = precision_score(y_test_yz.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "joint-evaluation",
      "metadata": {
        "id": "joint-evaluation"
      },
      "outputs": [],
      "source": [
        "# model_zx = get_model()\n",
        "# X_train_zx, y_train_zx = shuffle(X_train_zx, y_train_zx)\n",
        "# history_zx = model_zx.fit(X_train_zx, y_train_zx, validation_data=(X_test_zx, y_test_zx), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "designed-still",
      "metadata": {
        "id": "designed-still"
      },
      "outputs": [],
      "source": [
        "# # prob_zx = tf.keras.Sequential([model_zx, tf.keras.layers.Softmax()])\n",
        "# # y_pred_zx = prob_zx.predict(X_test_zx)\n",
        "# y_pred_zx = model_zx.predict(X_test_zx)\n",
        "# y_pred = np.argmax(y_pred_zx, axis=1)\n",
        "# print(classification_report(y_test_zx.ravel(), y_pred, zero_division=0))\n",
        "# prc_zx = precision_score(y_test_zx.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "selective-geography",
      "metadata": {
        "id": "selective-geography"
      },
      "outputs": [],
      "source": [
        "# y_total = y_pred_xy + y_pred_yz + y_pred_zx\n",
        "# y_pred = np.argmax(y_total, axis=1)\n",
        "# report = classification_report(y_test_xy.ravel(), y_pred, zero_division=0)\n",
        "# print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collected-palmer",
      "metadata": {
        "id": "collected-palmer"
      },
      "outputs": [],
      "source": [
        "# config = '\\n\\nTEST_USER ' + TEST_USER + ' T: ' + str(int(time.time())) + '\\n'\n",
        "# underline = '=====================================\\n'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'logs_sptl_bw' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(underline)\n",
        "# f.write(report)\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intellectual-lunch",
      "metadata": {
        "id": "intellectual-lunch"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_xy' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_xy, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trying-thread",
      "metadata": {
        "id": "trying-thread"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_yz' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_yz, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "general-plant",
      "metadata": {
        "id": "general-plant"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_zx' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_zx, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Spatial_Path_Transfer_Learning_BW.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}