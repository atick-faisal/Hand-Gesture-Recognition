{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/Hand-Gesture-Recognition/blob/dev/Spatial-Path-Analysis/Spatial_Path_Transfer_Learning_BW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "tamil-chrome",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tamil-chrome",
        "outputId": "ad53260b-2adc-4709-9d46-f580578cc775"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import time\n",
        "import joblib\n",
        "import shutil\n",
        "import tarfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, confusion_matrix\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, GlobalAvgPool2D, Dropout, Flatten, concatenate\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "MfafORnwZkyT",
      "metadata": {
        "id": "MfafORnwZkyT"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "casual-stack",
      "metadata": {
        "id": "casual-stack"
      },
      "outputs": [],
      "source": [
        "# -------- TEST USER ----------- #\n",
        "\n",
        "TEST_USER      = '010'\n",
        "DATASET_ID     = '1p0CSRb9gax0sKqdyzOYVt-BXvZ4GtrBv'\n",
        "\n",
        "# BASE_DIR       = '../Dataset/'\n",
        "\n",
        "# Google Drive\n",
        "BASE_DIR       = '.'\n",
        "DATA_DIR       = 'Sensor-Data/'\n",
        "BW_IMG_DIR     = 'BW-Spatial-Path-Images/'\n",
        "RGB_IMG_DIR    = 'RGB-Spatial-Path-Images/'\n",
        "CHANNELS_DIR   = 'Channels/'\n",
        "IMG_SIZE       = (3, 3) # INCHES\n",
        "\n",
        "IMG_DIR        = 'BW-Spatial-Path-Images/'\n",
        "LOG_DIR        = 'Logs/'\n",
        "\n",
        "USERS          = ['001', '002', '003', '004', '005', '006', '007', '008', '009',\n",
        "                  '010', '011', '012', '013', '014', '015', '016', '017', '018',\n",
        "                  '019', '020', '021', '022', '023', '024', '025']\n",
        "\n",
        "# ------------------------------- Only Dynalic Gestures ------------------------------ #\n",
        "GESTURES       = ['j', 'z', 'bad', 'deaf', 'fine', 'good', 'goodbye', 'hello', 'hungry',\n",
        "                  'me', 'no', 'please', 'sorry', 'thankyou', 'yes', 'you']\n",
        "\n",
        "PLANES         = ['XY', 'YZ', 'ZX']\n",
        "\n",
        "BATCH_SIZE     = 32\n",
        "IMG_LEN        = 224\n",
        "IMG_SIZE       = (IMG_LEN, IMG_LEN)\n",
        "\n",
        "# ------------- FOR THE GREATER GOOD :) ------------- #\n",
        "DATASET_LEN    = 4000\n",
        "TRAIN_LEN      = 3840\n",
        "TEST_LEN       = 160\n",
        "\n",
        "EPOCHS         = 50\n",
        "LEARNING_RATE  = 0.001\n",
        "DECAY          = 0.0\n",
        "\n",
        "DT             = 0.01\n",
        "SHAPES         = 100\n",
        "CUT_OFF        = 3.0\n",
        "ORDER          = 4\n",
        "FS             = 100\n",
        "\n",
        "WINDOW_LEN     = 150\n",
        "\n",
        "CONFIG         = '_L_7_S_160x160_E_7'\n",
        "CHANNELS_GROUP = 'DYNAMIC_ACC_ONLY_'\n",
        "\n",
        "XY_WEIGHTS     = np.array([0.91, 0.75, 0.61, 0.63, 0.51, 0.66, 0.81, 0.65, 0.65, 0.31,\n",
        "                           0.66, 0.29, 0.34, 0.64, 0.64, 0.31])\n",
        "YZ_WEIGHTS     = np.array([0.73, 0.71, 0.70, 0.79, 0.76, 0.38, 0.80, 0.61, 0.58, 0.73,\n",
        "                           0.49, 0.26, 0.26, 0.52, 0.59, 0.54])\n",
        "ZX_WEIGHTS     = np.array([0.33, 0.66, 0.51, 0.54, 0.37, 0.51, 0.71, 0.30, 0.75, 0.41,\n",
        "                           0.40, 0.27, 0.24, 0.61, 0.36, 0.49])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e65322ba",
      "metadata": {
        "id": "e65322ba"
      },
      "outputs": [],
      "source": [
        "class LowPassFilter(object): \n",
        "    def butter_lowpass(cutoff, fs, order):\n",
        "        nyq = 0.5 * fs\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "        return b, a\n",
        "\n",
        "    def apply(data, cutoff=CUT_OFF, fs=FS, order=ORDER):\n",
        "        b, a = LowPassFilter.butter_lowpass(cutoff, fs, order=order)\n",
        "        y = lfilter(b, a, data)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ksdY7MgcqHNr",
      "metadata": {
        "id": "ksdY7MgcqHNr"
      },
      "outputs": [],
      "source": [
        "#--------------------- Download util for Google Drive ------------------- #\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "        \n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "        \n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(fid, destination):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(destination)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "        \n",
        "    print('creating data directory ... ', end='')\n",
        "    os.mkdir(destination)\n",
        "    print('√')\n",
        "    \n",
        "    print('downloading dataset from the repository ... ', end='')\n",
        "    filename = os.path.join(destination, 'dataset.tar.xz')\n",
        "    try:\n",
        "        download_file_from_google_drive(fid, filename)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "        \n",
        "    print('extracting the dataset ... ', end='')\n",
        "    try:\n",
        "        tar = tarfile.open(filename)\n",
        "        tar.extractall(destination)\n",
        "        tar.close()\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "MtE4QJm-qOZY",
      "metadata": {
        "id": "MtE4QJm-qOZY"
      },
      "outputs": [],
      "source": [
        "# # ------- Comment This if already downloaded -------- #\n",
        "\n",
        "# destination = os.path.join(BASE_DIR, DATA_DIR)\n",
        "# download_data(DATASET_ID, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f9d84aa1",
      "metadata": {
        "id": "f9d84aa1"
      },
      "outputs": [],
      "source": [
        "def clean_dir(path):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "    \n",
        "    print('creating ' + path + ' directory ... ', end='')\n",
        "    os.mkdir(path)\n",
        "    print('√')\n",
        "\n",
        "def extract_channels():\n",
        "    channels_dir = os.path.join(BASE_DIR, CHANNELS_DIR)\n",
        "    clean_dir(channels_dir)\n",
        "        \n",
        "    for user in USERS:\n",
        "    # for gesture in GESTURES:\n",
        "        print('Processing data for user ' + user, end=' ')\n",
        "        # print(f\"processing data for gesture {gesture} \", end=\"...\" )\n",
        "        \n",
        "        X = []\n",
        "        y = []\n",
        "        first_time = True\n",
        "        \n",
        "        for gesture in GESTURES:\n",
        "        # for user in USERS:\n",
        "              \n",
        "            user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
        "            gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
        "\n",
        "            dataset = pd.read_csv(gesture_dir)\n",
        "\n",
        "            dataset['flex_1'] = dataset['flex_1'].rolling(3).median()\n",
        "            dataset['flex_2'] = dataset['flex_2'].rolling(3).median()\n",
        "            dataset['flex_3'] = dataset['flex_3'].rolling(3).median()\n",
        "            dataset['flex_4'] = dataset['flex_4'].rolling(3).median()\n",
        "            dataset['flex_5'] = dataset['flex_5'].rolling(3).median()\n",
        "\n",
        "            dataset.fillna(0, inplace=True)\n",
        "\n",
        "            # flex = ['flex_1', 'flex_2', 'flex_3', 'flex_4', 'flex_5']\n",
        "            # max_flex = dataset[flex].max(axis=1)\n",
        "            # max_flex.replace(0, 1, inplace=True)\n",
        "            # dataset[flex] = dataset[flex].divide(max_flex, axis=0)\n",
        "            \n",
        "            flx1 = dataset['flex_1'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx2 = dataset['flex_2'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx3 = dataset['flex_3'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx4 = dataset['flex_4'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx5 = dataset['flex_5'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            accx = dataset['ACCx'].to_numpy()\n",
        "            accy = dataset['ACCy'].to_numpy()\n",
        "            accz = dataset['ACCz'].to_numpy()\n",
        "            \n",
        "            accx = LowPassFilter.apply(accx).reshape(-1, WINDOW_LEN)\n",
        "            accy = LowPassFilter.apply(accy).reshape(-1, WINDOW_LEN)\n",
        "            accz = LowPassFilter.apply(accz).reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            gyrx = dataset['GYRx'].to_numpy()\n",
        "            gyry = dataset['GYRy'].to_numpy()\n",
        "            gyrz = dataset['GYRz'].to_numpy()\n",
        "            \n",
        "            gyrx = LowPassFilter.apply(gyrx).reshape(-1, WINDOW_LEN)\n",
        "            gyry = LowPassFilter.apply(gyry).reshape(-1, WINDOW_LEN)\n",
        "            gyrz = LowPassFilter.apply(gyrz).reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            accm = np.sqrt(accx ** 2 + accy ** 2 + accz ** 2)\n",
        "            gyrm = np.sqrt(gyrx ** 2 + gyry ** 2 + gyrz ** 2)\n",
        "            \n",
        "            g_idx = GESTURES.index(gesture)\n",
        "            labels = np.ones((accx.shape[0], 1)) * g_idx\n",
        "            \n",
        "            channels = np.stack([\n",
        "                flx1, flx2, flx3, flx4, flx5,\n",
        "                accx, accy, accz\n",
        "            ], axis=-1)\n",
        "            \n",
        "            if first_time == True:\n",
        "                X = channels\n",
        "                y = labels\n",
        "                first_time = False\n",
        "            else:\n",
        "                X = np.append(X, channels, axis=0)\n",
        "                y = np.append(y, labels, axis=0)\n",
        "            \n",
        "        \n",
        "        x_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_X.joblib')\n",
        "        y_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_y.joblib')\n",
        "        joblib.dump(X, x_path)\n",
        "        joblib.dump(y, y_path)\n",
        "        \n",
        "        print('√')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "QmReRMF-qQcl",
      "metadata": {
        "id": "QmReRMF-qQcl"
      },
      "outputs": [],
      "source": [
        "# ------------- Spatial Path Image Generation ----------- #\n",
        "\n",
        "def clean_dir(path):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "    \n",
        "    print('creating ' + path + ' directory ... ', end='')\n",
        "    os.mkdir(path)\n",
        "    print('√')\n",
        "    \n",
        "# ----------- Spatial Path Vector Calculation ----------- #\n",
        "\n",
        "def get_displacement(acc):\n",
        "    v = np.zeros(acc.shape)\n",
        "    d = np.zeros(acc.shape)\n",
        "    for i in range(acc.shape[0] - 1):\n",
        "        v[i + 1] = v[i] + acc[i] * DT\n",
        "        d[i + 1] = v[i] * DT + 0.5 * acc[i] * DT * DT\n",
        "        \n",
        "    return d\n",
        "\n",
        "def write_image(x, y, path):\n",
        "    fig, ax = plt.subplots(frameon=True, figsize=(3, 3))\n",
        "    ax.axis('off')\n",
        "    plt.scatter(x, y, s=SHAPES, c='black')\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "def generate_bw_images():\n",
        "    image_dir = os.path.join(BASE_DIR, BW_IMG_DIR)\n",
        "    clean_dir(image_dir)\n",
        "    \n",
        "    for plane in PLANES:\n",
        "        print('processing spatial path images for ' + plane + ' plane ... ', end='')\n",
        "        plane_dir = os.path.join(image_dir, plane)\n",
        "        os.mkdir(plane_dir)\n",
        "        \n",
        "        # for gesture in GESTURES:\n",
        "        #     os.mkdir(os.path.join(plane_dir, gesture))\n",
        "\n",
        "        for user in USERS:\n",
        "            os.mkdir(os.path.join(plane_dir, user))\n",
        "    \n",
        "            # for user in USERS:\n",
        "            for gesture in GESTURES:\n",
        "                os.mkdir(os.path.join(plane_dir, user, gesture))\n",
        "                user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
        "                gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
        "                \n",
        "                accx = pd.read_csv(gesture_dir)['ACCx'].to_numpy()\n",
        "                accy = pd.read_csv(gesture_dir)['ACCy'].to_numpy()\n",
        "                accz = pd.read_csv(gesture_dir)['ACCz'].to_numpy()\n",
        "\n",
        "                x = get_displacement(accx).reshape(-1, 150)\n",
        "                y = get_displacement(accy).reshape(-1, 150)\n",
        "                z = get_displacement(accz).reshape(-1, 150)\n",
        "\n",
        "                count = 0\n",
        "                for i in range(x.shape[0]):\n",
        "                    # image_name = 'u' + user + '_g' + '{:0>2d}'.format(GESTURES.index(gesture)) + \\\n",
        "                    #              '_s' + '{:0>7d}'.format(count) + '_p' + plane + '.jpg'\n",
        "                    image_name = \"{:0>7d}\".format(count) + \".jpg\"\n",
        "                    path = os.path.join(plane_dir, user, gesture, image_name)\n",
        "                    \n",
        "                    if plane == 'XY':\n",
        "                        write_image(x[i, :], y[i, :], path)\n",
        "                    elif plane == 'YZ':\n",
        "                        write_image(y[i, :], z[i, :], path)\n",
        "                    else:\n",
        "                        write_image(z[i, :], x[i, :], path)\n",
        "\n",
        "                    count = count + 1\n",
        "            \n",
        "        print('√')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "07jhA8ukqScv",
      "metadata": {
        "id": "07jhA8ukqScv"
      },
      "outputs": [],
      "source": [
        "# extract_channels()\n",
        "# generate_bw_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "stylish-banner",
      "metadata": {
        "id": "stylish-banner"
      },
      "outputs": [],
      "source": [
        "def load_data(plane, test_user):\n",
        "    X_train = np.zeros((TRAIN_LEN, IMG_LEN, IMG_LEN, 3), dtype='uint8')\n",
        "    X_test = np.zeros((TEST_LEN, IMG_LEN, IMG_LEN, 3), dtype='uint8')\n",
        "    y_train = np.zeros((TRAIN_LEN, 1), dtype='uint8')\n",
        "    y_test = np.zeros((TEST_LEN, 1), dtype='uint8')\n",
        "    \n",
        "    train_count = 0\n",
        "    test_count = 0\n",
        "        \n",
        "    # for gesture in GESTURES:\n",
        "    for user in USERS:\n",
        "        print('loading data for user ' + user + ' on the ' + plane + ' plane ... ', end='')\n",
        "        user_dir = os.path.join(BASE_DIR, IMG_DIR, plane, user)\n",
        "\n",
        "        for gesture in GESTURES:\n",
        "            gesture_dir = os.path.join(user_dir, gesture)\n",
        "\n",
        "            # for filename in os.listdir(path):\n",
        "\n",
        "            for count in range(10):\n",
        "                image_name = \"{:0>7d}\".format(count) + \".jpg\"\n",
        "                img = cv2.imread(os.path.join(gesture_dir, image_name))\n",
        "                resized = cv2.resize(img, IMG_SIZE)\n",
        "                # resized = np.expand_dims(resized, axis=-1)\n",
        "                # label = int(filename[6:8])\n",
        "                if user != test_user:\n",
        "                    X_train[train_count, :] = resized\n",
        "                    y_train[train_count, 0] = GESTURES.index(gesture)\n",
        "                    train_count = train_count + 1\n",
        "                else:\n",
        "                    X_test[test_count, :] = resized\n",
        "                    y_test[test_count, 0] = GESTURES.index(gesture)\n",
        "                    test_count = test_count + 1\n",
        "                \n",
        "        print('√')\n",
        "        \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def load_and_save_data(plane):\n",
        "    X = np.zeros((DATASET_LEN, IMG_LEN, IMG_LEN, 1))\n",
        "    y = np.zeros((DATASET_LEN, 1))\n",
        "    \n",
        "    train_count = 0\n",
        "    test_count  = TRAIN_LEN\n",
        "        \n",
        "    for gesture in GESTURES:\n",
        "        print('loading data for ' + gesture + ' gesture on the ' + plane + ' plane ... ', end='')\n",
        "        path = os.path.join(BASE_DIR, IMG_DIR, plane, gesture)\n",
        "        for filename in os.listdir(path):\n",
        "            img = cv2.imread(os.path.join(path, filename), cv2.IMREAD_GRAYSCALE)\n",
        "            resized = cv2.resize(img, IMG_SIZE)\n",
        "            if filename[1:4] != TEST_USER:\n",
        "                X[train_count, :] = resized\n",
        "                y[train_count, 0] = GESTURES.index(gesture)\n",
        "                train_count = train_count + 1\n",
        "            else:\n",
        "                X[test_count, :] = resized\n",
        "                y[test_count, 0] = GESTURES.index(gesture)\n",
        "                test_count = test_count + 1\n",
        "                \n",
        "        print('√')\n",
        "\n",
        "    joblib.dump(X, BASE_DIR + 'X_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    joblib.dump(y, BASE_DIR + 'Y_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "\n",
        "def load_data_from_joblib(plane):\n",
        "    print('Loading data for ' + plane + ' plane ... ', end='')\n",
        "    X = joblib.load(BASE_DIR + 'X_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    y = joblib.load(BASE_DIR + 'Y_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    test_user = int(TEST_USER)\n",
        "    X_train = X[:TRAIN_LEN, :, :, :]\n",
        "    y_train = y[:TRAIN_LEN, :]\n",
        "    X_test = X[TRAIN_LEN:, :, :, :]\n",
        "    y_test = y[TRAIN_LEN:, :]\n",
        "\n",
        "    print('√')\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "extra-disclosure",
      "metadata": {
        "id": "extra-disclosure"
      },
      "outputs": [],
      "source": [
        "# X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data('XY')\n",
        "# X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data('YZ')\n",
        "# X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data('ZX')\n",
        "\n",
        "# Save to Google  Drive\n",
        "# load_and_save_data('XY')X_train_xy, y_train_xy = shuffle(X_train_xy, y_train_xy)\n",
        "# load_and_save_data('YZ')\n",
        "# load_and_save_data('ZX')\n",
        "\n",
        "# Load from Google Drive\n",
        "# X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data_from_joblib('XY')\n",
        "# X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data_from_joblib('YZ')\n",
        "# X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data_from_joblib('ZX')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "conditional-advisory",
      "metadata": {
        "id": "conditional-advisory"
      },
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.nasnet.preprocess_input\n",
        "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "internal-arkansas",
      "metadata": {
        "id": "internal-arkansas"
      },
      "outputs": [],
      "source": [
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "\n",
        "# ... Change Model\n",
        "base_model = tf.keras.applications.NASNetMobile(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "strange-encoding",
      "metadata": {
        "id": "strange-encoding"
      },
      "outputs": [],
      "source": [
        "global_average_layer = GlobalAvgPool2D()\n",
        "prediction_layer = Dense(len(GESTURES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6e049e9a",
      "metadata": {
        "id": "6e049e9a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers.pooling import MaxPooling2D\n",
        "\n",
        "def get_conv_block_1D():\n",
        "    input = Input(shape=(150, 1))\n",
        "    x = BatchNormalization()(input)\n",
        "    x = Conv1D(filters=8, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(50, activation='relu')(x)\n",
        "\n",
        "    return input, x\n",
        "\n",
        "def get_conv_block_2D():\n",
        "    input = tf.keras.layers.Input(shape=IMG_SHAPE)\n",
        "    # x = data_augmentation(input)\n",
        "    x = preprocess_input(input)\n",
        "    x = base_model(x, training=False)\n",
        "    x = global_average_layer(x)\n",
        "\n",
        "    # x = layers.Conv2D(16, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),)(\n",
        "    #     input\n",
        "    # )\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.MaxPooling2D()(x)\n",
        "    # x = layers.Conv2D(32, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),)(\n",
        "    #     x\n",
        "    # )\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.MaxPooling2D()(x)\n",
        "    # x = layers.Conv2D(\n",
        "    #     64, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),\n",
        "    # )(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.Flatten()(x)\n",
        "\n",
        "    return input, x\n",
        "\n",
        "\n",
        "def get_stacked_model():\n",
        "    inputs = []\n",
        "    CNNs = []\n",
        "\n",
        "    for i in range(5):\n",
        "        input_i, CNN_i = get_conv_block_1D()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    for i in range(3):\n",
        "        input_i, CNN_i = get_conv_block_2D()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    x = concatenate(CNNs, axis=-1)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    output = Dense(16, activation=\"softmax\")(x)\n",
        "    model = Model(inputs, output)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
        "    model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "WTWKoIlYyzKW",
      "metadata": {
        "id": "WTWKoIlYyzKW",
        "outputId": "4d395a54-9c30-4849-efbe-1f4d966a8b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 150, 1)      4           ['input_2[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 150, 1)      4           ['input_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 150, 1)      4           ['input_4[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 150, 1)      4           ['input_5[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 150, 1)      4           ['input_6[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 148, 8)       32          ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 148, 8)       32          ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 148, 8)       32          ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 148, 8)       32          ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 148, 8)       32          ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 146, 16)      400         ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 146, 16)      400         ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 146, 16)      400         ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 146, 16)      400         ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 146, 16)      400         ['conv1d_16[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 73, 16)       0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_13[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 71, 16)       784         ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 71, 16)       784         ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_6[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_8[0][0]']        \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_9 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 69, 16)       784         ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 69, 16)       784         ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 69, 16)       784         ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 69, 16)       784         ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 69, 16)       784         ['conv1d_18[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.truediv (TFOpLambda)   (None, 224, 224, 3)  0           ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.truediv_1 (TFOpLambda)  (None, 224, 224, 3)  0          ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.truediv_2 (TFOpLambda)  (None, 224, 224, 3)  0          ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_11[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_15[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_19[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLambda)  (None, 224, 224, 3)  0           ['tf.math.truediv[0][0]']        \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLambda  (None, 224, 224, 3)  0          ['tf.math.truediv_1[0][0]']      \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.subtract_2 (TFOpLambda  (None, 224, 224, 3)  0          ['tf.math.truediv_2[0][0]']      \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 544)          0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 544)          0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 544)          0           ['max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 544)          0           ['max_pooling1d_7[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 544)          0           ['max_pooling1d_9[0][0]']        \n",
            "                                                                                                  \n",
            " NASNet (Functional)            (None, 7, 7, 1056)   4269716     ['tf.math.subtract[0][0]',       \n",
            "                                                                  'tf.math.subtract_1[0][0]',     \n",
            "                                                                  'tf.math.subtract_2[0][0]']     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 50)           27250       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 50)           27250       ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 50)           27250       ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 50)           27250       ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 50)           27250       ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 1056)        0           ['NASNet[0][0]',                 \n",
            " alAveragePooling2D)                                              'NASNet[1][0]',                 \n",
            "                                                                  'NASNet[2][0]']                 \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 3418)         0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_2[0][0]',                \n",
            "                                                                  'dense_3[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'dense_5[0][0]',                \n",
            "                                                                  'global_average_pooling2d[0][0]'\n",
            "                                                                 , 'global_average_pooling2d[1][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_average_pooling2d[2][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 3418)         0           ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          437632      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 16)           2064        ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,855,682\n",
            "Trainable params: 585,956\n",
            "Non-trainable params: 4,269,726\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_stacked_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "upper-tobago",
      "metadata": {
        "id": "upper-tobago"
      },
      "outputs": [],
      "source": [
        "# def get_model():\n",
        "#     inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
        "#     #     x = data_augmentation(inputs)\n",
        "#     x = preprocess_input(inputs)\n",
        "#     x = base_model(x, training=False)\n",
        "#     x = global_average_layer(x)\n",
        "#     x = tf.keras.layers.Dropout(0.2)(x)\n",
        "#     outputs = prediction_layer(x)\n",
        "#     model = tf.keras.Model(inputs, outputs)\n",
        "#     model.compile(\n",
        "#         optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE, decay=DECAY),\n",
        "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#         metrics=[\"accuracy\"],\n",
        "#     )\n",
        "#     return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a457f4",
      "metadata": {
        "id": "b3a457f4",
        "outputId": "e88d49b2-2b54-4de1-e13d-c932357bf458",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing results for user 017... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 77s 619ms/step - loss: 2.6371 - accuracy: 0.1714 - val_loss: 1.7790 - val_accuracy: 0.8125\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 24s 397ms/step - loss: 1.8010 - accuracy: 0.4461 - val_loss: 0.7188 - val_accuracy: 0.9375\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 22s 366ms/step - loss: 1.0997 - accuracy: 0.6714 - val_loss: 0.3647 - val_accuracy: 0.9375\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 22s 368ms/step - loss: 0.7485 - accuracy: 0.7758 - val_loss: 0.2244 - val_accuracy: 0.9563\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 22s 364ms/step - loss: 0.5904 - accuracy: 0.8214 - val_loss: 0.1544 - val_accuracy: 0.9688\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.4856 - accuracy: 0.8516 - val_loss: 0.1225 - val_accuracy: 0.9688\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 21s 357ms/step - loss: 0.3813 - accuracy: 0.8883 - val_loss: 0.0967 - val_accuracy: 0.9688\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 22s 365ms/step - loss: 0.3502 - accuracy: 0.8909 - val_loss: 0.0822 - val_accuracy: 0.9688\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 22s 366ms/step - loss: 0.3162 - accuracy: 0.9049 - val_loss: 0.0695 - val_accuracy: 0.9688\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 22s 364ms/step - loss: 0.2818 - accuracy: 0.9120 - val_loss: 0.0646 - val_accuracy: 0.9750\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 22s 363ms/step - loss: 0.2495 - accuracy: 0.9221 - val_loss: 0.0467 - val_accuracy: 0.9937\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 21s 359ms/step - loss: 0.2316 - accuracy: 0.9263 - val_loss: 0.0419 - val_accuracy: 0.9875\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.2079 - accuracy: 0.9380 - val_loss: 0.0439 - val_accuracy: 0.9937\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.2118 - accuracy: 0.9344 - val_loss: 0.0574 - val_accuracy: 0.9812\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.1940 - accuracy: 0.9378 - val_loss: 0.0402 - val_accuracy: 0.9937\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 22s 365ms/step - loss: 0.1752 - accuracy: 0.9482 - val_loss: 0.0346 - val_accuracy: 0.9937\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.1626 - accuracy: 0.9490 - val_loss: 0.0417 - val_accuracy: 0.9812\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.1535 - accuracy: 0.9518 - val_loss: 0.0441 - val_accuracy: 0.9750\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.1410 - accuracy: 0.9570 - val_loss: 0.0437 - val_accuracy: 0.9812\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 22s 366ms/step - loss: 0.1407 - accuracy: 0.9599 - val_loss: 0.0390 - val_accuracy: 0.9812\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 22s 366ms/step - loss: 0.1348 - accuracy: 0.9599 - val_loss: 0.0372 - val_accuracy: 0.9812\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 22s 365ms/step - loss: 0.1238 - accuracy: 0.9620 - val_loss: 0.0312 - val_accuracy: 0.9937\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.1130 - accuracy: 0.9659 - val_loss: 0.0309 - val_accuracy: 0.9937\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.1102 - accuracy: 0.9669 - val_loss: 0.0314 - val_accuracy: 0.9875\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.1146 - accuracy: 0.9651 - val_loss: 0.0302 - val_accuracy: 0.9937\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.1026 - accuracy: 0.9688 - val_loss: 0.0296 - val_accuracy: 0.9937\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.1016 - accuracy: 0.9719 - val_loss: 0.0353 - val_accuracy: 0.9812\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.0981 - accuracy: 0.9714 - val_loss: 0.0266 - val_accuracy: 0.9937\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.0962 - accuracy: 0.9708 - val_loss: 0.0327 - val_accuracy: 0.9875\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 22s 366ms/step - loss: 0.0961 - accuracy: 0.9695 - val_loss: 0.0252 - val_accuracy: 0.9937\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 22s 366ms/step - loss: 0.0855 - accuracy: 0.9742 - val_loss: 0.0516 - val_accuracy: 0.9688\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.0791 - accuracy: 0.9773 - val_loss: 0.0270 - val_accuracy: 0.9937\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.0831 - accuracy: 0.9758 - val_loss: 0.0372 - val_accuracy: 0.9812\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.0851 - accuracy: 0.9724 - val_loss: 0.0269 - val_accuracy: 0.9937\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.0773 - accuracy: 0.9763 - val_loss: 0.0290 - val_accuracy: 0.9812\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.0760 - accuracy: 0.9776 - val_loss: 0.0284 - val_accuracy: 0.9812\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.0772 - accuracy: 0.9727 - val_loss: 0.0325 - val_accuracy: 0.9812\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.0749 - accuracy: 0.9753 - val_loss: 0.0246 - val_accuracy: 0.9937\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 22s 366ms/step - loss: 0.0633 - accuracy: 0.9820 - val_loss: 0.0271 - val_accuracy: 0.9937\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.0621 - accuracy: 0.9828 - val_loss: 0.0317 - val_accuracy: 0.9937\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 22s 372ms/step - loss: 0.0751 - accuracy: 0.9760 - val_loss: 0.0166 - val_accuracy: 0.9937\n",
            "3/3 [==============================] - 1s 278ms/step - loss: 0.0467 - accuracy: 0.9937\n",
            "99.37 %\n",
            "Processing results for user 018... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 73s 580ms/step - loss: 2.6438 - accuracy: 0.1716 - val_loss: 1.9003 - val_accuracy: 0.8188\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 23s 376ms/step - loss: 1.8319 - accuracy: 0.4448 - val_loss: 0.8413 - val_accuracy: 0.9625\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 22s 371ms/step - loss: 1.0917 - accuracy: 0.6648 - val_loss: 0.3531 - val_accuracy: 0.9875\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 21s 357ms/step - loss: 0.7559 - accuracy: 0.7625 - val_loss: 0.2344 - val_accuracy: 0.9812\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.5974 - accuracy: 0.8169 - val_loss: 0.1919 - val_accuracy: 0.9812\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 22s 368ms/step - loss: 0.4724 - accuracy: 0.8557 - val_loss: 0.1376 - val_accuracy: 0.9812\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 22s 367ms/step - loss: 0.4229 - accuracy: 0.8745 - val_loss: 0.1198 - val_accuracy: 0.9812\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.3728 - accuracy: 0.8885 - val_loss: 0.1213 - val_accuracy: 0.9750\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.3175 - accuracy: 0.9034 - val_loss: 0.1134 - val_accuracy: 0.9750\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.2820 - accuracy: 0.9169 - val_loss: 0.1061 - val_accuracy: 0.9750\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 22s 367ms/step - loss: 0.2652 - accuracy: 0.9180 - val_loss: 0.1045 - val_accuracy: 0.9750\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.2348 - accuracy: 0.9312 - val_loss: 0.0880 - val_accuracy: 0.9750\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 22s 367ms/step - loss: 0.2116 - accuracy: 0.9385 - val_loss: 0.1061 - val_accuracy: 0.9750\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 22s 367ms/step - loss: 0.2102 - accuracy: 0.9380 - val_loss: 0.1039 - val_accuracy: 0.9750\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 22s 366ms/step - loss: 0.1891 - accuracy: 0.9411 - val_loss: 0.0956 - val_accuracy: 0.9750\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 22s 367ms/step - loss: 0.1936 - accuracy: 0.9398 - val_loss: 0.1160 - val_accuracy: 0.9688\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.1693 - accuracy: 0.9516 - val_loss: 0.0923 - val_accuracy: 0.9750\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.1631 - accuracy: 0.9508 - val_loss: 0.1134 - val_accuracy: 0.9688\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.1540 - accuracy: 0.9576 - val_loss: 0.1066 - val_accuracy: 0.9688\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.1457 - accuracy: 0.9570 - val_loss: 0.1082 - val_accuracy: 0.9688\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.1378 - accuracy: 0.9646 - val_loss: 0.1045 - val_accuracy: 0.9750\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.1339 - accuracy: 0.9586 - val_loss: 0.0908 - val_accuracy: 0.9750\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 22s 366ms/step - loss: 0.1245 - accuracy: 0.9609 - val_loss: 0.1409 - val_accuracy: 0.9625\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.1262 - accuracy: 0.9628 - val_loss: 0.1028 - val_accuracy: 0.9750\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.1110 - accuracy: 0.9682 - val_loss: 0.0903 - val_accuracy: 0.9750\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.1038 - accuracy: 0.9724 - val_loss: 0.0814 - val_accuracy: 0.9750\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.1053 - accuracy: 0.9695 - val_loss: 0.0899 - val_accuracy: 0.9750\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.1014 - accuracy: 0.9703 - val_loss: 0.0833 - val_accuracy: 0.9750\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.0950 - accuracy: 0.9737 - val_loss: 0.1033 - val_accuracy: 0.9750\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.0986 - accuracy: 0.9724 - val_loss: 0.0914 - val_accuracy: 0.9750\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.0853 - accuracy: 0.9747 - val_loss: 0.0971 - val_accuracy: 0.9750\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.0823 - accuracy: 0.9760 - val_loss: 0.1033 - val_accuracy: 0.9750\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 22s 373ms/step - loss: 0.0853 - accuracy: 0.9755 - val_loss: 0.0861 - val_accuracy: 0.9750\n",
            "3/3 [==============================] - 1s 273ms/step - loss: 0.3531 - accuracy: 0.9875\n",
            "98.75 %\n",
            "Processing results for user 019... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 73s 579ms/step - loss: 2.6204 - accuracy: 0.1747 - val_loss: 1.8157 - val_accuracy: 0.6875\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 22s 368ms/step - loss: 1.8140 - accuracy: 0.4523 - val_loss: 0.7980 - val_accuracy: 0.9563\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 22s 370ms/step - loss: 1.0803 - accuracy: 0.6807 - val_loss: 0.3616 - val_accuracy: 0.9688\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 21s 356ms/step - loss: 0.7650 - accuracy: 0.7612 - val_loss: 0.2550 - val_accuracy: 0.9563\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 22s 364ms/step - loss: 0.5893 - accuracy: 0.8156 - val_loss: 0.1848 - val_accuracy: 0.9750\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.4874 - accuracy: 0.8458 - val_loss: 0.1563 - val_accuracy: 0.9500\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.4169 - accuracy: 0.8732 - val_loss: 0.1516 - val_accuracy: 0.9625\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.3692 - accuracy: 0.8865 - val_loss: 0.1455 - val_accuracy: 0.9688\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.3105 - accuracy: 0.9008 - val_loss: 0.1544 - val_accuracy: 0.9438\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.2859 - accuracy: 0.9115 - val_loss: 0.1171 - val_accuracy: 0.9750\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.2651 - accuracy: 0.9195 - val_loss: 0.1126 - val_accuracy: 0.9688\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 21s 359ms/step - loss: 0.2286 - accuracy: 0.9315 - val_loss: 0.1224 - val_accuracy: 0.9563\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.2182 - accuracy: 0.9357 - val_loss: 0.0941 - val_accuracy: 0.9688\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 21s 359ms/step - loss: 0.2019 - accuracy: 0.9367 - val_loss: 0.1060 - val_accuracy: 0.9750\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.1894 - accuracy: 0.9435 - val_loss: 0.1206 - val_accuracy: 0.9625\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.1755 - accuracy: 0.9471 - val_loss: 0.1058 - val_accuracy: 0.9563\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.1681 - accuracy: 0.9448 - val_loss: 0.1253 - val_accuracy: 0.9500\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.1512 - accuracy: 0.9549 - val_loss: 0.1167 - val_accuracy: 0.9625\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.1472 - accuracy: 0.9560 - val_loss: 0.0961 - val_accuracy: 0.9625\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.1539 - accuracy: 0.9526 - val_loss: 0.0926 - val_accuracy: 0.9688\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 21s 359ms/step - loss: 0.1333 - accuracy: 0.9615 - val_loss: 0.0883 - val_accuracy: 0.9625\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 21s 358ms/step - loss: 0.1269 - accuracy: 0.9646 - val_loss: 0.0736 - val_accuracy: 0.9625\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 22s 364ms/step - loss: 0.1251 - accuracy: 0.9622 - val_loss: 0.1113 - val_accuracy: 0.9438\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.1180 - accuracy: 0.9641 - val_loss: 0.0860 - val_accuracy: 0.9563\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.1089 - accuracy: 0.9680 - val_loss: 0.0750 - val_accuracy: 0.9688\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.1137 - accuracy: 0.9669 - val_loss: 0.0721 - val_accuracy: 0.9688\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.1025 - accuracy: 0.9693 - val_loss: 0.0685 - val_accuracy: 0.9750\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 22s 361ms/step - loss: 0.0954 - accuracy: 0.9724 - val_loss: 0.0727 - val_accuracy: 0.9688\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.0916 - accuracy: 0.9734 - val_loss: 0.1104 - val_accuracy: 0.9625\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.1020 - accuracy: 0.9706 - val_loss: 0.0824 - val_accuracy: 0.9625\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 22s 359ms/step - loss: 0.0915 - accuracy: 0.9747 - val_loss: 0.0974 - val_accuracy: 0.9625\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 21s 359ms/step - loss: 0.0906 - accuracy: 0.9714 - val_loss: 0.0789 - val_accuracy: 0.9563\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.0854 - accuracy: 0.9714 - val_loss: 0.0722 - val_accuracy: 0.9750\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 22s 360ms/step - loss: 0.0811 - accuracy: 0.9760 - val_loss: 0.0724 - val_accuracy: 0.9625\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 22s 373ms/step - loss: 0.0765 - accuracy: 0.9771 - val_loss: 0.0763 - val_accuracy: 0.9688\n",
            "3/3 [==============================] - 1s 285ms/step - loss: 0.1848 - accuracy: 0.9750\n",
            "97.50 %\n",
            "Processing results for user 020... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ACC = []\n",
        "CM = []\n",
        "HISTORY = []\n",
        "logs = ''\n",
        "\n",
        "# ... Change user limit\n",
        "users = USERS[16:]\n",
        "\n",
        "for test_user in users:\n",
        "    print('Processing results for user ' + test_user, end='... \\n')\n",
        "    \n",
        "    X_train = []\n",
        "    X_test = []\n",
        "    y_train = []\n",
        "    y_test = []\n",
        "    \n",
        "    first_time_train = True\n",
        "    first_time_test = True\n",
        "\n",
        "    for user in USERS:\n",
        "        x_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_X.joblib')\n",
        "        y_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_y.joblib')\n",
        "        X = joblib.load(x_path)\n",
        "        y = joblib.load(y_path)\n",
        "\n",
        "        if user == test_user:\n",
        "            if first_time_train == True:\n",
        "                first_time_train = False\n",
        "                X_test = X\n",
        "                y_test = y\n",
        "                \n",
        "            else:\n",
        "                X_test = np.append(X_test, X, axis=0)\n",
        "                y_test = np.append(y_test, y, axis=0)\n",
        "                \n",
        "        else:\n",
        "            if first_time_test == True:\n",
        "                first_time_test = False\n",
        "                X_train = X\n",
        "                y_train = y\n",
        "                \n",
        "            else:\n",
        "                X_train = np.append(X_train, X, axis=0)\n",
        "                y_train = np.append(y_train, y, axis=0)\n",
        "\n",
        "\n",
        "    # X_train, y_train = shuffle(X_train, y_train)\n",
        "\n",
        "    X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data('XY', test_user)\n",
        "    X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data('YZ', test_user)\n",
        "    X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data('ZX', test_user)\n",
        "\n",
        "    X_train_xy, X_train_yz, X_train_zx, X_train, y_train = shuffle(\n",
        "        X_train_xy, X_train_yz, X_train_zx, X_train, y_train\n",
        "    )\n",
        "\n",
        "    X_train_combined = np.split(X_train, 8, axis=-1)[:5] + [X_train_xy, X_train_yz, X_train_zx]\n",
        "    X_test_combined = np.split(X_test, 8, axis=-1)[:5] + [X_test_xy, X_test_yz, X_test_zx]\n",
        "\n",
        "    del X_train_xy, X_test_xy, y_train_xy, y_test_xy\n",
        "    del X_train_yz, X_test_yz, y_train_yz, y_test_yz\n",
        "    del X_train_zx, X_test_zx, y_train_zx, y_test_zx\n",
        "    del X_train, X_test\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    print(len(X_train_combined))\n",
        "\n",
        "    callbacks = [\n",
        "                 tf.keras.callbacks.EarlyStopping(\n",
        "                    monitor='val_accuracy', \n",
        "                    patience=30, \n",
        "                    mode='max', \n",
        "                    restore_best_weights=True\n",
        "                ),\n",
        "    ]\n",
        "\n",
        "    model = get_stacked_model()\n",
        "    history = model.fit(\n",
        "        X_train_combined, \n",
        "        y_train, \n",
        "        validation_data=(X_test_combined, y_test),\n",
        "        epochs=300, \n",
        "        batch_size=64,\n",
        "        callbacks=[callbacks]\n",
        "    )\n",
        "    _, accuracy = model.evaluate(X_test_combined, y_test, batch_size=64)\n",
        "    y_pred = model.predict(X_test_combined)\n",
        "    cm = confusion_matrix(y_test.ravel(), np.argmax(y_pred, axis=-1))\n",
        "\n",
        "    CM.append(cm)\n",
        "    HISTORY.append(history)\n",
        "\n",
        "    accuracy = accuracy * 100\n",
        "    print(f'%.2f %%' %(accuracy))\n",
        "    logs = logs + 'Accuracy for user ' + str(test_user) + '... ' + str(accuracy) + '\\n'\n",
        "    ACC.append(accuracy)\n",
        "\n",
        "    del model, history\n",
        "    gc.collect()\n",
        "    \n",
        "AVG_ACC = np.mean(ACC)\n",
        "STD = np.std(ACC)\n",
        "print('------------------------------------')\n",
        "print(f'Average accuracy %.2f +/- %.2f' %(AVG_ACC, STD))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "data = {\n",
        "    \"cm\": CM,\n",
        "    \"accuracy\": ACC,\n",
        "    \"logs\": logs\n",
        "}\n",
        "\n",
        "joblib.dump(data, \"user16-25.joblib\")"
      ],
      "metadata": {
        "id": "HeOI8hw3zCXz"
      },
      "id": "HeOI8hw3zCXz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    pass"
      ],
      "metadata": {
        "id": "3N94GWOk9KHE"
      },
      "id": "3N94GWOk9KHE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92fd4333",
      "metadata": {
        "id": "92fd4333"
      },
      "outputs": [],
      "source": [
        "# model = get_stacked_model()\n",
        "# X_train_xy, X_train_yz, X_train_zx, y_train_xy = shuffle(\n",
        "#     X_train_xy, X_train_yz, X_train_zx, y_train_xy\n",
        "# )\n",
        "\n",
        "# history = model.fit(\n",
        "#     [X_train_xy, X_train_yz, X_train_zx],\n",
        "#     y_train_xy,\n",
        "#     validation_data=([X_test_xy, X_test_yz, X_test_zx], y_test_xy),\n",
        "#     batch_size=32,\n",
        "#     epochs=10,\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distinct-appendix",
      "metadata": {
        "id": "distinct-appendix",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# model_xy = get_model()\n",
        "# X_train_xy, y_train_xy = shuffle(X_train_xy, y_train_xy)\n",
        "# history_xy = model_xy.fit(X_train_xy, y_train_xy, validation_data=(X_test_xy, y_test_xy), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "constitutional-genre",
      "metadata": {
        "id": "constitutional-genre"
      },
      "outputs": [],
      "source": [
        "# # prob_xy = tf.keras.Sequential([model_xy, tf.keras.layers.Softmax()])\n",
        "# # y_pred_xy = prob_xy.predict(X_test_xy)\n",
        "# y_pred_xy = model_xy.predict(X_test_xy)\n",
        "# y_pred = np.argmax(y_pred_xy, axis=1)\n",
        "# print(classification_report(y_test_xy.ravel(), y_pred, zero_division=0))\n",
        "# prc_xy = precision_score(y_test_xy.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "emotional-chrome",
      "metadata": {
        "id": "emotional-chrome"
      },
      "outputs": [],
      "source": [
        "# model_yz = get_model()\n",
        "# X_train_yz, y_train_yz = shuffle(X_train_yz, y_train_yz)\n",
        "# history_yz = model_yz.fit(X_train_yz, y_train_yz, validation_data=(X_test_yz, y_test_yz), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "boring-insurance",
      "metadata": {
        "id": "boring-insurance"
      },
      "outputs": [],
      "source": [
        "# # prob_yz = tf.keras.Sequential([model_yz, tf.keras.layers.Softmax()])\n",
        "# # y_pred_yz = prob_yz.predict(X_test_yz)\n",
        "# y_pred_yz = model_yz.predict(X_test_yz)\n",
        "# y_pred = np.argmax(y_pred_yz, axis=1)\n",
        "# print(classification_report(y_test_yz.ravel(), y_pred, zero_division=0))\n",
        "# prc_yz = precision_score(y_test_yz.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "joint-evaluation",
      "metadata": {
        "id": "joint-evaluation"
      },
      "outputs": [],
      "source": [
        "# model_zx = get_model()\n",
        "# X_train_zx, y_train_zx = shuffle(X_train_zx, y_train_zx)\n",
        "# history_zx = model_zx.fit(X_train_zx, y_train_zx, validation_data=(X_test_zx, y_test_zx), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "designed-still",
      "metadata": {
        "id": "designed-still"
      },
      "outputs": [],
      "source": [
        "# # prob_zx = tf.keras.Sequential([model_zx, tf.keras.layers.Softmax()])\n",
        "# # y_pred_zx = prob_zx.predict(X_test_zx)\n",
        "# y_pred_zx = model_zx.predict(X_test_zx)\n",
        "# y_pred = np.argmax(y_pred_zx, axis=1)\n",
        "# print(classification_report(y_test_zx.ravel(), y_pred, zero_division=0))\n",
        "# prc_zx = precision_score(y_test_zx.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "selective-geography",
      "metadata": {
        "id": "selective-geography"
      },
      "outputs": [],
      "source": [
        "# y_total = y_pred_xy + y_pred_yz + y_pred_zx\n",
        "# y_pred = np.argmax(y_total, axis=1)\n",
        "# report = classification_report(y_test_xy.ravel(), y_pred, zero_division=0)\n",
        "# print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collected-palmer",
      "metadata": {
        "id": "collected-palmer"
      },
      "outputs": [],
      "source": [
        "# config = '\\n\\nTEST_USER ' + TEST_USER + ' T: ' + str(int(time.time())) + '\\n'\n",
        "# underline = '=====================================\\n'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'logs_sptl_bw' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(underline)\n",
        "# f.write(report)\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intellectual-lunch",
      "metadata": {
        "id": "intellectual-lunch"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_xy' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_xy, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trying-thread",
      "metadata": {
        "id": "trying-thread"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_yz' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_yz, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "general-plant",
      "metadata": {
        "id": "general-plant"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_zx' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_zx, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Spatial_Path_Transfer_Learning_BW.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}