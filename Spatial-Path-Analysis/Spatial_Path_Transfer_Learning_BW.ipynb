{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/Hand-Gesture-Recognition/blob/dev/Spatial-Path-Analysis/Spatial_Path_Transfer_Learning_BW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "tamil-chrome",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tamil-chrome",
        "outputId": "7a36c610-9a78-4bfe-a754-96fd94157284"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import time\n",
        "import joblib\n",
        "import shutil\n",
        "import tarfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, confusion_matrix\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, GlobalAvgPool2D, Dropout, Flatten, concatenate\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "MfafORnwZkyT",
      "metadata": {
        "id": "MfafORnwZkyT"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "casual-stack",
      "metadata": {
        "id": "casual-stack"
      },
      "outputs": [],
      "source": [
        "# -------- TEST USER ----------- #\n",
        "\n",
        "TEST_USER      = '010'\n",
        "DATASET_ID     = '1p0CSRb9gax0sKqdyzOYVt-BXvZ4GtrBv'\n",
        "\n",
        "# BASE_DIR       = '../Dataset/'\n",
        "\n",
        "# Google Drive\n",
        "BASE_DIR       = '.'\n",
        "DATA_DIR       = 'Sensor-Data/'\n",
        "BW_IMG_DIR     = 'BW-Spatial-Path-Images/'\n",
        "RGB_IMG_DIR    = 'RGB-Spatial-Path-Images/'\n",
        "CHANNELS_DIR   = 'Channels/'\n",
        "IMG_SIZE       = (3, 3) # INCHES\n",
        "\n",
        "IMG_DIR        = 'BW-Spatial-Path-Images/'\n",
        "LOG_DIR        = 'Logs/'\n",
        "\n",
        "USERS          = ['001', '002', '003', '004', '005', '006', '007', '008', '009',\n",
        "                  '010', '011', '012', '013', '014', '015', '016', '017', '018',\n",
        "                  '019', '020', '021', '022', '023', '024', '025']\n",
        "\n",
        "# ------------------------------- Only Dynalic Gestures ------------------------------ #\n",
        "GESTURES       = ['j', 'z', 'bad', 'deaf', 'fine', 'good', 'goodbye', 'hello', 'hungry',\n",
        "                  'me', 'no', 'please', 'sorry', 'thankyou', 'yes', 'you']\n",
        "\n",
        "PLANES         = ['XY', 'YZ', 'ZX']\n",
        "\n",
        "BATCH_SIZE     = 32\n",
        "IMG_LEN        = 160\n",
        "IMG_SIZE       = (IMG_LEN, IMG_LEN)\n",
        "\n",
        "# ------------- FOR THE GREATER GOOD :) ------------- #\n",
        "DATASET_LEN    = 4000\n",
        "TRAIN_LEN      = 3840\n",
        "TEST_LEN       = 160\n",
        "\n",
        "EPOCHS         = 50\n",
        "LEARNING_RATE  = 0.001\n",
        "DECAY          = 0.0\n",
        "\n",
        "DT             = 0.01\n",
        "SHAPES         = 100\n",
        "CUT_OFF        = 3.0\n",
        "ORDER          = 4\n",
        "FS             = 100\n",
        "\n",
        "WINDOW_LEN     = 150\n",
        "\n",
        "CONFIG         = '_L_7_S_160x160_E_7'\n",
        "CHANNELS_GROUP = 'DYNAMIC_ACC_ONLY_'\n",
        "\n",
        "XY_WEIGHTS     = np.array([0.91, 0.75, 0.61, 0.63, 0.51, 0.66, 0.81, 0.65, 0.65, 0.31,\n",
        "                           0.66, 0.29, 0.34, 0.64, 0.64, 0.31])\n",
        "YZ_WEIGHTS     = np.array([0.73, 0.71, 0.70, 0.79, 0.76, 0.38, 0.80, 0.61, 0.58, 0.73,\n",
        "                           0.49, 0.26, 0.26, 0.52, 0.59, 0.54])\n",
        "ZX_WEIGHTS     = np.array([0.33, 0.66, 0.51, 0.54, 0.37, 0.51, 0.71, 0.30, 0.75, 0.41,\n",
        "                           0.40, 0.27, 0.24, 0.61, 0.36, 0.49])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e65322ba",
      "metadata": {
        "id": "e65322ba"
      },
      "outputs": [],
      "source": [
        "class LowPassFilter(object): \n",
        "    def butter_lowpass(cutoff, fs, order):\n",
        "        nyq = 0.5 * fs\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "        return b, a\n",
        "\n",
        "    def apply(data, cutoff=CUT_OFF, fs=FS, order=ORDER):\n",
        "        b, a = LowPassFilter.butter_lowpass(cutoff, fs, order=order)\n",
        "        y = lfilter(b, a, data)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ksdY7MgcqHNr",
      "metadata": {
        "id": "ksdY7MgcqHNr"
      },
      "outputs": [],
      "source": [
        "#--------------------- Download util for Google Drive ------------------- #\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "        \n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "        \n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(fid, destination):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(destination)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "        \n",
        "    print('creating data directory ... ', end='')\n",
        "    os.mkdir(destination)\n",
        "    print('√')\n",
        "    \n",
        "    print('downloading dataset from the repository ... ', end='')\n",
        "    filename = os.path.join(destination, 'dataset.tar.xz')\n",
        "    try:\n",
        "        download_file_from_google_drive(fid, filename)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "        \n",
        "    print('extracting the dataset ... ', end='')\n",
        "    try:\n",
        "        tar = tarfile.open(filename)\n",
        "        tar.extractall(destination)\n",
        "        tar.close()\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "MtE4QJm-qOZY",
      "metadata": {
        "id": "MtE4QJm-qOZY"
      },
      "outputs": [],
      "source": [
        "# # ------- Comment This if already downloaded -------- #\n",
        "\n",
        "# destination = os.path.join(BASE_DIR, DATA_DIR)\n",
        "# download_data(DATASET_ID, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f9d84aa1",
      "metadata": {
        "id": "f9d84aa1"
      },
      "outputs": [],
      "source": [
        "def clean_dir(path):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "    \n",
        "    print('creating ' + path + ' directory ... ', end='')\n",
        "    os.mkdir(path)\n",
        "    print('√')\n",
        "\n",
        "def extract_channels():\n",
        "    channels_dir = os.path.join(BASE_DIR, CHANNELS_DIR)\n",
        "    clean_dir(channels_dir)\n",
        "        \n",
        "    for user in USERS:\n",
        "    # for gesture in GESTURES:\n",
        "        print('Processing data for user ' + user, end=' ')\n",
        "        # print(f\"processing data for gesture {gesture} \", end=\"...\" )\n",
        "        \n",
        "        X = []\n",
        "        y = []\n",
        "        first_time = True\n",
        "        \n",
        "        for gesture in GESTURES:\n",
        "        # for user in USERS:\n",
        "              \n",
        "            user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
        "            gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
        "\n",
        "            dataset = pd.read_csv(gesture_dir)\n",
        "\n",
        "            dataset['flex_1'] = dataset['flex_1'].rolling(3).median()\n",
        "            dataset['flex_2'] = dataset['flex_2'].rolling(3).median()\n",
        "            dataset['flex_3'] = dataset['flex_3'].rolling(3).median()\n",
        "            dataset['flex_4'] = dataset['flex_4'].rolling(3).median()\n",
        "            dataset['flex_5'] = dataset['flex_5'].rolling(3).median()\n",
        "\n",
        "            dataset.fillna(0, inplace=True)\n",
        "\n",
        "            # flex = ['flex_1', 'flex_2', 'flex_3', 'flex_4', 'flex_5']\n",
        "            # max_flex = dataset[flex].max(axis=1)\n",
        "            # max_flex.replace(0, 1, inplace=True)\n",
        "            # dataset[flex] = dataset[flex].divide(max_flex, axis=0)\n",
        "            \n",
        "            flx1 = dataset['flex_1'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx2 = dataset['flex_2'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx3 = dataset['flex_3'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx4 = dataset['flex_4'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx5 = dataset['flex_5'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            accx = dataset['ACCx'].to_numpy()\n",
        "            accy = dataset['ACCy'].to_numpy()\n",
        "            accz = dataset['ACCz'].to_numpy()\n",
        "            \n",
        "            accx = LowPassFilter.apply(accx).reshape(-1, WINDOW_LEN)\n",
        "            accy = LowPassFilter.apply(accy).reshape(-1, WINDOW_LEN)\n",
        "            accz = LowPassFilter.apply(accz).reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            gyrx = dataset['GYRx'].to_numpy()\n",
        "            gyry = dataset['GYRy'].to_numpy()\n",
        "            gyrz = dataset['GYRz'].to_numpy()\n",
        "            \n",
        "            gyrx = LowPassFilter.apply(gyrx).reshape(-1, WINDOW_LEN)\n",
        "            gyry = LowPassFilter.apply(gyry).reshape(-1, WINDOW_LEN)\n",
        "            gyrz = LowPassFilter.apply(gyrz).reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            accm = np.sqrt(accx ** 2 + accy ** 2 + accz ** 2)\n",
        "            gyrm = np.sqrt(gyrx ** 2 + gyry ** 2 + gyrz ** 2)\n",
        "            \n",
        "            g_idx = GESTURES.index(gesture)\n",
        "            labels = np.ones((accx.shape[0], 1)) * g_idx\n",
        "            \n",
        "            channels = np.stack([\n",
        "                flx1, flx2, flx3, flx4, flx5,\n",
        "                accx, accy, accz\n",
        "            ], axis=-1)\n",
        "            \n",
        "            if first_time == True:\n",
        "                X = channels\n",
        "                y = labels\n",
        "                first_time = False\n",
        "            else:\n",
        "                X = np.append(X, channels, axis=0)\n",
        "                y = np.append(y, labels, axis=0)\n",
        "            \n",
        "        \n",
        "        x_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_X.joblib')\n",
        "        y_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_y.joblib')\n",
        "        joblib.dump(X, x_path)\n",
        "        joblib.dump(y, y_path)\n",
        "        \n",
        "        print('√')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "QmReRMF-qQcl",
      "metadata": {
        "id": "QmReRMF-qQcl"
      },
      "outputs": [],
      "source": [
        "# ------------- Spatial Path Image Generation ----------- #\n",
        "\n",
        "def clean_dir(path):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "    \n",
        "    print('creating ' + path + ' directory ... ', end='')\n",
        "    os.mkdir(path)\n",
        "    print('√')\n",
        "    \n",
        "# ----------- Spatial Path Vector Calculation ----------- #\n",
        "\n",
        "def get_displacement(acc):\n",
        "    v = np.zeros(acc.shape)\n",
        "    d = np.zeros(acc.shape)\n",
        "    for i in range(acc.shape[0] - 1):\n",
        "        v[i + 1] = v[i] + acc[i] * DT\n",
        "        d[i + 1] = v[i] * DT + 0.5 * acc[i] * DT * DT\n",
        "        \n",
        "    return d\n",
        "\n",
        "def write_image(x, y, path):\n",
        "    fig, ax = plt.subplots(frameon=True, figsize=(3, 3))\n",
        "    ax.axis('off')\n",
        "    plt.scatter(x, y, s=SHAPES, c='black')\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "def generate_bw_images():\n",
        "    image_dir = os.path.join(BASE_DIR, BW_IMG_DIR)\n",
        "    clean_dir(image_dir)\n",
        "    \n",
        "    for plane in PLANES:\n",
        "        print('processing spatial path images for ' + plane + ' plane ... ', end='')\n",
        "        plane_dir = os.path.join(image_dir, plane)\n",
        "        os.mkdir(plane_dir)\n",
        "        \n",
        "        # for gesture in GESTURES:\n",
        "        #     os.mkdir(os.path.join(plane_dir, gesture))\n",
        "\n",
        "        for user in USERS:\n",
        "            os.mkdir(os.path.join(plane_dir, user))\n",
        "    \n",
        "            # for user in USERS:\n",
        "            for gesture in GESTURES:\n",
        "                os.mkdir(os.path.join(plane_dir, user, gesture))\n",
        "                user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
        "                gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
        "                \n",
        "                accx = pd.read_csv(gesture_dir)['ACCx'].to_numpy()\n",
        "                accy = pd.read_csv(gesture_dir)['ACCy'].to_numpy()\n",
        "                accz = pd.read_csv(gesture_dir)['ACCz'].to_numpy()\n",
        "\n",
        "                x = get_displacement(accx).reshape(-1, 150)\n",
        "                y = get_displacement(accy).reshape(-1, 150)\n",
        "                z = get_displacement(accz).reshape(-1, 150)\n",
        "\n",
        "                count = 0\n",
        "                for i in range(x.shape[0]):\n",
        "                    # image_name = 'u' + user + '_g' + '{:0>2d}'.format(GESTURES.index(gesture)) + \\\n",
        "                    #              '_s' + '{:0>7d}'.format(count) + '_p' + plane + '.jpg'\n",
        "                    image_name = \"{:0>7d}\".format(count) + \".jpg\"\n",
        "                    path = os.path.join(plane_dir, user, gesture, image_name)\n",
        "                    \n",
        "                    if plane == 'XY':\n",
        "                        write_image(x[i, :], y[i, :], path)\n",
        "                    elif plane == 'YZ':\n",
        "                        write_image(y[i, :], z[i, :], path)\n",
        "                    else:\n",
        "                        write_image(z[i, :], x[i, :], path)\n",
        "\n",
        "                    count = count + 1\n",
        "            \n",
        "        print('√')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "07jhA8ukqScv",
      "metadata": {
        "id": "07jhA8ukqScv"
      },
      "outputs": [],
      "source": [
        "# extract_channels()\n",
        "# generate_bw_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "stylish-banner",
      "metadata": {
        "id": "stylish-banner"
      },
      "outputs": [],
      "source": [
        "def load_data(plane, test_user):\n",
        "    X_train = np.zeros((TRAIN_LEN, IMG_LEN, IMG_LEN, 3), dtype='uint8')\n",
        "    X_test = np.zeros((TEST_LEN, IMG_LEN, IMG_LEN, 3), dtype='uint8')\n",
        "    y_train = np.zeros((TRAIN_LEN, 1), dtype='uint8')\n",
        "    y_test = np.zeros((TEST_LEN, 1), dtype='uint8')\n",
        "    \n",
        "    train_count = 0\n",
        "    test_count = 0\n",
        "        \n",
        "    # for gesture in GESTURES:\n",
        "    for user in USERS:\n",
        "        print('loading data for user ' + user + ' on the ' + plane + ' plane ... ', end='')\n",
        "        user_dir = os.path.join(BASE_DIR, IMG_DIR, plane, user)\n",
        "\n",
        "        for gesture in GESTURES:\n",
        "            gesture_dir = os.path.join(user_dir, gesture)\n",
        "\n",
        "            # for filename in os.listdir(path):\n",
        "\n",
        "            for count in range(10):\n",
        "                image_name = \"{:0>7d}\".format(count) + \".jpg\"\n",
        "                img = cv2.imread(os.path.join(gesture_dir, image_name))\n",
        "                resized = cv2.resize(img, IMG_SIZE)\n",
        "                # resized = np.expand_dims(resized, axis=-1)\n",
        "                # label = int(filename[6:8])\n",
        "                if user != test_user:\n",
        "                    X_train[train_count, :] = resized\n",
        "                    y_train[train_count, 0] = GESTURES.index(gesture)\n",
        "                    train_count = train_count + 1\n",
        "                else:\n",
        "                    X_test[test_count, :] = resized\n",
        "                    y_test[test_count, 0] = GESTURES.index(gesture)\n",
        "                    test_count = test_count + 1\n",
        "                \n",
        "        print('√')\n",
        "        \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def load_and_save_data(plane):\n",
        "    X = np.zeros((DATASET_LEN, IMG_LEN, IMG_LEN, 1))\n",
        "    y = np.zeros((DATASET_LEN, 1))\n",
        "    \n",
        "    train_count = 0\n",
        "    test_count  = TRAIN_LEN\n",
        "        \n",
        "    for gesture in GESTURES:\n",
        "        print('loading data for ' + gesture + ' gesture on the ' + plane + ' plane ... ', end='')\n",
        "        path = os.path.join(BASE_DIR, IMG_DIR, plane, gesture)\n",
        "        for filename in os.listdir(path):\n",
        "            img = cv2.imread(os.path.join(path, filename), cv2.IMREAD_GRAYSCALE)\n",
        "            resized = cv2.resize(img, IMG_SIZE)\n",
        "            if filename[1:4] != TEST_USER:\n",
        "                X[train_count, :] = resized\n",
        "                y[train_count, 0] = GESTURES.index(gesture)\n",
        "                train_count = train_count + 1\n",
        "            else:\n",
        "                X[test_count, :] = resized\n",
        "                y[test_count, 0] = GESTURES.index(gesture)\n",
        "                test_count = test_count + 1\n",
        "                \n",
        "        print('√')\n",
        "\n",
        "    joblib.dump(X, BASE_DIR + 'X_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    joblib.dump(y, BASE_DIR + 'Y_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "\n",
        "def load_data_from_joblib(plane):\n",
        "    print('Loading data for ' + plane + ' plane ... ', end='')\n",
        "    X = joblib.load(BASE_DIR + 'X_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    y = joblib.load(BASE_DIR + 'Y_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    test_user = int(TEST_USER)\n",
        "    X_train = X[:TRAIN_LEN, :, :, :]\n",
        "    y_train = y[:TRAIN_LEN, :]\n",
        "    X_test = X[TRAIN_LEN:, :, :, :]\n",
        "    y_test = y[TRAIN_LEN:, :]\n",
        "\n",
        "    print('√')\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "extra-disclosure",
      "metadata": {
        "id": "extra-disclosure"
      },
      "outputs": [],
      "source": [
        "# X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data('XY')\n",
        "# X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data('YZ')\n",
        "# X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data('ZX')\n",
        "\n",
        "# Save to Google  Drive\n",
        "# load_and_save_data('XY')X_train_xy, y_train_xy = shuffle(X_train_xy, y_train_xy)\n",
        "# load_and_save_data('YZ')\n",
        "# load_and_save_data('ZX')\n",
        "\n",
        "# Load from Google Drive\n",
        "# X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data_from_joblib('XY')\n",
        "# X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data_from_joblib('YZ')\n",
        "# X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data_from_joblib('ZX')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "conditional-advisory",
      "metadata": {
        "id": "conditional-advisory"
      },
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "internal-arkansas",
      "metadata": {
        "id": "internal-arkansas"
      },
      "outputs": [],
      "source": [
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "strange-encoding",
      "metadata": {
        "id": "strange-encoding"
      },
      "outputs": [],
      "source": [
        "global_average_layer = GlobalAvgPool2D()\n",
        "prediction_layer = Dense(len(GESTURES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6e049e9a",
      "metadata": {
        "id": "6e049e9a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers.pooling import MaxPooling2D\n",
        "\n",
        "def get_conv_block_1D():\n",
        "    input = Input(shape=(150, 1))\n",
        "    x = BatchNormalization()(input)\n",
        "    x = Conv1D(filters=8, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(50, activation='relu')(x)\n",
        "\n",
        "    return input, x\n",
        "\n",
        "def get_conv_block_2D():\n",
        "    input = tf.keras.layers.Input(shape=IMG_SHAPE)\n",
        "    # x = data_augmentation(input)\n",
        "    x = preprocess_input(input)\n",
        "    x = base_model(x, training=False)\n",
        "    x = global_average_layer(x)\n",
        "\n",
        "    # x = layers.Conv2D(16, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),)(\n",
        "    #     input\n",
        "    # )\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.MaxPooling2D()(x)\n",
        "    # x = layers.Conv2D(32, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),)(\n",
        "    #     x\n",
        "    # )\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.MaxPooling2D()(x)\n",
        "    # x = layers.Conv2D(\n",
        "    #     64, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),\n",
        "    # )(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.Flatten()(x)\n",
        "\n",
        "    return input, x\n",
        "\n",
        "\n",
        "def get_stacked_model():\n",
        "    inputs = []\n",
        "    CNNs = []\n",
        "\n",
        "    for i in range(5):\n",
        "        input_i, CNN_i = get_conv_block_1D()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    for i in range(3):\n",
        "        input_i, CNN_i = get_conv_block_2D()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    x = concatenate(CNNs, axis=-1)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    output = Dense(16, activation=\"softmax\")(x)\n",
        "    model = Model(inputs, output)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
        "    model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "WTWKoIlYyzKW",
      "metadata": {
        "id": "WTWKoIlYyzKW",
        "outputId": "e16e8295-ecba-4d2c-a92c-e1cac95896dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 150, 1)      4           ['input_2[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 150, 1)      4           ['input_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 150, 1)      4           ['input_4[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 150, 1)      4           ['input_5[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 150, 1)      4           ['input_6[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 148, 8)       32          ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 148, 8)       32          ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 148, 8)       32          ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 148, 8)       32          ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 148, 8)       32          ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 146, 16)      400         ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 146, 16)      400         ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 146, 16)      400         ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 146, 16)      400         ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 146, 16)      400         ['conv1d_16[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 73, 16)       0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_13[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 71, 16)       784         ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 71, 16)       784         ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_6[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_8[0][0]']        \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)           [(None, 160, 160, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 160, 160, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_9 (InputLayer)           [(None, 160, 160, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 69, 16)       784         ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 69, 16)       784         ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 69, 16)       784         ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 69, 16)       784         ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 69, 16)       784         ['conv1d_18[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.truediv (TFOpLambda)   (None, 160, 160, 3)  0           ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.truediv_1 (TFOpLambda)  (None, 160, 160, 3)  0          ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.truediv_2 (TFOpLambda)  (None, 160, 160, 3)  0          ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_11[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_15[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_19[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLambda)  (None, 160, 160, 3)  0           ['tf.math.truediv[0][0]']        \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLambda  (None, 160, 160, 3)  0          ['tf.math.truediv_1[0][0]']      \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.subtract_2 (TFOpLambda  (None, 160, 160, 3)  0          ['tf.math.truediv_2[0][0]']      \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 544)          0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 544)          0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 544)          0           ['max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 544)          0           ['max_pooling1d_7[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 544)          0           ['max_pooling1d_9[0][0]']        \n",
            "                                                                                                  \n",
            " mobilenetv2_1.00_160 (Function  (None, 5, 5, 1280)  2257984     ['tf.math.subtract[0][0]',       \n",
            " al)                                                              'tf.math.subtract_1[0][0]',     \n",
            "                                                                  'tf.math.subtract_2[0][0]']     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 50)           27250       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 50)           27250       ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 50)           27250       ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 50)           27250       ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 50)           27250       ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 1280)        0           ['mobilenetv2_1.00_160[0][0]',   \n",
            " alAveragePooling2D)                                              'mobilenetv2_1.00_160[1][0]',   \n",
            "                                                                  'mobilenetv2_1.00_160[2][0]']   \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 4090)         0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_2[0][0]',                \n",
            "                                                                  'dense_3[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'dense_5[0][0]',                \n",
            "                                                                  'global_average_pooling2d[0][0]'\n",
            "                                                                 , 'global_average_pooling2d[1][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_average_pooling2d[2][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 4090)         0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          523648      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 16)           2064        ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,929,966\n",
            "Trainable params: 671,972\n",
            "Non-trainable params: 2,257,994\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_stacked_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "upper-tobago",
      "metadata": {
        "id": "upper-tobago"
      },
      "outputs": [],
      "source": [
        "# def get_model():\n",
        "#     inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
        "#     #     x = data_augmentation(inputs)\n",
        "#     x = preprocess_input(inputs)\n",
        "#     x = base_model(x, training=False)\n",
        "#     x = global_average_layer(x)\n",
        "#     x = tf.keras.layers.Dropout(0.2)(x)\n",
        "#     outputs = prediction_layer(x)\n",
        "#     model = tf.keras.Model(inputs, outputs)\n",
        "#     model.compile(\n",
        "#         optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE, decay=DECAY),\n",
        "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#         metrics=[\"accuracy\"],\n",
        "#     )\n",
        "#     return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a457f4",
      "metadata": {
        "id": "b3a457f4",
        "outputId": "9490cadc-cef9-493b-c941-6911026ba33a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing results for user 001... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 42s 333ms/step - loss: 2.5587 - accuracy: 0.2156 - val_loss: 1.6639 - val_accuracy: 0.6125\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 1.4767 - accuracy: 0.5464 - val_loss: 0.7979 - val_accuracy: 0.8375\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 275ms/step - loss: 0.8288 - accuracy: 0.7406 - val_loss: 0.4763 - val_accuracy: 0.8813\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.5667 - accuracy: 0.8237 - val_loss: 0.3177 - val_accuracy: 0.9375\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.4341 - accuracy: 0.8669 - val_loss: 0.2592 - val_accuracy: 0.9563\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 16s 274ms/step - loss: 0.3601 - accuracy: 0.8813 - val_loss: 0.2006 - val_accuracy: 0.9563\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.2972 - accuracy: 0.9055 - val_loss: 0.1779 - val_accuracy: 0.9688\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.2485 - accuracy: 0.9203 - val_loss: 0.1392 - val_accuracy: 0.9812\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 16s 272ms/step - loss: 0.2092 - accuracy: 0.9315 - val_loss: 0.1136 - val_accuracy: 0.9812\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 16s 273ms/step - loss: 0.1925 - accuracy: 0.9385 - val_loss: 0.0988 - val_accuracy: 0.9812\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 16s 274ms/step - loss: 0.1648 - accuracy: 0.9466 - val_loss: 0.0877 - val_accuracy: 0.9875\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 16s 272ms/step - loss: 0.1495 - accuracy: 0.9539 - val_loss: 0.0844 - val_accuracy: 0.9812\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 16s 274ms/step - loss: 0.1331 - accuracy: 0.9568 - val_loss: 0.0713 - val_accuracy: 0.9875\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.1184 - accuracy: 0.9630 - val_loss: 0.0698 - val_accuracy: 0.9875\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 16s 274ms/step - loss: 0.1130 - accuracy: 0.9612 - val_loss: 0.0695 - val_accuracy: 0.9812\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.1065 - accuracy: 0.9654 - val_loss: 0.0655 - val_accuracy: 0.9812\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 16s 274ms/step - loss: 0.0979 - accuracy: 0.9711 - val_loss: 0.0644 - val_accuracy: 0.9812\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0961 - accuracy: 0.9693 - val_loss: 0.0604 - val_accuracy: 0.9812\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0823 - accuracy: 0.9732 - val_loss: 0.0546 - val_accuracy: 0.9812\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0714 - accuracy: 0.9773 - val_loss: 0.0500 - val_accuracy: 0.9875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0699 - accuracy: 0.9805 - val_loss: 0.0501 - val_accuracy: 0.9812\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0775 - accuracy: 0.9721 - val_loss: 0.0582 - val_accuracy: 0.9875\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0749 - accuracy: 0.9779 - val_loss: 0.0709 - val_accuracy: 0.9750\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0607 - accuracy: 0.9833 - val_loss: 0.0619 - val_accuracy: 0.9812\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0596 - accuracy: 0.9826 - val_loss: 0.0505 - val_accuracy: 0.9812\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0611 - accuracy: 0.9797 - val_loss: 0.0525 - val_accuracy: 0.9812\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 275ms/step - loss: 0.0552 - accuracy: 0.9846 - val_loss: 0.0584 - val_accuracy: 0.9812\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0546 - accuracy: 0.9862 - val_loss: 0.0542 - val_accuracy: 0.9812\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0513 - accuracy: 0.9849 - val_loss: 0.0609 - val_accuracy: 0.9750\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0500 - accuracy: 0.9862 - val_loss: 0.0576 - val_accuracy: 0.9812\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 275ms/step - loss: 0.0448 - accuracy: 0.9852 - val_loss: 0.0548 - val_accuracy: 0.9812\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 275ms/step - loss: 0.0422 - accuracy: 0.9854 - val_loss: 0.0599 - val_accuracy: 0.9812\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0448 - accuracy: 0.9867 - val_loss: 0.0536 - val_accuracy: 0.9812\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0369 - accuracy: 0.9891 - val_loss: 0.0636 - val_accuracy: 0.9812\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0406 - accuracy: 0.9883 - val_loss: 0.0534 - val_accuracy: 0.9812\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 16s 274ms/step - loss: 0.0383 - accuracy: 0.9893 - val_loss: 0.0463 - val_accuracy: 0.9812\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0372 - accuracy: 0.9891 - val_loss: 0.0485 - val_accuracy: 0.9812\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0337 - accuracy: 0.9909 - val_loss: 0.0585 - val_accuracy: 0.9812\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0276 - accuracy: 0.9932 - val_loss: 0.0425 - val_accuracy: 0.9875\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0355 - accuracy: 0.9911 - val_loss: 0.0557 - val_accuracy: 0.9812\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0369 - accuracy: 0.9893 - val_loss: 0.0485 - val_accuracy: 0.9812\n",
            "3/3 [==============================] - 1s 190ms/step - loss: 0.0877 - accuracy: 0.9875\n",
            "98.75 %\n",
            "Processing results for user 002... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 30s 326ms/step - loss: 2.6320 - accuracy: 0.1904 - val_loss: 1.8210 - val_accuracy: 0.5437\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 1.5940 - accuracy: 0.5073 - val_loss: 0.8725 - val_accuracy: 0.7875\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.8918 - accuracy: 0.7227 - val_loss: 0.4760 - val_accuracy: 0.8625\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.5670 - accuracy: 0.8193 - val_loss: 0.3526 - val_accuracy: 0.8750\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.4240 - accuracy: 0.8687 - val_loss: 0.2910 - val_accuracy: 0.8875\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.3454 - accuracy: 0.8914 - val_loss: 0.2491 - val_accuracy: 0.9187\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.2803 - accuracy: 0.9057 - val_loss: 0.2467 - val_accuracy: 0.9125\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.2195 - accuracy: 0.9349 - val_loss: 0.2032 - val_accuracy: 0.9312\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.2146 - accuracy: 0.9307 - val_loss: 0.1820 - val_accuracy: 0.9375\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.1729 - accuracy: 0.9474 - val_loss: 0.1734 - val_accuracy: 0.9438\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 275ms/step - loss: 0.1480 - accuracy: 0.9552 - val_loss: 0.1676 - val_accuracy: 0.9438\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.1375 - accuracy: 0.9609 - val_loss: 0.1640 - val_accuracy: 0.9312\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.1354 - accuracy: 0.9578 - val_loss: 0.1519 - val_accuracy: 0.9375\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.1197 - accuracy: 0.9617 - val_loss: 0.1682 - val_accuracy: 0.9312\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.1231 - accuracy: 0.9620 - val_loss: 0.1829 - val_accuracy: 0.9312\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.1093 - accuracy: 0.9648 - val_loss: 0.1654 - val_accuracy: 0.9312\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0991 - accuracy: 0.9734 - val_loss: 0.1646 - val_accuracy: 0.9375\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0913 - accuracy: 0.9698 - val_loss: 0.1612 - val_accuracy: 0.9438\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0904 - accuracy: 0.9711 - val_loss: 0.1536 - val_accuracy: 0.9438\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0811 - accuracy: 0.9760 - val_loss: 0.1595 - val_accuracy: 0.9375\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0650 - accuracy: 0.9810 - val_loss: 0.1552 - val_accuracy: 0.9438\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 275ms/step - loss: 0.0727 - accuracy: 0.9766 - val_loss: 0.1750 - val_accuracy: 0.9250\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0715 - accuracy: 0.9771 - val_loss: 0.1614 - val_accuracy: 0.9375\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0632 - accuracy: 0.9810 - val_loss: 0.1785 - val_accuracy: 0.9375\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0559 - accuracy: 0.9849 - val_loss: 0.1382 - val_accuracy: 0.9438\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0599 - accuracy: 0.9799 - val_loss: 0.1322 - val_accuracy: 0.9500\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 275ms/step - loss: 0.0580 - accuracy: 0.9823 - val_loss: 0.1578 - val_accuracy: 0.9375\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0562 - accuracy: 0.9807 - val_loss: 0.1327 - val_accuracy: 0.9438\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0428 - accuracy: 0.9891 - val_loss: 0.1494 - val_accuracy: 0.9438\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0447 - accuracy: 0.9872 - val_loss: 0.1396 - val_accuracy: 0.9375\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0482 - accuracy: 0.9836 - val_loss: 0.1494 - val_accuracy: 0.9375\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0393 - accuracy: 0.9909 - val_loss: 0.1471 - val_accuracy: 0.9438\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0484 - accuracy: 0.9849 - val_loss: 0.1601 - val_accuracy: 0.9438\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0416 - accuracy: 0.9862 - val_loss: 0.1507 - val_accuracy: 0.9312\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 275ms/step - loss: 0.0387 - accuracy: 0.9880 - val_loss: 0.1696 - val_accuracy: 0.9438\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0373 - accuracy: 0.9875 - val_loss: 0.1555 - val_accuracy: 0.9500\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0345 - accuracy: 0.9893 - val_loss: 0.1333 - val_accuracy: 0.9312\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0352 - accuracy: 0.9878 - val_loss: 0.1437 - val_accuracy: 0.9438\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0357 - accuracy: 0.9888 - val_loss: 0.1223 - val_accuracy: 0.9375\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0345 - accuracy: 0.9904 - val_loss: 0.1381 - val_accuracy: 0.9375\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0313 - accuracy: 0.9922 - val_loss: 0.1541 - val_accuracy: 0.9438\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0332 - accuracy: 0.9898 - val_loss: 0.1272 - val_accuracy: 0.9438\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0286 - accuracy: 0.9922 - val_loss: 0.1323 - val_accuracy: 0.9312\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0278 - accuracy: 0.9906 - val_loss: 0.1180 - val_accuracy: 0.9438\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 0.0297 - accuracy: 0.9898 - val_loss: 0.1370 - val_accuracy: 0.9250\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0309 - accuracy: 0.9914 - val_loss: 0.1215 - val_accuracy: 0.9500\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0228 - accuracy: 0.9940 - val_loss: 0.1183 - val_accuracy: 0.9563\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0244 - accuracy: 0.9932 - val_loss: 0.1143 - val_accuracy: 0.9500\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0256 - accuracy: 0.9937 - val_loss: 0.1447 - val_accuracy: 0.9500\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0212 - accuracy: 0.9951 - val_loss: 0.1170 - val_accuracy: 0.9563\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0176 - accuracy: 0.9964 - val_loss: 0.1423 - val_accuracy: 0.9438\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0213 - accuracy: 0.9935 - val_loss: 0.1191 - val_accuracy: 0.9563\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0223 - accuracy: 0.9937 - val_loss: 0.1333 - val_accuracy: 0.9375\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 0.1421 - val_accuracy: 0.9500\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0202 - accuracy: 0.9945 - val_loss: 0.1390 - val_accuracy: 0.9500\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0187 - accuracy: 0.9953 - val_loss: 0.1566 - val_accuracy: 0.9375\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0189 - accuracy: 0.9951 - val_loss: 0.1487 - val_accuracy: 0.9500\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0212 - accuracy: 0.9930 - val_loss: 0.1512 - val_accuracy: 0.9500\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0205 - accuracy: 0.9935 - val_loss: 0.1679 - val_accuracy: 0.9500\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0227 - accuracy: 0.9943 - val_loss: 0.1460 - val_accuracy: 0.9500\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0161 - accuracy: 0.9958 - val_loss: 0.1412 - val_accuracy: 0.9500\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0175 - accuracy: 0.9961 - val_loss: 0.1361 - val_accuracy: 0.9500\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0150 - accuracy: 0.9964 - val_loss: 0.1310 - val_accuracy: 0.9375\n",
            "Epoch 64/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0202 - accuracy: 0.9937 - val_loss: 0.1457 - val_accuracy: 0.9438\n",
            "Epoch 65/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0140 - accuracy: 0.9971 - val_loss: 0.1603 - val_accuracy: 0.9375\n",
            "Epoch 66/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0156 - accuracy: 0.9966 - val_loss: 0.1645 - val_accuracy: 0.9500\n",
            "Epoch 67/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0137 - accuracy: 0.9974 - val_loss: 0.1627 - val_accuracy: 0.9438\n",
            "Epoch 68/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.1422 - val_accuracy: 0.9312\n",
            "Epoch 69/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0114 - accuracy: 0.9982 - val_loss: 0.1408 - val_accuracy: 0.9375\n",
            "Epoch 70/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0146 - accuracy: 0.9961 - val_loss: 0.1633 - val_accuracy: 0.9438\n",
            "Epoch 71/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0164 - accuracy: 0.9935 - val_loss: 0.1539 - val_accuracy: 0.9500\n",
            "Epoch 72/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0149 - accuracy: 0.9956 - val_loss: 0.1506 - val_accuracy: 0.9500\n",
            "Epoch 73/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0127 - accuracy: 0.9969 - val_loss: 0.1408 - val_accuracy: 0.9438\n",
            "Epoch 74/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0122 - accuracy: 0.9971 - val_loss: 0.1461 - val_accuracy: 0.9438\n",
            "Epoch 75/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0143 - accuracy: 0.9951 - val_loss: 0.2011 - val_accuracy: 0.9500\n",
            "Epoch 76/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0137 - accuracy: 0.9966 - val_loss: 0.1337 - val_accuracy: 0.9438\n",
            "Epoch 77/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0136 - accuracy: 0.9969 - val_loss: 0.1365 - val_accuracy: 0.9563\n",
            "3/3 [==============================] - 1s 208ms/step - loss: 0.1183 - accuracy: 0.9563\n",
            "95.63 %\n",
            "Processing results for user 003... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 29s 327ms/step - loss: 2.5985 - accuracy: 0.1974 - val_loss: 1.7119 - val_accuracy: 0.6625\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 1.5938 - accuracy: 0.5060 - val_loss: 0.8038 - val_accuracy: 0.8125\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.9283 - accuracy: 0.7161 - val_loss: 0.3820 - val_accuracy: 0.9125\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.5575 - accuracy: 0.8258 - val_loss: 0.1989 - val_accuracy: 0.9500\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.4423 - accuracy: 0.8604 - val_loss: 0.1458 - val_accuracy: 0.9750\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.3437 - accuracy: 0.8927 - val_loss: 0.1174 - val_accuracy: 0.9750\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.2699 - accuracy: 0.9143 - val_loss: 0.0987 - val_accuracy: 0.9812\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.2452 - accuracy: 0.9221 - val_loss: 0.0751 - val_accuracy: 0.9875\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.2094 - accuracy: 0.9357 - val_loss: 0.0835 - val_accuracy: 0.9812\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.1812 - accuracy: 0.9443 - val_loss: 0.0701 - val_accuracy: 0.9812\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.1680 - accuracy: 0.9492 - val_loss: 0.0741 - val_accuracy: 0.9750\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.1404 - accuracy: 0.9563 - val_loss: 0.0604 - val_accuracy: 0.9875\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.1383 - accuracy: 0.9539 - val_loss: 0.0614 - val_accuracy: 0.9875\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.1411 - accuracy: 0.9516 - val_loss: 0.0561 - val_accuracy: 0.9875\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.1218 - accuracy: 0.9612 - val_loss: 0.0419 - val_accuracy: 0.9875\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.1038 - accuracy: 0.9672 - val_loss: 0.0584 - val_accuracy: 0.9812\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.1118 - accuracy: 0.9659 - val_loss: 0.0444 - val_accuracy: 0.9875\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0934 - accuracy: 0.9714 - val_loss: 0.0400 - val_accuracy: 0.9875\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0836 - accuracy: 0.9740 - val_loss: 0.0362 - val_accuracy: 0.9875\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0892 - accuracy: 0.9740 - val_loss: 0.0376 - val_accuracy: 0.9875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0853 - accuracy: 0.9734 - val_loss: 0.0242 - val_accuracy: 1.0000\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0723 - accuracy: 0.9789 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0700 - accuracy: 0.9781 - val_loss: 0.0543 - val_accuracy: 0.9812\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0633 - accuracy: 0.9818 - val_loss: 0.0179 - val_accuracy: 1.0000\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0646 - accuracy: 0.9828 - val_loss: 0.0415 - val_accuracy: 0.9812\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0631 - accuracy: 0.9802 - val_loss: 0.0283 - val_accuracy: 0.9875\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0594 - accuracy: 0.9820 - val_loss: 0.0301 - val_accuracy: 0.9875\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0563 - accuracy: 0.9831 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0490 - accuracy: 0.9852 - val_loss: 0.0213 - val_accuracy: 0.9937\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0441 - accuracy: 0.9867 - val_loss: 0.0274 - val_accuracy: 0.9875\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0496 - accuracy: 0.9857 - val_loss: 0.0292 - val_accuracy: 0.9875\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0493 - accuracy: 0.9852 - val_loss: 0.0362 - val_accuracy: 0.9875\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0466 - accuracy: 0.9859 - val_loss: 0.0204 - val_accuracy: 0.9937\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0408 - accuracy: 0.9891 - val_loss: 0.0224 - val_accuracy: 1.0000\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0426 - accuracy: 0.9872 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0420 - accuracy: 0.9872 - val_loss: 0.0220 - val_accuracy: 0.9937\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0392 - accuracy: 0.9872 - val_loss: 0.0246 - val_accuracy: 0.9875\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0351 - accuracy: 0.9896 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0443 - accuracy: 0.9846 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0343 - accuracy: 0.9893 - val_loss: 0.0280 - val_accuracy: 0.9875\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0333 - accuracy: 0.9901 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0294 - accuracy: 0.9919 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0270 - accuracy: 0.9935 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0280 - accuracy: 0.9919 - val_loss: 0.0222 - val_accuracy: 0.9937\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0317 - accuracy: 0.9909 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0277 - accuracy: 0.9911 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0257 - accuracy: 0.9937 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0244 - accuracy: 0.9932 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 0.0271 - accuracy: 0.9930 - val_loss: 0.0199 - val_accuracy: 0.9937\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 17s 277ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 0.0189 - val_accuracy: 1.0000\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0219 - accuracy: 0.9940 - val_loss: 0.0233 - val_accuracy: 0.9937\n",
            "3/3 [==============================] - 1s 193ms/step - loss: 0.0242 - accuracy: 1.0000\n",
            "100.00 %\n",
            "Processing results for user 004... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 29s 329ms/step - loss: 2.5654 - accuracy: 0.2161 - val_loss: 2.2839 - val_accuracy: 0.2875\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 1.4546 - accuracy: 0.5422 - val_loss: 1.8475 - val_accuracy: 0.3875\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.8090 - accuracy: 0.7487 - val_loss: 1.7127 - val_accuracy: 0.4313\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.5285 - accuracy: 0.8375 - val_loss: 1.5768 - val_accuracy: 0.4812\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.4070 - accuracy: 0.8716 - val_loss: 1.5428 - val_accuracy: 0.5437\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.3176 - accuracy: 0.9029 - val_loss: 1.5100 - val_accuracy: 0.5688\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.2642 - accuracy: 0.9161 - val_loss: 1.3674 - val_accuracy: 0.5938\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.2153 - accuracy: 0.9352 - val_loss: 1.4628 - val_accuracy: 0.6187\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.1896 - accuracy: 0.9411 - val_loss: 1.4264 - val_accuracy: 0.6062\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1795 - accuracy: 0.9398 - val_loss: 1.4025 - val_accuracy: 0.6125\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1465 - accuracy: 0.9557 - val_loss: 1.4020 - val_accuracy: 0.6313\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1229 - accuracy: 0.9617 - val_loss: 1.3966 - val_accuracy: 0.6125\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1217 - accuracy: 0.9596 - val_loss: 1.2770 - val_accuracy: 0.6313\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1090 - accuracy: 0.9672 - val_loss: 1.3991 - val_accuracy: 0.6062\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1034 - accuracy: 0.9706 - val_loss: 1.4277 - val_accuracy: 0.6250\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0986 - accuracy: 0.9680 - val_loss: 1.6332 - val_accuracy: 0.6125\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0926 - accuracy: 0.9727 - val_loss: 1.4082 - val_accuracy: 0.6500\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0856 - accuracy: 0.9740 - val_loss: 1.5173 - val_accuracy: 0.6250\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0752 - accuracy: 0.9763 - val_loss: 1.5010 - val_accuracy: 0.6438\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0746 - accuracy: 0.9799 - val_loss: 1.4012 - val_accuracy: 0.6438\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0623 - accuracy: 0.9826 - val_loss: 1.4804 - val_accuracy: 0.6313\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0608 - accuracy: 0.9810 - val_loss: 1.5814 - val_accuracy: 0.6062\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0592 - accuracy: 0.9802 - val_loss: 1.4101 - val_accuracy: 0.6438\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0547 - accuracy: 0.9810 - val_loss: 1.5841 - val_accuracy: 0.6125\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0552 - accuracy: 0.9833 - val_loss: 1.6404 - val_accuracy: 0.6375\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0522 - accuracy: 0.9857 - val_loss: 1.6345 - val_accuracy: 0.6250\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0433 - accuracy: 0.9852 - val_loss: 1.5687 - val_accuracy: 0.6438\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0508 - accuracy: 0.9865 - val_loss: 1.5327 - val_accuracy: 0.6562\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0474 - accuracy: 0.9852 - val_loss: 1.5286 - val_accuracy: 0.6375\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0427 - accuracy: 0.9865 - val_loss: 1.4447 - val_accuracy: 0.6625\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0467 - accuracy: 0.9854 - val_loss: 1.5565 - val_accuracy: 0.6187\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0425 - accuracy: 0.9862 - val_loss: 1.6231 - val_accuracy: 0.6125\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0449 - accuracy: 0.9870 - val_loss: 1.6700 - val_accuracy: 0.6313\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0379 - accuracy: 0.9906 - val_loss: 1.6882 - val_accuracy: 0.6313\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0374 - accuracy: 0.9891 - val_loss: 1.6170 - val_accuracy: 0.6125\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0302 - accuracy: 0.9917 - val_loss: 1.5974 - val_accuracy: 0.6500\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0328 - accuracy: 0.9906 - val_loss: 1.6889 - val_accuracy: 0.6313\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0371 - accuracy: 0.9891 - val_loss: 1.5849 - val_accuracy: 0.6375\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0342 - accuracy: 0.9891 - val_loss: 1.5263 - val_accuracy: 0.6500\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0277 - accuracy: 0.9930 - val_loss: 1.6652 - val_accuracy: 0.6000\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0299 - accuracy: 0.9909 - val_loss: 1.5786 - val_accuracy: 0.6500\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0286 - accuracy: 0.9909 - val_loss: 1.4660 - val_accuracy: 0.6438\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0288 - accuracy: 0.9911 - val_loss: 1.6406 - val_accuracy: 0.6250\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0268 - accuracy: 0.9930 - val_loss: 1.5420 - val_accuracy: 0.6000\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0247 - accuracy: 0.9924 - val_loss: 1.6490 - val_accuracy: 0.6438\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0245 - accuracy: 0.9917 - val_loss: 1.4361 - val_accuracy: 0.6687\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0218 - accuracy: 0.9948 - val_loss: 1.6228 - val_accuracy: 0.6562\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0206 - accuracy: 0.9953 - val_loss: 1.7024 - val_accuracy: 0.6250\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0288 - accuracy: 0.9917 - val_loss: 1.5912 - val_accuracy: 0.6375\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0218 - accuracy: 0.9940 - val_loss: 1.6441 - val_accuracy: 0.6125\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 1.6869 - val_accuracy: 0.6187\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0168 - accuracy: 0.9958 - val_loss: 1.7625 - val_accuracy: 0.6062\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0169 - accuracy: 0.9953 - val_loss: 1.6379 - val_accuracy: 0.6438\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0200 - accuracy: 0.9943 - val_loss: 1.6944 - val_accuracy: 0.6250\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0179 - accuracy: 0.9951 - val_loss: 1.6523 - val_accuracy: 0.6500\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0198 - accuracy: 0.9948 - val_loss: 1.7399 - val_accuracy: 0.6125\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0172 - accuracy: 0.9953 - val_loss: 1.5521 - val_accuracy: 0.6438\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0173 - accuracy: 0.9951 - val_loss: 1.5977 - val_accuracy: 0.6313\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0177 - accuracy: 0.9964 - val_loss: 1.7622 - val_accuracy: 0.6187\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0181 - accuracy: 0.9940 - val_loss: 1.7593 - val_accuracy: 0.6000\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0165 - accuracy: 0.9956 - val_loss: 1.6315 - val_accuracy: 0.6187\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0145 - accuracy: 0.9969 - val_loss: 1.7355 - val_accuracy: 0.6250\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0127 - accuracy: 0.9964 - val_loss: 1.7296 - val_accuracy: 0.6000\n",
            "Epoch 64/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0149 - accuracy: 0.9964 - val_loss: 1.6673 - val_accuracy: 0.5938\n",
            "Epoch 65/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0147 - accuracy: 0.9966 - val_loss: 1.6491 - val_accuracy: 0.6125\n",
            "Epoch 66/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0140 - accuracy: 0.9964 - val_loss: 1.7459 - val_accuracy: 0.6062\n",
            "Epoch 67/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0170 - accuracy: 0.9945 - val_loss: 1.8344 - val_accuracy: 0.5875\n",
            "Epoch 68/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0116 - accuracy: 0.9974 - val_loss: 1.7632 - val_accuracy: 0.6000\n",
            "Epoch 69/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0134 - accuracy: 0.9969 - val_loss: 1.8229 - val_accuracy: 0.6000\n",
            "Epoch 70/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0178 - accuracy: 0.9943 - val_loss: 1.7681 - val_accuracy: 0.6187\n",
            "Epoch 71/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0132 - accuracy: 0.9953 - val_loss: 1.7074 - val_accuracy: 0.6000\n",
            "Epoch 72/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0145 - accuracy: 0.9966 - val_loss: 1.8570 - val_accuracy: 0.6000\n",
            "Epoch 73/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0119 - accuracy: 0.9974 - val_loss: 1.7965 - val_accuracy: 0.5875\n",
            "Epoch 74/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 1.8935 - val_accuracy: 0.6125\n",
            "Epoch 75/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 1.7710 - val_accuracy: 0.6125\n",
            "Epoch 76/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0130 - accuracy: 0.9966 - val_loss: 1.8575 - val_accuracy: 0.6000\n",
            "3/3 [==============================] - 1s 206ms/step - loss: 1.4361 - accuracy: 0.6687\n",
            "66.87 %\n",
            "Processing results for user 005... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 29s 329ms/step - loss: 2.6586 - accuracy: 0.1927 - val_loss: 1.9065 - val_accuracy: 0.5625\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 1.5505 - accuracy: 0.5201 - val_loss: 1.1796 - val_accuracy: 0.6687\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.8747 - accuracy: 0.7354 - val_loss: 0.8049 - val_accuracy: 0.7750\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.5784 - accuracy: 0.8268 - val_loss: 0.6260 - val_accuracy: 0.7875\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.4333 - accuracy: 0.8646 - val_loss: 0.5155 - val_accuracy: 0.8250\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.3461 - accuracy: 0.8924 - val_loss: 0.4179 - val_accuracy: 0.8500\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.2752 - accuracy: 0.9154 - val_loss: 0.3483 - val_accuracy: 0.8875\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.2416 - accuracy: 0.9263 - val_loss: 0.3279 - val_accuracy: 0.8875\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.2092 - accuracy: 0.9357 - val_loss: 0.3197 - val_accuracy: 0.8938\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1814 - accuracy: 0.9443 - val_loss: 0.2826 - val_accuracy: 0.8875\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1768 - accuracy: 0.9469 - val_loss: 0.2947 - val_accuracy: 0.8875\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1597 - accuracy: 0.9479 - val_loss: 0.2640 - val_accuracy: 0.9062\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1352 - accuracy: 0.9599 - val_loss: 0.2625 - val_accuracy: 0.8938\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1246 - accuracy: 0.9651 - val_loss: 0.2396 - val_accuracy: 0.9000\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1121 - accuracy: 0.9669 - val_loss: 0.2628 - val_accuracy: 0.9000\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1031 - accuracy: 0.9714 - val_loss: 0.2304 - val_accuracy: 0.8813\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0889 - accuracy: 0.9771 - val_loss: 0.2771 - val_accuracy: 0.9000\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0891 - accuracy: 0.9740 - val_loss: 0.2408 - val_accuracy: 0.8875\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0871 - accuracy: 0.9737 - val_loss: 0.2387 - val_accuracy: 0.8938\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0819 - accuracy: 0.9758 - val_loss: 0.2391 - val_accuracy: 0.9000\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0748 - accuracy: 0.9773 - val_loss: 0.2331 - val_accuracy: 0.9000\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0748 - accuracy: 0.9747 - val_loss: 0.2538 - val_accuracy: 0.9000\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0744 - accuracy: 0.9776 - val_loss: 0.2729 - val_accuracy: 0.8938\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0638 - accuracy: 0.9831 - val_loss: 0.2713 - val_accuracy: 0.8938\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0596 - accuracy: 0.9823 - val_loss: 0.2286 - val_accuracy: 0.9062\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0537 - accuracy: 0.9852 - val_loss: 0.2187 - val_accuracy: 0.9062\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0556 - accuracy: 0.9831 - val_loss: 0.2595 - val_accuracy: 0.9062\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0582 - accuracy: 0.9836 - val_loss: 0.2667 - val_accuracy: 0.9062\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0476 - accuracy: 0.9867 - val_loss: 0.2263 - val_accuracy: 0.9062\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0515 - accuracy: 0.9852 - val_loss: 0.2288 - val_accuracy: 0.9000\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0480 - accuracy: 0.9859 - val_loss: 0.2086 - val_accuracy: 0.9000\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0489 - accuracy: 0.9839 - val_loss: 0.2666 - val_accuracy: 0.9000\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0436 - accuracy: 0.9854 - val_loss: 0.2129 - val_accuracy: 0.9062\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0437 - accuracy: 0.9872 - val_loss: 0.2643 - val_accuracy: 0.9000\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0401 - accuracy: 0.9883 - val_loss: 0.2726 - val_accuracy: 0.9000\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 0.2584 - val_accuracy: 0.9000\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0363 - accuracy: 0.9885 - val_loss: 0.2241 - val_accuracy: 0.9000\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0293 - accuracy: 0.9922 - val_loss: 0.2482 - val_accuracy: 0.9062\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0282 - accuracy: 0.9951 - val_loss: 0.2631 - val_accuracy: 0.9000\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0278 - accuracy: 0.9924 - val_loss: 0.2327 - val_accuracy: 0.9062\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0293 - accuracy: 0.9919 - val_loss: 0.2473 - val_accuracy: 0.9062\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0278 - accuracy: 0.9924 - val_loss: 0.2821 - val_accuracy: 0.9062\n",
            "3/3 [==============================] - 1s 217ms/step - loss: 0.2640 - accuracy: 0.9062\n",
            "90.62 %\n",
            "Processing results for user 006... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 29s 329ms/step - loss: 2.5934 - accuracy: 0.2023 - val_loss: 1.6565 - val_accuracy: 0.6187\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 1.5382 - accuracy: 0.5234 - val_loss: 0.8972 - val_accuracy: 0.7375\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.8523 - accuracy: 0.7406 - val_loss: 0.6436 - val_accuracy: 0.7625\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.5726 - accuracy: 0.8148 - val_loss: 0.5374 - val_accuracy: 0.8000\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.4216 - accuracy: 0.8672 - val_loss: 0.4696 - val_accuracy: 0.8375\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.3322 - accuracy: 0.8966 - val_loss: 0.4232 - val_accuracy: 0.8562\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.2725 - accuracy: 0.9148 - val_loss: 0.3936 - val_accuracy: 0.8813\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.2304 - accuracy: 0.9263 - val_loss: 0.3742 - val_accuracy: 0.8813\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.2012 - accuracy: 0.9359 - val_loss: 0.3694 - val_accuracy: 0.8875\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1788 - accuracy: 0.9406 - val_loss: 0.3770 - val_accuracy: 0.8813\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.1562 - accuracy: 0.9518 - val_loss: 0.3529 - val_accuracy: 0.9125\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1427 - accuracy: 0.9578 - val_loss: 0.3535 - val_accuracy: 0.9000\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1372 - accuracy: 0.9534 - val_loss: 0.3648 - val_accuracy: 0.8875\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1235 - accuracy: 0.9635 - val_loss: 0.3745 - val_accuracy: 0.9062\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1047 - accuracy: 0.9721 - val_loss: 0.3447 - val_accuracy: 0.9125\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1076 - accuracy: 0.9643 - val_loss: 0.3234 - val_accuracy: 0.9125\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0955 - accuracy: 0.9703 - val_loss: 0.3337 - val_accuracy: 0.9187\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0836 - accuracy: 0.9755 - val_loss: 0.3305 - val_accuracy: 0.9187\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0874 - accuracy: 0.9732 - val_loss: 0.3708 - val_accuracy: 0.9125\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0770 - accuracy: 0.9766 - val_loss: 0.3510 - val_accuracy: 0.8938\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0774 - accuracy: 0.9753 - val_loss: 0.3525 - val_accuracy: 0.9187\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0633 - accuracy: 0.9799 - val_loss: 0.3620 - val_accuracy: 0.9250\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0619 - accuracy: 0.9794 - val_loss: 0.3390 - val_accuracy: 0.9250\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0700 - accuracy: 0.9802 - val_loss: 0.3305 - val_accuracy: 0.9250\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0606 - accuracy: 0.9815 - val_loss: 0.3498 - val_accuracy: 0.9187\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0467 - accuracy: 0.9885 - val_loss: 0.3409 - val_accuracy: 0.9250\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0485 - accuracy: 0.9844 - val_loss: 0.3440 - val_accuracy: 0.9250\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0455 - accuracy: 0.9872 - val_loss: 0.3385 - val_accuracy: 0.9250\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0498 - accuracy: 0.9859 - val_loss: 0.3250 - val_accuracy: 0.9250\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0451 - accuracy: 0.9862 - val_loss: 0.3381 - val_accuracy: 0.9250\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0397 - accuracy: 0.9878 - val_loss: 0.3471 - val_accuracy: 0.9250\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0400 - accuracy: 0.9880 - val_loss: 0.3407 - val_accuracy: 0.9312\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0390 - accuracy: 0.9859 - val_loss: 0.3655 - val_accuracy: 0.9187\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0343 - accuracy: 0.9909 - val_loss: 0.3252 - val_accuracy: 0.9250\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0386 - accuracy: 0.9888 - val_loss: 0.3773 - val_accuracy: 0.9187\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0330 - accuracy: 0.9922 - val_loss: 0.3419 - val_accuracy: 0.9312\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0380 - accuracy: 0.9867 - val_loss: 0.3312 - val_accuracy: 0.9375\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0321 - accuracy: 0.9917 - val_loss: 0.3523 - val_accuracy: 0.9375\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0343 - accuracy: 0.9891 - val_loss: 0.3823 - val_accuracy: 0.9250\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0348 - accuracy: 0.9872 - val_loss: 0.3432 - val_accuracy: 0.9375\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0288 - accuracy: 0.9917 - val_loss: 0.3791 - val_accuracy: 0.9312\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0286 - accuracy: 0.9914 - val_loss: 0.3570 - val_accuracy: 0.9312\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0289 - accuracy: 0.9911 - val_loss: 0.3591 - val_accuracy: 0.9375\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0233 - accuracy: 0.9937 - val_loss: 0.3702 - val_accuracy: 0.9312\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0251 - accuracy: 0.9935 - val_loss: 0.3502 - val_accuracy: 0.9375\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0252 - accuracy: 0.9917 - val_loss: 0.3612 - val_accuracy: 0.9250\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0250 - accuracy: 0.9937 - val_loss: 0.3579 - val_accuracy: 0.9375\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0264 - accuracy: 0.9919 - val_loss: 0.3591 - val_accuracy: 0.9312\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0219 - accuracy: 0.9943 - val_loss: 0.3532 - val_accuracy: 0.9375\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0224 - accuracy: 0.9937 - val_loss: 0.3751 - val_accuracy: 0.9312\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0239 - accuracy: 0.9930 - val_loss: 0.3493 - val_accuracy: 0.9375\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0192 - accuracy: 0.9951 - val_loss: 0.3353 - val_accuracy: 0.9312\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0168 - accuracy: 0.9958 - val_loss: 0.3418 - val_accuracy: 0.9375\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0192 - accuracy: 0.9948 - val_loss: 0.3390 - val_accuracy: 0.9312\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0189 - accuracy: 0.9961 - val_loss: 0.3861 - val_accuracy: 0.9375\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0164 - accuracy: 0.9953 - val_loss: 0.3453 - val_accuracy: 0.9375\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.3812 - val_accuracy: 0.9312\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0172 - accuracy: 0.9953 - val_loss: 0.3833 - val_accuracy: 0.9312\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0176 - accuracy: 0.9956 - val_loss: 0.3794 - val_accuracy: 0.9312\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0177 - accuracy: 0.9945 - val_loss: 0.3674 - val_accuracy: 0.9250\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0164 - accuracy: 0.9958 - val_loss: 0.3853 - val_accuracy: 0.9250\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0155 - accuracy: 0.9945 - val_loss: 0.3921 - val_accuracy: 0.9187\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0189 - accuracy: 0.9948 - val_loss: 0.3853 - val_accuracy: 0.9187\n",
            "Epoch 64/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0185 - accuracy: 0.9943 - val_loss: 0.3767 - val_accuracy: 0.9250\n",
            "Epoch 65/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 0.4115 - val_accuracy: 0.9312\n",
            "Epoch 66/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 0.4105 - val_accuracy: 0.9312\n",
            "Epoch 67/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.0123 - accuracy: 0.9958 - val_loss: 0.4006 - val_accuracy: 0.9312\n",
            "3/3 [==============================] - 1s 198ms/step - loss: 0.3312 - accuracy: 0.9375\n",
            "93.75 %\n",
            "Processing results for user 007... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 29s 327ms/step - loss: 2.6317 - accuracy: 0.2094 - val_loss: 1.5984 - val_accuracy: 0.7437\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 1.5049 - accuracy: 0.5271 - val_loss: 0.7580 - val_accuracy: 0.8375\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.8905 - accuracy: 0.7195 - val_loss: 0.3668 - val_accuracy: 0.9375\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.5668 - accuracy: 0.8164 - val_loss: 0.2384 - val_accuracy: 0.9625\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.4394 - accuracy: 0.8602 - val_loss: 0.1708 - val_accuracy: 0.9750\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.3412 - accuracy: 0.8990 - val_loss: 0.1298 - val_accuracy: 0.9812\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.2973 - accuracy: 0.9070 - val_loss: 0.1158 - val_accuracy: 0.9875\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.2431 - accuracy: 0.9227 - val_loss: 0.0920 - val_accuracy: 0.9812\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.2090 - accuracy: 0.9323 - val_loss: 0.0850 - val_accuracy: 0.9812\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1747 - accuracy: 0.9438 - val_loss: 0.0720 - val_accuracy: 0.9812\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1636 - accuracy: 0.9516 - val_loss: 0.0741 - val_accuracy: 0.9812\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1528 - accuracy: 0.9490 - val_loss: 0.0661 - val_accuracy: 0.9875\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1428 - accuracy: 0.9531 - val_loss: 0.0578 - val_accuracy: 0.9812\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1331 - accuracy: 0.9563 - val_loss: 0.0545 - val_accuracy: 0.9812\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1083 - accuracy: 0.9685 - val_loss: 0.0543 - val_accuracy: 0.9875\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0987 - accuracy: 0.9729 - val_loss: 0.0562 - val_accuracy: 0.9875\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1044 - accuracy: 0.9654 - val_loss: 0.0774 - val_accuracy: 0.9688\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0995 - accuracy: 0.9690 - val_loss: 0.0759 - val_accuracy: 0.9750\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0858 - accuracy: 0.9742 - val_loss: 0.0551 - val_accuracy: 0.9875\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0749 - accuracy: 0.9789 - val_loss: 0.0458 - val_accuracy: 0.9875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0691 - accuracy: 0.9802 - val_loss: 0.0580 - val_accuracy: 0.9875\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0703 - accuracy: 0.9802 - val_loss: 0.0603 - val_accuracy: 0.9812\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0682 - accuracy: 0.9807 - val_loss: 0.0575 - val_accuracy: 0.9750\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0665 - accuracy: 0.9797 - val_loss: 0.0456 - val_accuracy: 0.9875\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0670 - accuracy: 0.9776 - val_loss: 0.0557 - val_accuracy: 0.9875\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0608 - accuracy: 0.9826 - val_loss: 0.0514 - val_accuracy: 0.9875\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0600 - accuracy: 0.9831 - val_loss: 0.0477 - val_accuracy: 0.9875\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0567 - accuracy: 0.9815 - val_loss: 0.0451 - val_accuracy: 0.9875\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0547 - accuracy: 0.9831 - val_loss: 0.0580 - val_accuracy: 0.9812\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0478 - accuracy: 0.9870 - val_loss: 0.0668 - val_accuracy: 0.9812\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0494 - accuracy: 0.9839 - val_loss: 0.0504 - val_accuracy: 0.9875\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0468 - accuracy: 0.9862 - val_loss: 0.0548 - val_accuracy: 0.9875\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0442 - accuracy: 0.9872 - val_loss: 0.0455 - val_accuracy: 0.9875\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0429 - accuracy: 0.9878 - val_loss: 0.0509 - val_accuracy: 0.9875\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0423 - accuracy: 0.9872 - val_loss: 0.0449 - val_accuracy: 0.9875\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0445 - accuracy: 0.9867 - val_loss: 0.0531 - val_accuracy: 0.9875\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0373 - accuracy: 0.9885 - val_loss: 0.0603 - val_accuracy: 0.9812\n",
            "3/3 [==============================] - 1s 211ms/step - loss: 0.1158 - accuracy: 0.9875\n",
            "98.75 %\n",
            "Processing results for user 008... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 30s 330ms/step - loss: 2.5893 - accuracy: 0.2073 - val_loss: 1.8348 - val_accuracy: 0.5938\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 1.5339 - accuracy: 0.5237 - val_loss: 0.8575 - val_accuracy: 0.7875\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.8941 - accuracy: 0.7258 - val_loss: 0.4423 - val_accuracy: 0.8687\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.5542 - accuracy: 0.8247 - val_loss: 0.2967 - val_accuracy: 0.9125\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.4210 - accuracy: 0.8711 - val_loss: 0.2321 - val_accuracy: 0.9375\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.3413 - accuracy: 0.8917 - val_loss: 0.2068 - val_accuracy: 0.9250\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.2874 - accuracy: 0.9086 - val_loss: 0.1771 - val_accuracy: 0.9375\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.2329 - accuracy: 0.9276 - val_loss: 0.1570 - val_accuracy: 0.9438\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.2029 - accuracy: 0.9375 - val_loss: 0.1645 - val_accuracy: 0.9438\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.1737 - accuracy: 0.9456 - val_loss: 0.1426 - val_accuracy: 0.9500\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1608 - accuracy: 0.9479 - val_loss: 0.1548 - val_accuracy: 0.9312\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1479 - accuracy: 0.9549 - val_loss: 0.1251 - val_accuracy: 0.9438\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1372 - accuracy: 0.9547 - val_loss: 0.1270 - val_accuracy: 0.9438\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1251 - accuracy: 0.9599 - val_loss: 0.1354 - val_accuracy: 0.9375\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1114 - accuracy: 0.9664 - val_loss: 0.1743 - val_accuracy: 0.9250\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1076 - accuracy: 0.9654 - val_loss: 0.1408 - val_accuracy: 0.9312\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0981 - accuracy: 0.9672 - val_loss: 0.1345 - val_accuracy: 0.9438\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0859 - accuracy: 0.9755 - val_loss: 0.1175 - val_accuracy: 0.9438\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0842 - accuracy: 0.9721 - val_loss: 0.1401 - val_accuracy: 0.9375\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0748 - accuracy: 0.9753 - val_loss: 0.1036 - val_accuracy: 0.9500\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0763 - accuracy: 0.9753 - val_loss: 0.1065 - val_accuracy: 0.9500\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0721 - accuracy: 0.9755 - val_loss: 0.1469 - val_accuracy: 0.9375\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0623 - accuracy: 0.9823 - val_loss: 0.1738 - val_accuracy: 0.9250\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0590 - accuracy: 0.9836 - val_loss: 0.1970 - val_accuracy: 0.9250\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0608 - accuracy: 0.9820 - val_loss: 0.1408 - val_accuracy: 0.9375\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0601 - accuracy: 0.9820 - val_loss: 0.1269 - val_accuracy: 0.9438\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0510 - accuracy: 0.9865 - val_loss: 0.1283 - val_accuracy: 0.9438\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0502 - accuracy: 0.9841 - val_loss: 0.1258 - val_accuracy: 0.9375\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0508 - accuracy: 0.9833 - val_loss: 0.1431 - val_accuracy: 0.9312\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0448 - accuracy: 0.9862 - val_loss: 0.1410 - val_accuracy: 0.9375\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0459 - accuracy: 0.9870 - val_loss: 0.1322 - val_accuracy: 0.9438\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0481 - accuracy: 0.9859 - val_loss: 0.1325 - val_accuracy: 0.9375\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0407 - accuracy: 0.9867 - val_loss: 0.1369 - val_accuracy: 0.9375\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0385 - accuracy: 0.9896 - val_loss: 0.1505 - val_accuracy: 0.9312\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0379 - accuracy: 0.9898 - val_loss: 0.1609 - val_accuracy: 0.9312\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0365 - accuracy: 0.9891 - val_loss: 0.1551 - val_accuracy: 0.9250\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0414 - accuracy: 0.9872 - val_loss: 0.1405 - val_accuracy: 0.9438\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0337 - accuracy: 0.9888 - val_loss: 0.1536 - val_accuracy: 0.9312\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0347 - accuracy: 0.9891 - val_loss: 0.2013 - val_accuracy: 0.9250\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0374 - accuracy: 0.9883 - val_loss: 0.1549 - val_accuracy: 0.9375\n",
            "3/3 [==============================] - 1s 210ms/step - loss: 0.1426 - accuracy: 0.9500\n",
            "95.00 %\n",
            "Processing results for user 009... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 29s 329ms/step - loss: 2.7818 - accuracy: 0.1565 - val_loss: 1.9228 - val_accuracy: 0.6125\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 1.6988 - accuracy: 0.4826 - val_loss: 0.7036 - val_accuracy: 0.8813\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.9035 - accuracy: 0.7260 - val_loss: 0.2959 - val_accuracy: 0.9375\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.5778 - accuracy: 0.8286 - val_loss: 0.1715 - val_accuracy: 0.9688\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.4550 - accuracy: 0.8547 - val_loss: 0.1350 - val_accuracy: 0.9750\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.3606 - accuracy: 0.8875 - val_loss: 0.1048 - val_accuracy: 0.9812\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.2952 - accuracy: 0.9052 - val_loss: 0.0765 - val_accuracy: 0.9875\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.2554 - accuracy: 0.9167 - val_loss: 0.0646 - val_accuracy: 0.9875\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.2178 - accuracy: 0.9271 - val_loss: 0.0571 - val_accuracy: 0.9875\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1957 - accuracy: 0.9430 - val_loss: 0.0586 - val_accuracy: 0.9875\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1852 - accuracy: 0.9435 - val_loss: 0.0467 - val_accuracy: 0.9875\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.1596 - accuracy: 0.9495 - val_loss: 0.0430 - val_accuracy: 0.9937\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1393 - accuracy: 0.9581 - val_loss: 0.0462 - val_accuracy: 0.9875\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1254 - accuracy: 0.9625 - val_loss: 0.0404 - val_accuracy: 0.9937\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1216 - accuracy: 0.9599 - val_loss: 0.0516 - val_accuracy: 0.9812\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1117 - accuracy: 0.9669 - val_loss: 0.0465 - val_accuracy: 0.9875\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1048 - accuracy: 0.9682 - val_loss: 0.0375 - val_accuracy: 0.9937\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1036 - accuracy: 0.9669 - val_loss: 0.0321 - val_accuracy: 0.9937\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0906 - accuracy: 0.9706 - val_loss: 0.0358 - val_accuracy: 0.9937\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0829 - accuracy: 0.9763 - val_loss: 0.0343 - val_accuracy: 0.9875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0766 - accuracy: 0.9763 - val_loss: 0.0277 - val_accuracy: 0.9937\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0806 - accuracy: 0.9755 - val_loss: 0.0340 - val_accuracy: 0.9875\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0755 - accuracy: 0.9755 - val_loss: 0.0294 - val_accuracy: 0.9937\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0702 - accuracy: 0.9763 - val_loss: 0.0462 - val_accuracy: 0.9812\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0671 - accuracy: 0.9786 - val_loss: 0.0235 - val_accuracy: 0.9937\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0597 - accuracy: 0.9818 - val_loss: 0.0287 - val_accuracy: 0.9937\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0597 - accuracy: 0.9818 - val_loss: 0.0145 - val_accuracy: 0.9937\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0534 - accuracy: 0.9839 - val_loss: 0.0174 - val_accuracy: 0.9937\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0510 - accuracy: 0.9872 - val_loss: 0.0189 - val_accuracy: 0.9937\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0551 - accuracy: 0.9839 - val_loss: 0.0156 - val_accuracy: 0.9937\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0508 - accuracy: 0.9846 - val_loss: 0.0238 - val_accuracy: 0.9937\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0458 - accuracy: 0.9865 - val_loss: 0.0190 - val_accuracy: 0.9937\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0473 - accuracy: 0.9852 - val_loss: 0.0262 - val_accuracy: 0.9937\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0453 - accuracy: 0.9859 - val_loss: 0.0192 - val_accuracy: 0.9937\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0409 - accuracy: 0.9865 - val_loss: 0.0287 - val_accuracy: 0.9875\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0413 - accuracy: 0.9880 - val_loss: 0.0373 - val_accuracy: 0.9875\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0447 - accuracy: 0.9870 - val_loss: 0.0262 - val_accuracy: 0.9937\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0370 - accuracy: 0.9878 - val_loss: 0.0234 - val_accuracy: 0.9937\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0346 - accuracy: 0.9901 - val_loss: 0.0244 - val_accuracy: 0.9875\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0336 - accuracy: 0.9911 - val_loss: 0.0161 - val_accuracy: 0.9937\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0343 - accuracy: 0.9878 - val_loss: 0.0344 - val_accuracy: 0.9875\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0356 - accuracy: 0.9914 - val_loss: 0.0257 - val_accuracy: 0.9875\n",
            "3/3 [==============================] - 1s 214ms/step - loss: 0.0430 - accuracy: 0.9937\n",
            "99.37 %\n",
            "Processing results for user 010... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 30s 339ms/step - loss: 2.6888 - accuracy: 0.1802 - val_loss: 1.7979 - val_accuracy: 0.5875\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 1.6561 - accuracy: 0.4836 - val_loss: 0.8144 - val_accuracy: 0.8313\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.9351 - accuracy: 0.7026 - val_loss: 0.4333 - val_accuracy: 0.8750\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.5969 - accuracy: 0.8128 - val_loss: 0.3374 - val_accuracy: 0.9000\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.4470 - accuracy: 0.8589 - val_loss: 0.3273 - val_accuracy: 0.9187\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.3648 - accuracy: 0.8849 - val_loss: 0.2555 - val_accuracy: 0.9438\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.2983 - accuracy: 0.9057 - val_loss: 0.2355 - val_accuracy: 0.9625\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.2611 - accuracy: 0.9159 - val_loss: 0.2273 - val_accuracy: 0.9563\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.2344 - accuracy: 0.9292 - val_loss: 0.2067 - val_accuracy: 0.9563\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1964 - accuracy: 0.9383 - val_loss: 0.1886 - val_accuracy: 0.9563\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.1613 - accuracy: 0.9521 - val_loss: 0.1879 - val_accuracy: 0.9563\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1642 - accuracy: 0.9505 - val_loss: 0.1736 - val_accuracy: 0.9625\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1440 - accuracy: 0.9557 - val_loss: 0.1881 - val_accuracy: 0.9625\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1248 - accuracy: 0.9599 - val_loss: 0.1686 - val_accuracy: 0.9625\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1207 - accuracy: 0.9643 - val_loss: 0.1790 - val_accuracy: 0.9625\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1148 - accuracy: 0.9648 - val_loss: 0.1328 - val_accuracy: 0.9625\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1040 - accuracy: 0.9672 - val_loss: 0.1524 - val_accuracy: 0.9563\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0925 - accuracy: 0.9714 - val_loss: 0.1941 - val_accuracy: 0.9500\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0899 - accuracy: 0.9719 - val_loss: 0.1598 - val_accuracy: 0.9563\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0893 - accuracy: 0.9706 - val_loss: 0.1467 - val_accuracy: 0.9625\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0784 - accuracy: 0.9760 - val_loss: 0.1504 - val_accuracy: 0.9625\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0806 - accuracy: 0.9768 - val_loss: 0.1626 - val_accuracy: 0.9625\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0750 - accuracy: 0.9771 - val_loss: 0.1684 - val_accuracy: 0.9625\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0664 - accuracy: 0.9799 - val_loss: 0.1630 - val_accuracy: 0.9563\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0653 - accuracy: 0.9812 - val_loss: 0.1205 - val_accuracy: 0.9625\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0567 - accuracy: 0.9833 - val_loss: 0.1683 - val_accuracy: 0.9625\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0592 - accuracy: 0.9810 - val_loss: 0.1414 - val_accuracy: 0.9563\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0568 - accuracy: 0.9826 - val_loss: 0.1551 - val_accuracy: 0.9625\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0498 - accuracy: 0.9865 - val_loss: 0.1647 - val_accuracy: 0.9625\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0491 - accuracy: 0.9867 - val_loss: 0.1376 - val_accuracy: 0.9625\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0485 - accuracy: 0.9867 - val_loss: 0.1575 - val_accuracy: 0.9625\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0526 - accuracy: 0.9828 - val_loss: 0.1204 - val_accuracy: 0.9625\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0436 - accuracy: 0.9883 - val_loss: 0.1063 - val_accuracy: 0.9688\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0458 - accuracy: 0.9857 - val_loss: 0.1092 - val_accuracy: 0.9688\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0461 - accuracy: 0.9833 - val_loss: 0.1485 - val_accuracy: 0.9563\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0379 - accuracy: 0.9901 - val_loss: 0.1885 - val_accuracy: 0.9625\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0325 - accuracy: 0.9904 - val_loss: 0.1560 - val_accuracy: 0.9625\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0401 - accuracy: 0.9878 - val_loss: 0.1263 - val_accuracy: 0.9688\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0388 - accuracy: 0.9878 - val_loss: 0.1248 - val_accuracy: 0.9625\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0340 - accuracy: 0.9909 - val_loss: 0.1751 - val_accuracy: 0.9625\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0318 - accuracy: 0.9917 - val_loss: 0.1306 - val_accuracy: 0.9625\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0345 - accuracy: 0.9893 - val_loss: 0.1181 - val_accuracy: 0.9688\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0300 - accuracy: 0.9919 - val_loss: 0.1386 - val_accuracy: 0.9563\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0268 - accuracy: 0.9909 - val_loss: 0.1197 - val_accuracy: 0.9688\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0288 - accuracy: 0.9917 - val_loss: 0.1248 - val_accuracy: 0.9688\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0289 - accuracy: 0.9904 - val_loss: 0.1514 - val_accuracy: 0.9625\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0245 - accuracy: 0.9932 - val_loss: 0.1388 - val_accuracy: 0.9625\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0264 - accuracy: 0.9911 - val_loss: 0.1590 - val_accuracy: 0.9625\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0310 - accuracy: 0.9901 - val_loss: 0.0897 - val_accuracy: 0.9750\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0276 - accuracy: 0.9909 - val_loss: 0.1428 - val_accuracy: 0.9625\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0261 - accuracy: 0.9932 - val_loss: 0.1254 - val_accuracy: 0.9688\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0261 - accuracy: 0.9911 - val_loss: 0.1361 - val_accuracy: 0.9625\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0248 - accuracy: 0.9937 - val_loss: 0.1229 - val_accuracy: 0.9625\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0263 - accuracy: 0.9919 - val_loss: 0.1325 - val_accuracy: 0.9688\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0249 - accuracy: 0.9930 - val_loss: 0.1086 - val_accuracy: 0.9688\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0245 - accuracy: 0.9922 - val_loss: 0.1049 - val_accuracy: 0.9688\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0238 - accuracy: 0.9924 - val_loss: 0.1176 - val_accuracy: 0.9688\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0200 - accuracy: 0.9945 - val_loss: 0.1068 - val_accuracy: 0.9688\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0202 - accuracy: 0.9948 - val_loss: 0.0970 - val_accuracy: 0.9750\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0206 - accuracy: 0.9943 - val_loss: 0.1106 - val_accuracy: 0.9688\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0175 - accuracy: 0.9964 - val_loss: 0.1117 - val_accuracy: 0.9688\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0194 - accuracy: 0.9940 - val_loss: 0.1336 - val_accuracy: 0.9688\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0186 - accuracy: 0.9953 - val_loss: 0.1027 - val_accuracy: 0.9688\n",
            "Epoch 64/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0139 - accuracy: 0.9977 - val_loss: 0.1536 - val_accuracy: 0.9625\n",
            "Epoch 65/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0165 - accuracy: 0.9958 - val_loss: 0.1422 - val_accuracy: 0.9688\n",
            "Epoch 66/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0197 - accuracy: 0.9937 - val_loss: 0.1550 - val_accuracy: 0.9688\n",
            "Epoch 67/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0166 - accuracy: 0.9953 - val_loss: 0.1206 - val_accuracy: 0.9625\n",
            "Epoch 68/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0146 - accuracy: 0.9945 - val_loss: 0.1479 - val_accuracy: 0.9688\n",
            "Epoch 69/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0170 - accuracy: 0.9943 - val_loss: 0.1340 - val_accuracy: 0.9625\n",
            "Epoch 70/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.1167 - val_accuracy: 0.9688\n",
            "Epoch 71/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.2099 - val_accuracy: 0.9625\n",
            "Epoch 72/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0140 - accuracy: 0.9956 - val_loss: 0.1341 - val_accuracy: 0.9688\n",
            "Epoch 73/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.1653 - val_accuracy: 0.9625\n",
            "Epoch 74/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0164 - accuracy: 0.9943 - val_loss: 0.2025 - val_accuracy: 0.9625\n",
            "Epoch 75/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0134 - accuracy: 0.9974 - val_loss: 0.0978 - val_accuracy: 0.9688\n",
            "Epoch 76/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0143 - accuracy: 0.9958 - val_loss: 0.1104 - val_accuracy: 0.9625\n",
            "Epoch 77/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0125 - accuracy: 0.9966 - val_loss: 0.1385 - val_accuracy: 0.9688\n",
            "Epoch 78/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0145 - accuracy: 0.9966 - val_loss: 0.1301 - val_accuracy: 0.9688\n",
            "Epoch 79/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 0.1011 - val_accuracy: 0.9688\n",
            "3/3 [==============================] - 1s 198ms/step - loss: 0.0897 - accuracy: 0.9750\n",
            "97.50 %\n",
            "Processing results for user 011... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 29s 330ms/step - loss: 2.5592 - accuracy: 0.2232 - val_loss: 1.5387 - val_accuracy: 0.7188\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 1.5629 - accuracy: 0.5010 - val_loss: 0.6245 - val_accuracy: 0.9000\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.8680 - accuracy: 0.7271 - val_loss: 0.2481 - val_accuracy: 0.9438\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.5777 - accuracy: 0.8138 - val_loss: 0.1289 - val_accuracy: 0.9875\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.4252 - accuracy: 0.8648 - val_loss: 0.0806 - val_accuracy: 0.9937\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.3389 - accuracy: 0.8917 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.2740 - accuracy: 0.9143 - val_loss: 0.0630 - val_accuracy: 0.9937\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.2533 - accuracy: 0.9164 - val_loss: 0.0437 - val_accuracy: 0.9937\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.2114 - accuracy: 0.9307 - val_loss: 0.0285 - val_accuracy: 1.0000\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1887 - accuracy: 0.9354 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1652 - accuracy: 0.9456 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1542 - accuracy: 0.9510 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1378 - accuracy: 0.9552 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1254 - accuracy: 0.9596 - val_loss: 0.0177 - val_accuracy: 1.0000\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1107 - accuracy: 0.9654 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1100 - accuracy: 0.9669 - val_loss: 0.0110 - val_accuracy: 1.0000\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0960 - accuracy: 0.9698 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0973 - accuracy: 0.9698 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1035 - accuracy: 0.9690 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0851 - accuracy: 0.9732 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0868 - accuracy: 0.9721 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0755 - accuracy: 0.9776 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0695 - accuracy: 0.9805 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0705 - accuracy: 0.9799 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0663 - accuracy: 0.9789 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0709 - accuracy: 0.9758 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0615 - accuracy: 0.9812 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0602 - accuracy: 0.9810 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0568 - accuracy: 0.9826 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0518 - accuracy: 0.9831 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0446 - accuracy: 0.9870 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0430 - accuracy: 0.9859 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0498 - accuracy: 0.9836 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0429 - accuracy: 0.9875 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0481 - accuracy: 0.9857 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.0440 - accuracy: 0.9849 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
            "3/3 [==============================] - 1s 204ms/step - loss: 0.0593 - accuracy: 1.0000\n",
            "100.00 %\n",
            "Processing results for user 012... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 29s 331ms/step - loss: 2.6390 - accuracy: 0.2016 - val_loss: 1.5550 - val_accuracy: 0.8125\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 1.5825 - accuracy: 0.5125 - val_loss: 0.4624 - val_accuracy: 0.9625\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.9119 - accuracy: 0.7263 - val_loss: 0.1672 - val_accuracy: 0.9563\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.6012 - accuracy: 0.8081 - val_loss: 0.0955 - val_accuracy: 0.9812\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.4510 - accuracy: 0.8638 - val_loss: 0.0796 - val_accuracy: 0.9875\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.3660 - accuracy: 0.8859 - val_loss: 0.0591 - val_accuracy: 0.9875\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.2968 - accuracy: 0.9078 - val_loss: 0.0485 - val_accuracy: 0.9750\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.2463 - accuracy: 0.9211 - val_loss: 0.0468 - val_accuracy: 0.9812\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.2191 - accuracy: 0.9284 - val_loss: 0.0515 - val_accuracy: 0.9812\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1949 - accuracy: 0.9396 - val_loss: 0.0401 - val_accuracy: 0.9812\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1757 - accuracy: 0.9430 - val_loss: 0.0518 - val_accuracy: 0.9750\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1598 - accuracy: 0.9497 - val_loss: 0.0468 - val_accuracy: 0.9812\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.1367 - accuracy: 0.9589 - val_loss: 0.0307 - val_accuracy: 0.9875\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1499 - accuracy: 0.9490 - val_loss: 0.0533 - val_accuracy: 0.9750\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1228 - accuracy: 0.9615 - val_loss: 0.0449 - val_accuracy: 0.9812\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.1038 - accuracy: 0.9688 - val_loss: 0.0315 - val_accuracy: 0.9875\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.1081 - accuracy: 0.9651 - val_loss: 0.0369 - val_accuracy: 0.9875\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 0.1066 - accuracy: 0.9646 - val_loss: 0.0316 - val_accuracy: 0.9937\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0966 - accuracy: 0.9693 - val_loss: 0.0271 - val_accuracy: 0.9812\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0861 - accuracy: 0.9719 - val_loss: 0.0277 - val_accuracy: 0.9875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0863 - accuracy: 0.9721 - val_loss: 0.0421 - val_accuracy: 0.9812\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0784 - accuracy: 0.9747 - val_loss: 0.0425 - val_accuracy: 0.9812\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0856 - accuracy: 0.9711 - val_loss: 0.0267 - val_accuracy: 0.9812\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0697 - accuracy: 0.9797 - val_loss: 0.0326 - val_accuracy: 0.9875\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0659 - accuracy: 0.9807 - val_loss: 0.0270 - val_accuracy: 0.9875\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0660 - accuracy: 0.9810 - val_loss: 0.0302 - val_accuracy: 0.9875\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0598 - accuracy: 0.9820 - val_loss: 0.0260 - val_accuracy: 0.9875\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 0.0614 - accuracy: 0.9789 - val_loss: 0.0266 - val_accuracy: 0.9875\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0629 - accuracy: 0.9805 - val_loss: 0.0282 - val_accuracy: 0.9937\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0519 - accuracy: 0.9828 - val_loss: 0.0415 - val_accuracy: 0.9812\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0538 - accuracy: 0.9841 - val_loss: 0.0429 - val_accuracy: 0.9812\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0452 - accuracy: 0.9872 - val_loss: 0.0447 - val_accuracy: 0.9812\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0457 - accuracy: 0.9857 - val_loss: 0.0391 - val_accuracy: 0.9875\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0436 - accuracy: 0.9880 - val_loss: 0.0339 - val_accuracy: 0.9937\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0385 - accuracy: 0.9888 - val_loss: 0.0312 - val_accuracy: 0.9937\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0458 - accuracy: 0.9875 - val_loss: 0.0364 - val_accuracy: 0.9875\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0415 - accuracy: 0.9836 - val_loss: 0.0350 - val_accuracy: 0.9937\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0332 - accuracy: 0.9917 - val_loss: 0.0355 - val_accuracy: 0.9875\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0366 - accuracy: 0.9888 - val_loss: 0.0309 - val_accuracy: 0.9937\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0368 - accuracy: 0.9891 - val_loss: 0.0294 - val_accuracy: 0.9875\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 17s 281ms/step - loss: 0.0331 - accuracy: 0.9906 - val_loss: 0.0337 - val_accuracy: 0.9875\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0342 - accuracy: 0.9898 - val_loss: 0.0291 - val_accuracy: 0.9875\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0306 - accuracy: 0.9914 - val_loss: 0.0291 - val_accuracy: 0.9812\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0316 - accuracy: 0.9898 - val_loss: 0.0275 - val_accuracy: 0.9937\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 17s 279ms/step - loss: 0.0285 - accuracy: 0.9922 - val_loss: 0.0268 - val_accuracy: 0.9937\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0312 - accuracy: 0.9901 - val_loss: 0.0358 - val_accuracy: 0.9875\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 17s 280ms/step - loss: 0.0313 - accuracy: 0.9904 - val_loss: 0.0346 - val_accuracy: 0.9937\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 0.0235 - accuracy: 0.9945 - val_loss: 0.0386 - val_accuracy: 0.9812\n",
            "3/3 [==============================] - 1s 194ms/step - loss: 0.0316 - accuracy: 0.9937\n",
            "99.37 %\n",
            "Processing results for user 013... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "25/60 [===========>..................] - ETA: 9s - loss: 2.9425 - accuracy: 0.1213"
          ]
        }
      ],
      "source": [
        "\n",
        "ACC = []\n",
        "CM = []\n",
        "HISTORY = []\n",
        "logs = ''\n",
        "\n",
        "for test_user in USERS:\n",
        "    print('Processing results for user ' + test_user, end='... \\n')\n",
        "    \n",
        "    X_train = []\n",
        "    X_test = []\n",
        "    y_train = []\n",
        "    y_test = []\n",
        "    \n",
        "    first_time_train = True\n",
        "    first_time_test = True\n",
        "\n",
        "    for user in USERS:\n",
        "        x_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_X.joblib')\n",
        "        y_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_y.joblib')\n",
        "        X = joblib.load(x_path)\n",
        "        y = joblib.load(y_path)\n",
        "\n",
        "        if user == test_user:\n",
        "            if first_time_train == True:\n",
        "                first_time_train = False\n",
        "                X_test = X\n",
        "                y_test = y\n",
        "                \n",
        "            else:\n",
        "                X_test = np.append(X_test, X, axis=0)\n",
        "                y_test = np.append(y_test, y, axis=0)\n",
        "                \n",
        "        else:\n",
        "            if first_time_test == True:\n",
        "                first_time_test = False\n",
        "                X_train = X\n",
        "                y_train = y\n",
        "                \n",
        "            else:\n",
        "                X_train = np.append(X_train, X, axis=0)\n",
        "                y_train = np.append(y_train, y, axis=0)\n",
        "\n",
        "\n",
        "    # X_train, y_train = shuffle(X_train, y_train)\n",
        "\n",
        "    X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data('XY', test_user)\n",
        "    X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data('YZ', test_user)\n",
        "    X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data('ZX', test_user)\n",
        "\n",
        "    X_train_xy, X_train_yz, X_train_zx, X_train, y_train = shuffle(\n",
        "        X_train_xy, X_train_yz, X_train_zx, X_train, y_train\n",
        "    )\n",
        "\n",
        "    X_train_combined = np.split(X_train, 8, axis=-1)[:5] + [X_train_xy, X_train_yz, X_train_zx]\n",
        "    X_test_combined = np.split(X_test, 8, axis=-1)[:5] + [X_test_xy, X_test_yz, X_test_zx]\n",
        "\n",
        "    del X_train_xy, X_test_xy, y_train_xy, y_test_xy\n",
        "    del X_train_yz, X_test_yz, y_train_yz, y_test_yz\n",
        "    del X_train_zx, X_test_zx, y_train_zx, y_test_zx\n",
        "    del X_train, X_test\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    print(len(X_train_combined))\n",
        "\n",
        "    callbacks = [\n",
        "                 tf.keras.callbacks.EarlyStopping(\n",
        "                    monitor='val_accuracy', \n",
        "                    patience=30, \n",
        "                    mode='max', \n",
        "                    restore_best_weights=True\n",
        "                ),\n",
        "    ]\n",
        "\n",
        "    model = get_stacked_model()\n",
        "    history = model.fit(\n",
        "        X_train_combined, \n",
        "        y_train, \n",
        "        validation_data=(X_test_combined, y_test),\n",
        "        epochs=300, \n",
        "        batch_size=64,\n",
        "        callbacks=[callbacks]\n",
        "    )\n",
        "    _, accuracy = model.evaluate(X_test_combined, y_test, batch_size=64)\n",
        "    y_pred = model.predict(X_test_combined)\n",
        "    cm = confusion_matrix(y_test.ravel(), np.argmax(y_pred, axis=-1))\n",
        "\n",
        "    CM.append(cm)\n",
        "    HISTORY.append(history)\n",
        "\n",
        "    accuracy = accuracy * 100\n",
        "    print(f'%.2f %%' %(accuracy))\n",
        "    logs = logs + 'Accuracy for user ' + str(test_user) + '... ' + str(accuracy) + '\\n'\n",
        "    ACC.append(accuracy)\n",
        "    \n",
        "AVG_ACC = np.mean(ACC)\n",
        "STD = np.std(ACC)\n",
        "print('------------------------------------')\n",
        "print(f'Average accuracy %.2f +/- %.2f' %(AVG_ACC, STD))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    pass"
      ],
      "metadata": {
        "id": "3N94GWOk9KHE"
      },
      "id": "3N94GWOk9KHE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92fd4333",
      "metadata": {
        "id": "92fd4333"
      },
      "outputs": [],
      "source": [
        "# model = get_stacked_model()\n",
        "# X_train_xy, X_train_yz, X_train_zx, y_train_xy = shuffle(\n",
        "#     X_train_xy, X_train_yz, X_train_zx, y_train_xy\n",
        "# )\n",
        "\n",
        "# history = model.fit(\n",
        "#     [X_train_xy, X_train_yz, X_train_zx],\n",
        "#     y_train_xy,\n",
        "#     validation_data=([X_test_xy, X_test_yz, X_test_zx], y_test_xy),\n",
        "#     batch_size=32,\n",
        "#     epochs=10,\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distinct-appendix",
      "metadata": {
        "id": "distinct-appendix",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# model_xy = get_model()\n",
        "# X_train_xy, y_train_xy = shuffle(X_train_xy, y_train_xy)\n",
        "# history_xy = model_xy.fit(X_train_xy, y_train_xy, validation_data=(X_test_xy, y_test_xy), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "constitutional-genre",
      "metadata": {
        "id": "constitutional-genre"
      },
      "outputs": [],
      "source": [
        "# # prob_xy = tf.keras.Sequential([model_xy, tf.keras.layers.Softmax()])\n",
        "# # y_pred_xy = prob_xy.predict(X_test_xy)\n",
        "# y_pred_xy = model_xy.predict(X_test_xy)\n",
        "# y_pred = np.argmax(y_pred_xy, axis=1)\n",
        "# print(classification_report(y_test_xy.ravel(), y_pred, zero_division=0))\n",
        "# prc_xy = precision_score(y_test_xy.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "emotional-chrome",
      "metadata": {
        "id": "emotional-chrome"
      },
      "outputs": [],
      "source": [
        "# model_yz = get_model()\n",
        "# X_train_yz, y_train_yz = shuffle(X_train_yz, y_train_yz)\n",
        "# history_yz = model_yz.fit(X_train_yz, y_train_yz, validation_data=(X_test_yz, y_test_yz), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "boring-insurance",
      "metadata": {
        "id": "boring-insurance"
      },
      "outputs": [],
      "source": [
        "# # prob_yz = tf.keras.Sequential([model_yz, tf.keras.layers.Softmax()])\n",
        "# # y_pred_yz = prob_yz.predict(X_test_yz)\n",
        "# y_pred_yz = model_yz.predict(X_test_yz)\n",
        "# y_pred = np.argmax(y_pred_yz, axis=1)\n",
        "# print(classification_report(y_test_yz.ravel(), y_pred, zero_division=0))\n",
        "# prc_yz = precision_score(y_test_yz.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "joint-evaluation",
      "metadata": {
        "id": "joint-evaluation"
      },
      "outputs": [],
      "source": [
        "# model_zx = get_model()\n",
        "# X_train_zx, y_train_zx = shuffle(X_train_zx, y_train_zx)\n",
        "# history_zx = model_zx.fit(X_train_zx, y_train_zx, validation_data=(X_test_zx, y_test_zx), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "designed-still",
      "metadata": {
        "id": "designed-still"
      },
      "outputs": [],
      "source": [
        "# # prob_zx = tf.keras.Sequential([model_zx, tf.keras.layers.Softmax()])\n",
        "# # y_pred_zx = prob_zx.predict(X_test_zx)\n",
        "# y_pred_zx = model_zx.predict(X_test_zx)\n",
        "# y_pred = np.argmax(y_pred_zx, axis=1)\n",
        "# print(classification_report(y_test_zx.ravel(), y_pred, zero_division=0))\n",
        "# prc_zx = precision_score(y_test_zx.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "selective-geography",
      "metadata": {
        "id": "selective-geography"
      },
      "outputs": [],
      "source": [
        "# y_total = y_pred_xy + y_pred_yz + y_pred_zx\n",
        "# y_pred = np.argmax(y_total, axis=1)\n",
        "# report = classification_report(y_test_xy.ravel(), y_pred, zero_division=0)\n",
        "# print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collected-palmer",
      "metadata": {
        "id": "collected-palmer"
      },
      "outputs": [],
      "source": [
        "# config = '\\n\\nTEST_USER ' + TEST_USER + ' T: ' + str(int(time.time())) + '\\n'\n",
        "# underline = '=====================================\\n'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'logs_sptl_bw' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(underline)\n",
        "# f.write(report)\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intellectual-lunch",
      "metadata": {
        "id": "intellectual-lunch"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_xy' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_xy, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trying-thread",
      "metadata": {
        "id": "trying-thread"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_yz' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_yz, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "general-plant",
      "metadata": {
        "id": "general-plant"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_zx' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_zx, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Spatial_Path_Transfer_Learning_BW.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}