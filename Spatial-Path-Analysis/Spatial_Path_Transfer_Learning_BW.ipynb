{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/Hand-Gesture-Recognition/blob/dev/Spatial-Path-Analysis/Spatial_Path_Transfer_Learning_BW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "tamil-chrome",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tamil-chrome",
        "outputId": "e0639e59-459f-48ec-a4c0-7cbf2e9f1e3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import cv2\n",
        "import time\n",
        "import joblib\n",
        "import shutil\n",
        "import tarfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, confusion_matrix\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, GlobalAvgPool2D, Dropout, Flatten, concatenate\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "MfafORnwZkyT",
      "metadata": {
        "id": "MfafORnwZkyT"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "casual-stack",
      "metadata": {
        "id": "casual-stack"
      },
      "outputs": [],
      "source": [
        "# -------- TEST USER ----------- #\n",
        "\n",
        "TEST_USER      = '010'\n",
        "DATASET_ID     = '1p0CSRb9gax0sKqdyzOYVt-BXvZ4GtrBv'\n",
        "\n",
        "# BASE_DIR       = '../Dataset/'\n",
        "\n",
        "# Google Drive\n",
        "BASE_DIR       = '.'\n",
        "DATA_DIR       = 'Sensor-Data/'\n",
        "BW_IMG_DIR     = 'BW-Spatial-Path-Images/'\n",
        "RGB_IMG_DIR    = 'RGB-Spatial-Path-Images/'\n",
        "CHANNELS_DIR   = 'Channels/'\n",
        "IMG_SIZE       = (3, 3) # INCHES\n",
        "\n",
        "IMG_DIR        = 'BW-Spatial-Path-Images/'\n",
        "LOG_DIR        = 'Logs/'\n",
        "\n",
        "USERS          = ['001', '002', '003', '004', '005', '006', '007', '008', '009',\n",
        "                  '010', '011', '012', '013', '014', '015', '016', '017', '018',\n",
        "                  '019', '020', '021', '022', '023', '024', '025']\n",
        "\n",
        "# ------------------------------- Only Dynalic Gestures ------------------------------ #\n",
        "GESTURES       = ['j', 'z', 'bad', 'deaf', 'fine', 'good', 'goodbye', 'hello', 'hungry',\n",
        "                  'me', 'no', 'please', 'sorry', 'thankyou', 'yes', 'you']\n",
        "\n",
        "PLANES         = ['XY', 'YZ', 'ZX']\n",
        "\n",
        "BATCH_SIZE     = 32\n",
        "IMG_LEN        = 224\n",
        "IMG_SIZE       = (IMG_LEN, IMG_LEN)\n",
        "\n",
        "# ------------- FOR THE GREATER GOOD :) ------------- #\n",
        "DATASET_LEN    = 4000\n",
        "TRAIN_LEN      = 3840\n",
        "TEST_LEN       = 160\n",
        "\n",
        "EPOCHS         = 50\n",
        "LEARNING_RATE  = 0.001\n",
        "DECAY          = 0.0\n",
        "\n",
        "DT             = 0.01\n",
        "SHAPES         = 100\n",
        "CUT_OFF        = 3.0\n",
        "ORDER          = 4\n",
        "FS             = 100\n",
        "\n",
        "WINDOW_LEN     = 150\n",
        "\n",
        "CONFIG         = '_L_7_S_160x160_E_7'\n",
        "CHANNELS_GROUP = 'DYNAMIC_ACC_ONLY_'\n",
        "\n",
        "XY_WEIGHTS     = np.array([0.91, 0.75, 0.61, 0.63, 0.51, 0.66, 0.81, 0.65, 0.65, 0.31,\n",
        "                           0.66, 0.29, 0.34, 0.64, 0.64, 0.31])\n",
        "YZ_WEIGHTS     = np.array([0.73, 0.71, 0.70, 0.79, 0.76, 0.38, 0.80, 0.61, 0.58, 0.73,\n",
        "                           0.49, 0.26, 0.26, 0.52, 0.59, 0.54])\n",
        "ZX_WEIGHTS     = np.array([0.33, 0.66, 0.51, 0.54, 0.37, 0.51, 0.71, 0.30, 0.75, 0.41,\n",
        "                           0.40, 0.27, 0.24, 0.61, 0.36, 0.49])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e65322ba",
      "metadata": {
        "id": "e65322ba"
      },
      "outputs": [],
      "source": [
        "class LowPassFilter(object): \n",
        "    def butter_lowpass(cutoff, fs, order):\n",
        "        nyq = 0.5 * fs\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "        return b, a\n",
        "\n",
        "    def apply(data, cutoff=CUT_OFF, fs=FS, order=ORDER):\n",
        "        b, a = LowPassFilter.butter_lowpass(cutoff, fs, order=order)\n",
        "        y = lfilter(b, a, data)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ksdY7MgcqHNr",
      "metadata": {
        "id": "ksdY7MgcqHNr"
      },
      "outputs": [],
      "source": [
        "#--------------------- Download util for Google Drive ------------------- #\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "        \n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "        \n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(fid, destination):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(destination)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "        \n",
        "    print('creating data directory ... ', end='')\n",
        "    os.mkdir(destination)\n",
        "    print('√')\n",
        "    \n",
        "    print('downloading dataset from the repository ... ', end='')\n",
        "    filename = os.path.join(destination, 'dataset.tar.xz')\n",
        "    try:\n",
        "        download_file_from_google_drive(fid, filename)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "        \n",
        "    print('extracting the dataset ... ', end='')\n",
        "    try:\n",
        "        tar = tarfile.open(filename)\n",
        "        tar.extractall(destination)\n",
        "        tar.close()\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "MtE4QJm-qOZY",
      "metadata": {
        "id": "MtE4QJm-qOZY"
      },
      "outputs": [],
      "source": [
        "# # ------- Comment This if already downloaded -------- #\n",
        "\n",
        "# destination = os.path.join(BASE_DIR, DATA_DIR)\n",
        "# download_data(DATASET_ID, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f9d84aa1",
      "metadata": {
        "id": "f9d84aa1"
      },
      "outputs": [],
      "source": [
        "def clean_dir(path):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "    \n",
        "    print('creating ' + path + ' directory ... ', end='')\n",
        "    os.mkdir(path)\n",
        "    print('√')\n",
        "\n",
        "def extract_channels():\n",
        "    channels_dir = os.path.join(BASE_DIR, CHANNELS_DIR)\n",
        "    clean_dir(channels_dir)\n",
        "        \n",
        "    for user in USERS:\n",
        "    # for gesture in GESTURES:\n",
        "        print('Processing data for user ' + user, end=' ')\n",
        "        # print(f\"processing data for gesture {gesture} \", end=\"...\" )\n",
        "        \n",
        "        X = []\n",
        "        y = []\n",
        "        first_time = True\n",
        "        \n",
        "        for gesture in GESTURES:\n",
        "        # for user in USERS:\n",
        "              \n",
        "            user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
        "            gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
        "\n",
        "            dataset = pd.read_csv(gesture_dir)\n",
        "\n",
        "            dataset['flex_1'] = dataset['flex_1'].rolling(3).median()\n",
        "            dataset['flex_2'] = dataset['flex_2'].rolling(3).median()\n",
        "            dataset['flex_3'] = dataset['flex_3'].rolling(3).median()\n",
        "            dataset['flex_4'] = dataset['flex_4'].rolling(3).median()\n",
        "            dataset['flex_5'] = dataset['flex_5'].rolling(3).median()\n",
        "\n",
        "            dataset.fillna(0, inplace=True)\n",
        "\n",
        "            # flex = ['flex_1', 'flex_2', 'flex_3', 'flex_4', 'flex_5']\n",
        "            # max_flex = dataset[flex].max(axis=1)\n",
        "            # max_flex.replace(0, 1, inplace=True)\n",
        "            # dataset[flex] = dataset[flex].divide(max_flex, axis=0)\n",
        "            \n",
        "            flx1 = dataset['flex_1'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx2 = dataset['flex_2'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx3 = dataset['flex_3'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx4 = dataset['flex_4'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx5 = dataset['flex_5'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            accx = dataset['ACCx'].to_numpy()\n",
        "            accy = dataset['ACCy'].to_numpy()\n",
        "            accz = dataset['ACCz'].to_numpy()\n",
        "            \n",
        "            accx = LowPassFilter.apply(accx).reshape(-1, WINDOW_LEN)\n",
        "            accy = LowPassFilter.apply(accy).reshape(-1, WINDOW_LEN)\n",
        "            accz = LowPassFilter.apply(accz).reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            gyrx = dataset['GYRx'].to_numpy()\n",
        "            gyry = dataset['GYRy'].to_numpy()\n",
        "            gyrz = dataset['GYRz'].to_numpy()\n",
        "            \n",
        "            gyrx = LowPassFilter.apply(gyrx).reshape(-1, WINDOW_LEN)\n",
        "            gyry = LowPassFilter.apply(gyry).reshape(-1, WINDOW_LEN)\n",
        "            gyrz = LowPassFilter.apply(gyrz).reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            accm = np.sqrt(accx ** 2 + accy ** 2 + accz ** 2)\n",
        "            gyrm = np.sqrt(gyrx ** 2 + gyry ** 2 + gyrz ** 2)\n",
        "            \n",
        "            g_idx = GESTURES.index(gesture)\n",
        "            labels = np.ones((accx.shape[0], 1)) * g_idx\n",
        "            \n",
        "            channels = np.stack([\n",
        "                flx1, flx2, flx3, flx4, flx5,\n",
        "                accx, accy, accz\n",
        "            ], axis=-1)\n",
        "            \n",
        "            if first_time == True:\n",
        "                X = channels\n",
        "                y = labels\n",
        "                first_time = False\n",
        "            else:\n",
        "                X = np.append(X, channels, axis=0)\n",
        "                y = np.append(y, labels, axis=0)\n",
        "            \n",
        "        \n",
        "        x_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_X.joblib')\n",
        "        y_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_y.joblib')\n",
        "        joblib.dump(X, x_path)\n",
        "        joblib.dump(y, y_path)\n",
        "        \n",
        "        print('√')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "QmReRMF-qQcl",
      "metadata": {
        "id": "QmReRMF-qQcl"
      },
      "outputs": [],
      "source": [
        "# ------------- Spatial Path Image Generation ----------- #\n",
        "\n",
        "def clean_dir(path):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "    \n",
        "    print('creating ' + path + ' directory ... ', end='')\n",
        "    os.mkdir(path)\n",
        "    print('√')\n",
        "    \n",
        "# ----------- Spatial Path Vector Calculation ----------- #\n",
        "\n",
        "def get_displacement(acc):\n",
        "    v = np.zeros(acc.shape)\n",
        "    d = np.zeros(acc.shape)\n",
        "    for i in range(acc.shape[0] - 1):\n",
        "        v[i + 1] = v[i] + acc[i] * DT\n",
        "        d[i + 1] = v[i] * DT + 0.5 * acc[i] * DT * DT\n",
        "        \n",
        "    return d\n",
        "\n",
        "def write_image(x, y, path):\n",
        "    fig, ax = plt.subplots(frameon=True, figsize=(3, 3))\n",
        "    ax.axis('off')\n",
        "    plt.scatter(x, y, s=SHAPES, c='black')\n",
        "    plt.savefig(path)\n",
        "    plt.close()\n",
        "\n",
        "def generate_bw_images():\n",
        "    image_dir = os.path.join(BASE_DIR, BW_IMG_DIR)\n",
        "    clean_dir(image_dir)\n",
        "    \n",
        "    for plane in PLANES:\n",
        "        print('processing spatial path images for ' + plane + ' plane ... ', end='')\n",
        "        plane_dir = os.path.join(image_dir, plane)\n",
        "        os.mkdir(plane_dir)\n",
        "        \n",
        "        # for gesture in GESTURES:\n",
        "        #     os.mkdir(os.path.join(plane_dir, gesture))\n",
        "\n",
        "        for user in USERS:\n",
        "            os.mkdir(os.path.join(plane_dir, user))\n",
        "    \n",
        "            # for user in USERS:\n",
        "            for gesture in GESTURES:\n",
        "                os.mkdir(os.path.join(plane_dir, user, gesture))\n",
        "                user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
        "                gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
        "                \n",
        "                accx = pd.read_csv(gesture_dir)['ACCx'].to_numpy()\n",
        "                accy = pd.read_csv(gesture_dir)['ACCy'].to_numpy()\n",
        "                accz = pd.read_csv(gesture_dir)['ACCz'].to_numpy()\n",
        "\n",
        "                x = get_displacement(accx).reshape(-1, 150)\n",
        "                y = get_displacement(accy).reshape(-1, 150)\n",
        "                z = get_displacement(accz).reshape(-1, 150)\n",
        "\n",
        "                count = 0\n",
        "                for i in range(x.shape[0]):\n",
        "                    # image_name = 'u' + user + '_g' + '{:0>2d}'.format(GESTURES.index(gesture)) + \\\n",
        "                    #              '_s' + '{:0>7d}'.format(count) + '_p' + plane + '.jpg'\n",
        "                    image_name = \"{:0>7d}\".format(count) + \".jpg\"\n",
        "                    path = os.path.join(plane_dir, user, gesture, image_name)\n",
        "                    \n",
        "                    if plane == 'XY':\n",
        "                        write_image(x[i, :], y[i, :], path)\n",
        "                    elif plane == 'YZ':\n",
        "                        write_image(y[i, :], z[i, :], path)\n",
        "                    else:\n",
        "                        write_image(z[i, :], x[i, :], path)\n",
        "\n",
        "                    count = count + 1\n",
        "            \n",
        "        print('√')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "07jhA8ukqScv",
      "metadata": {
        "id": "07jhA8ukqScv"
      },
      "outputs": [],
      "source": [
        "# extract_channels()\n",
        "# generate_bw_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "stylish-banner",
      "metadata": {
        "id": "stylish-banner"
      },
      "outputs": [],
      "source": [
        "def load_data(plane, test_user):\n",
        "    X_train = np.zeros((TRAIN_LEN, IMG_LEN, IMG_LEN, 3), dtype='uint8')\n",
        "    X_test = np.zeros((TEST_LEN, IMG_LEN, IMG_LEN, 3), dtype='uint8')\n",
        "    y_train = np.zeros((TRAIN_LEN, 1), dtype='uint8')\n",
        "    y_test = np.zeros((TEST_LEN, 1), dtype='uint8')\n",
        "    \n",
        "    train_count = 0\n",
        "    test_count = 0\n",
        "        \n",
        "    # for gesture in GESTURES:\n",
        "    for user in USERS:\n",
        "        print('loading data for user ' + user + ' on the ' + plane + ' plane ... ', end='')\n",
        "        user_dir = os.path.join(BASE_DIR, IMG_DIR, plane, user)\n",
        "\n",
        "        for gesture in GESTURES:\n",
        "            gesture_dir = os.path.join(user_dir, gesture)\n",
        "\n",
        "            # for filename in os.listdir(path):\n",
        "\n",
        "            for count in range(10):\n",
        "                image_name = \"{:0>7d}\".format(count) + \".jpg\"\n",
        "                img = cv2.imread(os.path.join(gesture_dir, image_name))\n",
        "                resized = cv2.resize(img, IMG_SIZE)\n",
        "                # resized = np.expand_dims(resized, axis=-1)\n",
        "                # label = int(filename[6:8])\n",
        "                if user != test_user:\n",
        "                    X_train[train_count, :] = resized\n",
        "                    y_train[train_count, 0] = GESTURES.index(gesture)\n",
        "                    train_count = train_count + 1\n",
        "                else:\n",
        "                    X_test[test_count, :] = resized\n",
        "                    y_test[test_count, 0] = GESTURES.index(gesture)\n",
        "                    test_count = test_count + 1\n",
        "                \n",
        "        print('√')\n",
        "        \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def load_and_save_data(plane):\n",
        "    X = np.zeros((DATASET_LEN, IMG_LEN, IMG_LEN, 1))\n",
        "    y = np.zeros((DATASET_LEN, 1))\n",
        "    \n",
        "    train_count = 0\n",
        "    test_count  = TRAIN_LEN\n",
        "        \n",
        "    for gesture in GESTURES:\n",
        "        print('loading data for ' + gesture + ' gesture on the ' + plane + ' plane ... ', end='')\n",
        "        path = os.path.join(BASE_DIR, IMG_DIR, plane, gesture)\n",
        "        for filename in os.listdir(path):\n",
        "            img = cv2.imread(os.path.join(path, filename), cv2.IMREAD_GRAYSCALE)\n",
        "            resized = cv2.resize(img, IMG_SIZE)\n",
        "            if filename[1:4] != TEST_USER:\n",
        "                X[train_count, :] = resized\n",
        "                y[train_count, 0] = GESTURES.index(gesture)\n",
        "                train_count = train_count + 1\n",
        "            else:\n",
        "                X[test_count, :] = resized\n",
        "                y[test_count, 0] = GESTURES.index(gesture)\n",
        "                test_count = test_count + 1\n",
        "                \n",
        "        print('√')\n",
        "\n",
        "    joblib.dump(X, BASE_DIR + 'X_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    joblib.dump(y, BASE_DIR + 'Y_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "\n",
        "def load_data_from_joblib(plane):\n",
        "    print('Loading data for ' + plane + ' plane ... ', end='')\n",
        "    X = joblib.load(BASE_DIR + 'X_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    y = joblib.load(BASE_DIR + 'Y_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
        "    test_user = int(TEST_USER)\n",
        "    X_train = X[:TRAIN_LEN, :, :, :]\n",
        "    y_train = y[:TRAIN_LEN, :]\n",
        "    X_test = X[TRAIN_LEN:, :, :, :]\n",
        "    y_test = y[TRAIN_LEN:, :]\n",
        "\n",
        "    print('√')\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "extra-disclosure",
      "metadata": {
        "id": "extra-disclosure"
      },
      "outputs": [],
      "source": [
        "# X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data('XY')\n",
        "# X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data('YZ')\n",
        "# X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data('ZX')\n",
        "\n",
        "# Save to Google  Drive\n",
        "# load_and_save_data('XY')X_train_xy, y_train_xy = shuffle(X_train_xy, y_train_xy)\n",
        "# load_and_save_data('YZ')\n",
        "# load_and_save_data('ZX')\n",
        "\n",
        "# Load from Google Drive\n",
        "# X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data_from_joblib('XY')\n",
        "# X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data_from_joblib('YZ')\n",
        "# X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data_from_joblib('ZX')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "conditional-advisory",
      "metadata": {
        "id": "conditional-advisory"
      },
      "outputs": [],
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "internal-arkansas",
      "metadata": {
        "id": "internal-arkansas",
        "outputId": "e26ca62a-a2aa-406e-8ea9-75b0d34a91a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "\n",
        "# ... Change Model\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "strange-encoding",
      "metadata": {
        "id": "strange-encoding"
      },
      "outputs": [],
      "source": [
        "global_average_layer = GlobalAvgPool2D()\n",
        "prediction_layer = Dense(len(GESTURES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6e049e9a",
      "metadata": {
        "id": "6e049e9a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.keras.layers.pooling import MaxPooling2D\n",
        "\n",
        "def get_conv_block_1D():\n",
        "    input = Input(shape=(150, 1))\n",
        "    x = BatchNormalization()(input)\n",
        "    x = Conv1D(filters=8, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='relu', padding='valid')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(50, activation='relu')(x)\n",
        "\n",
        "    return input, x\n",
        "\n",
        "def get_conv_block_2D():\n",
        "    input = tf.keras.layers.Input(shape=IMG_SHAPE)\n",
        "    # x = data_augmentation(input)\n",
        "    x = preprocess_input(input)\n",
        "    x = base_model(x, training=False)\n",
        "    x = global_average_layer(x)\n",
        "\n",
        "    # x = layers.Conv2D(16, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),)(\n",
        "    #     input\n",
        "    # )\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.MaxPooling2D()(x)\n",
        "    # x = layers.Conv2D(32, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),)(\n",
        "    #     x\n",
        "    # )\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.MaxPooling2D()(x)\n",
        "    # x = layers.Conv2D(\n",
        "    #     64, 3, padding=\"valid\", kernel_regularizer=regularizers.l2(0.001),\n",
        "    # )(x)\n",
        "    # x = layers.BatchNormalization()(x)\n",
        "    # x = tf.keras.activations.relu(x)\n",
        "    # x = layers.Flatten()(x)\n",
        "\n",
        "    return input, x\n",
        "\n",
        "\n",
        "def get_stacked_model():\n",
        "    inputs = []\n",
        "    CNNs = []\n",
        "\n",
        "    for i in range(5):\n",
        "        input_i, CNN_i = get_conv_block_1D()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    for i in range(3):\n",
        "        input_i, CNN_i = get_conv_block_2D()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    x = concatenate(CNNs, axis=-1)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    output = Dense(16, activation=\"softmax\")(x)\n",
        "    model = Model(inputs, output)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    loss = SparseCategoricalCrossentropy(from_logits=False)\n",
        "    model.compile(loss=loss, optimizer=opt, metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "WTWKoIlYyzKW",
      "metadata": {
        "id": "WTWKoIlYyzKW",
        "outputId": "028080d6-2e8c-4603-afa6-d739f090ab78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_5 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " input_6 (InputLayer)           [(None, 150, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 150, 1)      4           ['input_2[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 150, 1)      4           ['input_3[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 150, 1)      4           ['input_4[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 150, 1)      4           ['input_5[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 150, 1)      4           ['input_6[0][0]']                \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 148, 8)       32          ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 148, 8)       32          ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 148, 8)       32          ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 148, 8)       32          ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_16 (Conv1D)             (None, 148, 8)       32          ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 146, 16)      400         ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 146, 16)      400         ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 146, 16)      400         ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 146, 16)      400         ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_17 (Conv1D)             (None, 146, 16)      400         ['conv1d_16[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 73, 16)       0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_6 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_13[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPooling1D)  (None, 73, 16)      0           ['conv1d_17[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 71, 16)       784         ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 71, 16)       784         ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_6[0][0]']        \n",
            "                                                                                                  \n",
            " conv1d_18 (Conv1D)             (None, 71, 16)       784         ['max_pooling1d_8[0][0]']        \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " input_9 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 69, 16)       784         ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 69, 16)       784         ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 69, 16)       784         ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 69, 16)       784         ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_19 (Conv1D)             (None, 69, 16)       784         ['conv1d_18[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.truediv (TFOpLambda)   (None, 224, 224, 3)  0           ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.truediv_1 (TFOpLambda)  (None, 224, 224, 3)  0          ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " tf.math.truediv_2 (TFOpLambda)  (None, 224, 224, 3)  0          ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_5 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_11[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_7 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_15[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPooling1D)  (None, 34, 16)      0           ['conv1d_19[0][0]']              \n",
            "                                                                                                  \n",
            " tf.math.subtract (TFOpLambda)  (None, 224, 224, 3)  0           ['tf.math.truediv[0][0]']        \n",
            "                                                                                                  \n",
            " tf.math.subtract_1 (TFOpLambda  (None, 224, 224, 3)  0          ['tf.math.truediv_1[0][0]']      \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.math.subtract_2 (TFOpLambda  (None, 224, 224, 3)  0          ['tf.math.truediv_2[0][0]']      \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 544)          0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 544)          0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 544)          0           ['max_pooling1d_5[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 544)          0           ['max_pooling1d_7[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 544)          0           ['max_pooling1d_9[0][0]']        \n",
            "                                                                                                  \n",
            " mobilenetv2_1.00_224 (Function  (None, 7, 7, 1280)  2257984     ['tf.math.subtract[0][0]',       \n",
            " al)                                                              'tf.math.subtract_1[0][0]',     \n",
            "                                                                  'tf.math.subtract_2[0][0]']     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 50)           27250       ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 50)           27250       ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 50)           27250       ['flatten_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 50)           27250       ['flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 50)           27250       ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 1280)        0           ['mobilenetv2_1.00_224[0][0]',   \n",
            " alAveragePooling2D)                                              'mobilenetv2_1.00_224[1][0]',   \n",
            "                                                                  'mobilenetv2_1.00_224[2][0]']   \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 4090)         0           ['dense_1[0][0]',                \n",
            "                                                                  'dense_2[0][0]',                \n",
            "                                                                  'dense_3[0][0]',                \n",
            "                                                                  'dense_4[0][0]',                \n",
            "                                                                  'dense_5[0][0]',                \n",
            "                                                                  'global_average_pooling2d[0][0]'\n",
            "                                                                 , 'global_average_pooling2d[1][0]\n",
            "                                                                 ',                               \n",
            "                                                                  'global_average_pooling2d[2][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 4090)         0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 128)          523648      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 16)           2064        ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,929,966\n",
            "Trainable params: 671,972\n",
            "Non-trainable params: 2,257,994\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_stacked_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "upper-tobago",
      "metadata": {
        "id": "upper-tobago"
      },
      "outputs": [],
      "source": [
        "# def get_model():\n",
        "#     inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
        "#     #     x = data_augmentation(inputs)\n",
        "#     x = preprocess_input(inputs)\n",
        "#     x = base_model(x, training=False)\n",
        "#     x = global_average_layer(x)\n",
        "#     x = tf.keras.layers.Dropout(0.2)(x)\n",
        "#     outputs = prediction_layer(x)\n",
        "#     model = tf.keras.Model(inputs, outputs)\n",
        "#     model.compile(\n",
        "#         optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE, decay=DECAY),\n",
        "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#         metrics=[\"accuracy\"],\n",
        "#     )\n",
        "#     return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b3a457f4",
      "metadata": {
        "id": "b3a457f4",
        "outputId": "0d68cf6f-bf19-458c-aadd-3e5711fd7c54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing results for user 009... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 52s 534ms/step - loss: 2.5951 - accuracy: 0.2013 - val_loss: 1.6671 - val_accuracy: 0.7063\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 1.5813 - accuracy: 0.4961 - val_loss: 0.5938 - val_accuracy: 0.9062\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.8730 - accuracy: 0.7232 - val_loss: 0.2358 - val_accuracy: 0.9937\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.5944 - accuracy: 0.8125 - val_loss: 0.1715 - val_accuracy: 1.0000\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 28s 468ms/step - loss: 0.4306 - accuracy: 0.8633 - val_loss: 0.0928 - val_accuracy: 1.0000\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.3632 - accuracy: 0.8852 - val_loss: 0.0915 - val_accuracy: 0.9937\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.2946 - accuracy: 0.9049 - val_loss: 0.0685 - val_accuracy: 1.0000\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.2637 - accuracy: 0.9115 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.2141 - accuracy: 0.9315 - val_loss: 0.0390 - val_accuracy: 1.0000\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 28s 466ms/step - loss: 0.1996 - accuracy: 0.9380 - val_loss: 0.0462 - val_accuracy: 0.9937\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.1891 - accuracy: 0.9391 - val_loss: 0.0322 - val_accuracy: 1.0000\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.1653 - accuracy: 0.9503 - val_loss: 0.0303 - val_accuracy: 1.0000\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.1532 - accuracy: 0.9536 - val_loss: 0.0360 - val_accuracy: 0.9937\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.1342 - accuracy: 0.9581 - val_loss: 0.0235 - val_accuracy: 1.0000\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 28s 468ms/step - loss: 0.1275 - accuracy: 0.9609 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.1201 - accuracy: 0.9594 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.1138 - accuracy: 0.9641 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.1033 - accuracy: 0.9656 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 28s 468ms/step - loss: 0.0977 - accuracy: 0.9703 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.1003 - accuracy: 0.9703 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 28s 466ms/step - loss: 0.0840 - accuracy: 0.9753 - val_loss: 0.0164 - val_accuracy: 1.0000\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.0870 - accuracy: 0.9729 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.0785 - accuracy: 0.9750 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.0764 - accuracy: 0.9750 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.0728 - accuracy: 0.9781 - val_loss: 0.0131 - val_accuracy: 1.0000\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.0706 - accuracy: 0.9760 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.0597 - accuracy: 0.9799 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.0586 - accuracy: 0.9802 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 28s 466ms/step - loss: 0.0594 - accuracy: 0.9826 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.0533 - accuracy: 0.9841 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.0501 - accuracy: 0.9844 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 28s 467ms/step - loss: 0.0519 - accuracy: 0.9836 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 28s 466ms/step - loss: 0.0474 - accuracy: 0.9867 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.0465 - accuracy: 0.9846 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "3/3 [==============================] - 1s 341ms/step - loss: 0.1715 - accuracy: 1.0000\n",
            "100.00 %\n",
            "Processing results for user 010... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 41s 520ms/step - loss: 2.5922 - accuracy: 0.2029 - val_loss: 1.7094 - val_accuracy: 0.6187\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 1.5966 - accuracy: 0.5078 - val_loss: 0.7903 - val_accuracy: 0.7937\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.9205 - accuracy: 0.7227 - val_loss: 0.4393 - val_accuracy: 0.8750\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.5969 - accuracy: 0.8109 - val_loss: 0.3421 - val_accuracy: 0.8875\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.4424 - accuracy: 0.8648 - val_loss: 0.3067 - val_accuracy: 0.8938\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.3608 - accuracy: 0.8846 - val_loss: 0.2597 - val_accuracy: 0.9250\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.2995 - accuracy: 0.9068 - val_loss: 0.2687 - val_accuracy: 0.9250\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.2682 - accuracy: 0.9185 - val_loss: 0.2563 - val_accuracy: 0.9250\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.2165 - accuracy: 0.9344 - val_loss: 0.2503 - val_accuracy: 0.9375\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.1897 - accuracy: 0.9383 - val_loss: 0.2156 - val_accuracy: 0.9375\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.1793 - accuracy: 0.9414 - val_loss: 0.2199 - val_accuracy: 0.9375\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1705 - accuracy: 0.9456 - val_loss: 0.2150 - val_accuracy: 0.9438\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.1474 - accuracy: 0.9531 - val_loss: 0.2144 - val_accuracy: 0.9375\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.1322 - accuracy: 0.9581 - val_loss: 0.2428 - val_accuracy: 0.9375\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.1207 - accuracy: 0.9612 - val_loss: 0.1918 - val_accuracy: 0.9500\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.1158 - accuracy: 0.9599 - val_loss: 0.1859 - val_accuracy: 0.9500\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.1106 - accuracy: 0.9661 - val_loss: 0.1859 - val_accuracy: 0.9563\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.1035 - accuracy: 0.9695 - val_loss: 0.1638 - val_accuracy: 0.9563\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.1017 - accuracy: 0.9659 - val_loss: 0.1630 - val_accuracy: 0.9563\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0888 - accuracy: 0.9745 - val_loss: 0.1409 - val_accuracy: 0.9625\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.0877 - accuracy: 0.9734 - val_loss: 0.1644 - val_accuracy: 0.9563\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 29s 484ms/step - loss: 0.0814 - accuracy: 0.9727 - val_loss: 0.1489 - val_accuracy: 0.9625\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0790 - accuracy: 0.9755 - val_loss: 0.1538 - val_accuracy: 0.9625\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0620 - accuracy: 0.9815 - val_loss: 0.1520 - val_accuracy: 0.9625\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 29s 480ms/step - loss: 0.0654 - accuracy: 0.9826 - val_loss: 0.1621 - val_accuracy: 0.9625\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0674 - accuracy: 0.9792 - val_loss: 0.1403 - val_accuracy: 0.9625\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0644 - accuracy: 0.9823 - val_loss: 0.1515 - val_accuracy: 0.9625\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0593 - accuracy: 0.9831 - val_loss: 0.1142 - val_accuracy: 0.9625\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0608 - accuracy: 0.9807 - val_loss: 0.1201 - val_accuracy: 0.9625\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0520 - accuracy: 0.9836 - val_loss: 0.1234 - val_accuracy: 0.9625\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.1326 - val_accuracy: 0.9625\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0517 - accuracy: 0.9849 - val_loss: 0.1679 - val_accuracy: 0.9563\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0493 - accuracy: 0.9857 - val_loss: 0.1486 - val_accuracy: 0.9688\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 29s 481ms/step - loss: 0.0500 - accuracy: 0.9846 - val_loss: 0.1357 - val_accuracy: 0.9625\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 28s 476ms/step - loss: 0.0391 - accuracy: 0.9878 - val_loss: 0.1454 - val_accuracy: 0.9625\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0470 - accuracy: 0.9875 - val_loss: 0.1156 - val_accuracy: 0.9625\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0369 - accuracy: 0.9904 - val_loss: 0.1319 - val_accuracy: 0.9688\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0430 - accuracy: 0.9865 - val_loss: 0.0847 - val_accuracy: 0.9688\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0397 - accuracy: 0.9862 - val_loss: 0.1198 - val_accuracy: 0.9625\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 29s 483ms/step - loss: 0.0419 - accuracy: 0.9888 - val_loss: 0.1077 - val_accuracy: 0.9625\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.0346 - accuracy: 0.9909 - val_loss: 0.1116 - val_accuracy: 0.9625\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.0345 - accuracy: 0.9888 - val_loss: 0.1070 - val_accuracy: 0.9625\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.0336 - accuracy: 0.9891 - val_loss: 0.1192 - val_accuracy: 0.9563\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0338 - accuracy: 0.9904 - val_loss: 0.1073 - val_accuracy: 0.9625\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0324 - accuracy: 0.9909 - val_loss: 0.1062 - val_accuracy: 0.9688\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0289 - accuracy: 0.9927 - val_loss: 0.0912 - val_accuracy: 0.9625\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0350 - accuracy: 0.9888 - val_loss: 0.0713 - val_accuracy: 0.9688\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 29s 480ms/step - loss: 0.0305 - accuracy: 0.9930 - val_loss: 0.1398 - val_accuracy: 0.9625\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 29s 475ms/step - loss: 0.0284 - accuracy: 0.9914 - val_loss: 0.1143 - val_accuracy: 0.9625\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 29s 481ms/step - loss: 0.0281 - accuracy: 0.9927 - val_loss: 0.0847 - val_accuracy: 0.9688\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.0269 - accuracy: 0.9937 - val_loss: 0.0948 - val_accuracy: 0.9688\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0282 - accuracy: 0.9914 - val_loss: 0.1204 - val_accuracy: 0.9625\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0216 - accuracy: 0.9945 - val_loss: 0.1148 - val_accuracy: 0.9625\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0229 - accuracy: 0.9930 - val_loss: 0.0954 - val_accuracy: 0.9625\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 29s 482ms/step - loss: 0.0219 - accuracy: 0.9930 - val_loss: 0.1167 - val_accuracy: 0.9688\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0233 - accuracy: 0.9937 - val_loss: 0.1201 - val_accuracy: 0.9688\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.0300 - accuracy: 0.9901 - val_loss: 0.1065 - val_accuracy: 0.9625\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 28s 468ms/step - loss: 0.0216 - accuracy: 0.9937 - val_loss: 0.0836 - val_accuracy: 0.9688\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0221 - accuracy: 0.9932 - val_loss: 0.0968 - val_accuracy: 0.9688\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 0.0986 - val_accuracy: 0.9688\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0186 - accuracy: 0.9958 - val_loss: 0.0863 - val_accuracy: 0.9688\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.0697 - val_accuracy: 0.9688\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 29s 480ms/step - loss: 0.0187 - accuracy: 0.9948 - val_loss: 0.0743 - val_accuracy: 0.9688\n",
            "3/3 [==============================] - 1s 333ms/step - loss: 0.1486 - accuracy: 0.9688\n",
            "96.88 %\n",
            "Processing results for user 011... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 41s 521ms/step - loss: 2.6487 - accuracy: 0.1924 - val_loss: 1.7783 - val_accuracy: 0.7188\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 1.6695 - accuracy: 0.4818 - val_loss: 0.7901 - val_accuracy: 0.8250\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.9735 - accuracy: 0.6984 - val_loss: 0.3261 - val_accuracy: 0.9625\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.6466 - accuracy: 0.7969 - val_loss: 0.2030 - val_accuracy: 0.9688\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.4885 - accuracy: 0.8526 - val_loss: 0.1332 - val_accuracy: 0.9812\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.3938 - accuracy: 0.8753 - val_loss: 0.1060 - val_accuracy: 0.9812\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.3349 - accuracy: 0.8961 - val_loss: 0.0722 - val_accuracy: 0.9875\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.2886 - accuracy: 0.9081 - val_loss: 0.0627 - val_accuracy: 0.9937\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.2379 - accuracy: 0.9271 - val_loss: 0.0421 - val_accuracy: 0.9937\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.2015 - accuracy: 0.9365 - val_loss: 0.0390 - val_accuracy: 0.9937\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.1861 - accuracy: 0.9440 - val_loss: 0.0299 - val_accuracy: 0.9937\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.1590 - accuracy: 0.9529 - val_loss: 0.0304 - val_accuracy: 0.9937\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.1619 - accuracy: 0.9471 - val_loss: 0.0283 - val_accuracy: 0.9937\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.1339 - accuracy: 0.9589 - val_loss: 0.0305 - val_accuracy: 0.9937\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.1371 - accuracy: 0.9583 - val_loss: 0.0230 - val_accuracy: 0.9937\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.1330 - accuracy: 0.9617 - val_loss: 0.0202 - val_accuracy: 0.9937\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.1262 - accuracy: 0.9617 - val_loss: 0.0161 - val_accuracy: 0.9937\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.1025 - accuracy: 0.9711 - val_loss: 0.0214 - val_accuracy: 0.9937\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1047 - accuracy: 0.9661 - val_loss: 0.0221 - val_accuracy: 0.9937\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.0945 - accuracy: 0.9732 - val_loss: 0.0182 - val_accuracy: 0.9937\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.0834 - accuracy: 0.9755 - val_loss: 0.0275 - val_accuracy: 0.9937\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0744 - accuracy: 0.9784 - val_loss: 0.0190 - val_accuracy: 0.9937\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.0768 - accuracy: 0.9763 - val_loss: 0.0224 - val_accuracy: 0.9937\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.0704 - accuracy: 0.9802 - val_loss: 0.0227 - val_accuracy: 0.9937\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.0655 - accuracy: 0.9807 - val_loss: 0.0136 - val_accuracy: 0.9937\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.0693 - accuracy: 0.9797 - val_loss: 0.0185 - val_accuracy: 0.9937\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.0747 - accuracy: 0.9768 - val_loss: 0.0185 - val_accuracy: 0.9937\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0651 - accuracy: 0.9805 - val_loss: 0.0229 - val_accuracy: 0.9937\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0603 - accuracy: 0.9839 - val_loss: 0.0189 - val_accuracy: 0.9937\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0594 - accuracy: 0.9820 - val_loss: 0.0275 - val_accuracy: 0.9937\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0186 - val_accuracy: 0.9937\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0538 - accuracy: 0.9846 - val_loss: 0.0213 - val_accuracy: 0.9937\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.0467 - accuracy: 0.9859 - val_loss: 0.0258 - val_accuracy: 0.9937\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0489 - accuracy: 0.9852 - val_loss: 0.0210 - val_accuracy: 0.9937\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.0416 - accuracy: 0.9880 - val_loss: 0.0199 - val_accuracy: 0.9937\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0447 - accuracy: 0.9880 - val_loss: 0.0153 - val_accuracy: 0.9937\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 28s 469ms/step - loss: 0.0450 - accuracy: 0.9846 - val_loss: 0.0217 - val_accuracy: 0.9937\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0417 - accuracy: 0.9875 - val_loss: 0.0179 - val_accuracy: 0.9937\n",
            "3/3 [==============================] - 1s 343ms/step - loss: 0.0627 - accuracy: 0.9937\n",
            "99.37 %\n",
            "Processing results for user 012... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 41s 521ms/step - loss: 2.5733 - accuracy: 0.2104 - val_loss: 1.4172 - val_accuracy: 0.8062\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 1.5971 - accuracy: 0.5052 - val_loss: 0.4864 - val_accuracy: 0.9875\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 1.0035 - accuracy: 0.6888 - val_loss: 0.1570 - val_accuracy: 0.9937\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.6452 - accuracy: 0.8023 - val_loss: 0.0848 - val_accuracy: 0.9937\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.4806 - accuracy: 0.8516 - val_loss: 0.0567 - val_accuracy: 0.9875\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.3699 - accuracy: 0.8885 - val_loss: 0.0429 - val_accuracy: 0.9875\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.3252 - accuracy: 0.8982 - val_loss: 0.0375 - val_accuracy: 0.9937\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.2690 - accuracy: 0.9148 - val_loss: 0.0396 - val_accuracy: 0.9875\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.2398 - accuracy: 0.9273 - val_loss: 0.0308 - val_accuracy: 0.9875\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.2066 - accuracy: 0.9328 - val_loss: 0.0336 - val_accuracy: 0.9875\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.1858 - accuracy: 0.9380 - val_loss: 0.0287 - val_accuracy: 0.9875\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.1774 - accuracy: 0.9438 - val_loss: 0.0288 - val_accuracy: 0.9875\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1507 - accuracy: 0.9510 - val_loss: 0.0269 - val_accuracy: 0.9875\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.1467 - accuracy: 0.9549 - val_loss: 0.0260 - val_accuracy: 0.9875\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 28s 470ms/step - loss: 0.1234 - accuracy: 0.9656 - val_loss: 0.0240 - val_accuracy: 0.9875\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1221 - accuracy: 0.9643 - val_loss: 0.0301 - val_accuracy: 0.9875\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1178 - accuracy: 0.9622 - val_loss: 0.0223 - val_accuracy: 0.9937\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1040 - accuracy: 0.9695 - val_loss: 0.0294 - val_accuracy: 0.9875\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.1034 - accuracy: 0.9680 - val_loss: 0.0263 - val_accuracy: 0.9875\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0951 - accuracy: 0.9701 - val_loss: 0.0206 - val_accuracy: 0.9875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0902 - accuracy: 0.9742 - val_loss: 0.0244 - val_accuracy: 0.9875\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0814 - accuracy: 0.9755 - val_loss: 0.0282 - val_accuracy: 0.9875\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0746 - accuracy: 0.9779 - val_loss: 0.0247 - val_accuracy: 0.9875\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0830 - accuracy: 0.9747 - val_loss: 0.0205 - val_accuracy: 0.9875\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0625 - accuracy: 0.9820 - val_loss: 0.0228 - val_accuracy: 0.9875\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0619 - accuracy: 0.9807 - val_loss: 0.0216 - val_accuracy: 0.9875\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0563 - accuracy: 0.9820 - val_loss: 0.0186 - val_accuracy: 0.9875\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0632 - accuracy: 0.9799 - val_loss: 0.0235 - val_accuracy: 0.9875\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0588 - accuracy: 0.9818 - val_loss: 0.0246 - val_accuracy: 0.9875\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0608 - accuracy: 0.9826 - val_loss: 0.0286 - val_accuracy: 0.9875\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0581 - accuracy: 0.9828 - val_loss: 0.0199 - val_accuracy: 0.9875\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.0488 - accuracy: 0.9859 - val_loss: 0.0245 - val_accuracy: 0.9875\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0539 - accuracy: 0.9818 - val_loss: 0.0198 - val_accuracy: 0.9875\n",
            "3/3 [==============================] - 1s 338ms/step - loss: 0.1570 - accuracy: 0.9937\n",
            "99.37 %\n",
            "Processing results for user 013... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 41s 524ms/step - loss: 2.6006 - accuracy: 0.2096 - val_loss: 1.8123 - val_accuracy: 0.5875\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 1.6258 - accuracy: 0.4956 - val_loss: 0.9216 - val_accuracy: 0.7688\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 1.0119 - accuracy: 0.6849 - val_loss: 0.4396 - val_accuracy: 0.8750\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.6853 - accuracy: 0.7859 - val_loss: 0.2793 - val_accuracy: 0.9375\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.5025 - accuracy: 0.8419 - val_loss: 0.1770 - val_accuracy: 0.9750\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.3930 - accuracy: 0.8771 - val_loss: 0.1567 - val_accuracy: 0.9563\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.3201 - accuracy: 0.8958 - val_loss: 0.1171 - val_accuracy: 0.9750\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.2801 - accuracy: 0.9135 - val_loss: 0.0943 - val_accuracy: 0.9812\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.2477 - accuracy: 0.9224 - val_loss: 0.0937 - val_accuracy: 0.9750\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 29s 480ms/step - loss: 0.2054 - accuracy: 0.9378 - val_loss: 0.0818 - val_accuracy: 0.9875\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.1816 - accuracy: 0.9404 - val_loss: 0.0570 - val_accuracy: 0.9875\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.1776 - accuracy: 0.9419 - val_loss: 0.0515 - val_accuracy: 0.9875\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.1580 - accuracy: 0.9477 - val_loss: 0.0603 - val_accuracy: 0.9812\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.1379 - accuracy: 0.9568 - val_loss: 0.0464 - val_accuracy: 0.9937\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.1326 - accuracy: 0.9568 - val_loss: 0.0407 - val_accuracy: 0.9875\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.1159 - accuracy: 0.9654 - val_loss: 0.0436 - val_accuracy: 0.9812\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.1224 - accuracy: 0.9612 - val_loss: 0.0369 - val_accuracy: 0.9937\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.1006 - accuracy: 0.9677 - val_loss: 0.0444 - val_accuracy: 0.9875\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0977 - accuracy: 0.9698 - val_loss: 0.0347 - val_accuracy: 0.9937\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.1034 - accuracy: 0.9682 - val_loss: 0.0369 - val_accuracy: 0.9875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0867 - accuracy: 0.9747 - val_loss: 0.0333 - val_accuracy: 0.9875\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0885 - accuracy: 0.9740 - val_loss: 0.0333 - val_accuracy: 0.9937\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0685 - accuracy: 0.9786 - val_loss: 0.0280 - val_accuracy: 0.9875\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0759 - accuracy: 0.9753 - val_loss: 0.0344 - val_accuracy: 0.9937\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0754 - accuracy: 0.9766 - val_loss: 0.0396 - val_accuracy: 0.9812\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 29s 475ms/step - loss: 0.0686 - accuracy: 0.9789 - val_loss: 0.0264 - val_accuracy: 0.9875\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 29s 480ms/step - loss: 0.0626 - accuracy: 0.9833 - val_loss: 0.0250 - val_accuracy: 0.9875\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0583 - accuracy: 0.9820 - val_loss: 0.0257 - val_accuracy: 0.9875\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0629 - accuracy: 0.9773 - val_loss: 0.0213 - val_accuracy: 0.9875\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0590 - accuracy: 0.9802 - val_loss: 0.0239 - val_accuracy: 0.9937\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0557 - accuracy: 0.9815 - val_loss: 0.0236 - val_accuracy: 0.9937\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0603 - accuracy: 0.9797 - val_loss: 0.0263 - val_accuracy: 0.9937\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0504 - accuracy: 0.9839 - val_loss: 0.0207 - val_accuracy: 0.9937\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0519 - accuracy: 0.9852 - val_loss: 0.0223 - val_accuracy: 0.9937\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0504 - accuracy: 0.9862 - val_loss: 0.0222 - val_accuracy: 0.9937\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0484 - accuracy: 0.9854 - val_loss: 0.0202 - val_accuracy: 0.9875\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0412 - accuracy: 0.9883 - val_loss: 0.0214 - val_accuracy: 0.9937\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0449 - accuracy: 0.9857 - val_loss: 0.0224 - val_accuracy: 0.9937\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0414 - accuracy: 0.9883 - val_loss: 0.0175 - val_accuracy: 0.9875\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0373 - accuracy: 0.9898 - val_loss: 0.0182 - val_accuracy: 0.9937\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0367 - accuracy: 0.9888 - val_loss: 0.0189 - val_accuracy: 0.9937\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0414 - accuracy: 0.9883 - val_loss: 0.0174 - val_accuracy: 0.9875\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0355 - accuracy: 0.9898 - val_loss: 0.0239 - val_accuracy: 0.9937\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0396 - accuracy: 0.9891 - val_loss: 0.0197 - val_accuracy: 0.9875\n",
            "3/3 [==============================] - 1s 336ms/step - loss: 0.0464 - accuracy: 0.9937\n",
            "99.37 %\n",
            "Processing results for user 014... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 42s 528ms/step - loss: 2.6464 - accuracy: 0.1906 - val_loss: 1.6288 - val_accuracy: 0.7000\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 1.6179 - accuracy: 0.4875 - val_loss: 0.6545 - val_accuracy: 0.9375\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 1.0128 - accuracy: 0.6737 - val_loss: 0.2618 - val_accuracy: 0.9563\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.6466 - accuracy: 0.7990 - val_loss: 0.1544 - val_accuracy: 0.9625\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.4785 - accuracy: 0.8505 - val_loss: 0.1447 - val_accuracy: 0.9625\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.3863 - accuracy: 0.8805 - val_loss: 0.1131 - val_accuracy: 0.9625\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.3047 - accuracy: 0.9068 - val_loss: 0.0871 - val_accuracy: 0.9688\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.2648 - accuracy: 0.9154 - val_loss: 0.1035 - val_accuracy: 0.9625\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.2315 - accuracy: 0.9232 - val_loss: 0.0770 - val_accuracy: 0.9688\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.1992 - accuracy: 0.9372 - val_loss: 0.0737 - val_accuracy: 0.9688\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.1770 - accuracy: 0.9453 - val_loss: 0.0714 - val_accuracy: 0.9750\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.1802 - accuracy: 0.9419 - val_loss: 0.0574 - val_accuracy: 0.9812\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.1512 - accuracy: 0.9565 - val_loss: 0.0582 - val_accuracy: 0.9812\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1437 - accuracy: 0.9576 - val_loss: 0.0484 - val_accuracy: 0.9812\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.1136 - accuracy: 0.9685 - val_loss: 0.0539 - val_accuracy: 0.9812\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.1152 - accuracy: 0.9635 - val_loss: 0.0364 - val_accuracy: 0.9875\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.1185 - accuracy: 0.9609 - val_loss: 0.0619 - val_accuracy: 0.9750\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.1108 - accuracy: 0.9659 - val_loss: 0.0589 - val_accuracy: 0.9750\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.1025 - accuracy: 0.9664 - val_loss: 0.0628 - val_accuracy: 0.9750\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0960 - accuracy: 0.9682 - val_loss: 0.0500 - val_accuracy: 0.9812\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0884 - accuracy: 0.9734 - val_loss: 0.0581 - val_accuracy: 0.9812\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0830 - accuracy: 0.9771 - val_loss: 0.0510 - val_accuracy: 0.9812\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0752 - accuracy: 0.9794 - val_loss: 0.0387 - val_accuracy: 0.9812\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0713 - accuracy: 0.9794 - val_loss: 0.0622 - val_accuracy: 0.9750\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0698 - accuracy: 0.9794 - val_loss: 0.0652 - val_accuracy: 0.9750\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0687 - accuracy: 0.9802 - val_loss: 0.0633 - val_accuracy: 0.9750\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0712 - accuracy: 0.9747 - val_loss: 0.0638 - val_accuracy: 0.9750\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0582 - accuracy: 0.9823 - val_loss: 0.0648 - val_accuracy: 0.9750\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0646 - accuracy: 0.9784 - val_loss: 0.0709 - val_accuracy: 0.9688\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0508 - val_accuracy: 0.9812\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0573 - accuracy: 0.9833 - val_loss: 0.0545 - val_accuracy: 0.9812\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0496 - accuracy: 0.9846 - val_loss: 0.0468 - val_accuracy: 0.9812\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0514 - accuracy: 0.9841 - val_loss: 0.0480 - val_accuracy: 0.9812\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0502 - accuracy: 0.9839 - val_loss: 0.0633 - val_accuracy: 0.9750\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0517 - accuracy: 0.9867 - val_loss: 0.0513 - val_accuracy: 0.9812\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.0469 - accuracy: 0.9862 - val_loss: 0.0534 - val_accuracy: 0.9812\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0447 - accuracy: 0.9849 - val_loss: 0.0497 - val_accuracy: 0.9812\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0406 - accuracy: 0.9872 - val_loss: 0.0710 - val_accuracy: 0.9750\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0466 - accuracy: 0.9852 - val_loss: 0.0648 - val_accuracy: 0.9750\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0400 - accuracy: 0.9883 - val_loss: 0.0459 - val_accuracy: 0.9812\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0370 - accuracy: 0.9885 - val_loss: 0.0452 - val_accuracy: 0.9812\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0352 - accuracy: 0.9901 - val_loss: 0.0742 - val_accuracy: 0.9688\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0368 - accuracy: 0.9888 - val_loss: 0.0560 - val_accuracy: 0.9812\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.0340 - accuracy: 0.9901 - val_loss: 0.0467 - val_accuracy: 0.9812\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0368 - accuracy: 0.9901 - val_loss: 0.0392 - val_accuracy: 0.9875\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0375 - accuracy: 0.9891 - val_loss: 0.0710 - val_accuracy: 0.9750\n",
            "3/3 [==============================] - 1s 342ms/step - loss: 0.0364 - accuracy: 0.9875\n",
            "98.75 %\n",
            "Processing results for user 015... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 43s 528ms/step - loss: 2.5849 - accuracy: 0.2029 - val_loss: 1.6933 - val_accuracy: 0.6938\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 1.6268 - accuracy: 0.4862 - val_loss: 0.8207 - val_accuracy: 0.7875\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.9440 - accuracy: 0.7086 - val_loss: 0.4256 - val_accuracy: 0.9187\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 29s 475ms/step - loss: 0.5990 - accuracy: 0.8148 - val_loss: 0.3009 - val_accuracy: 0.9250\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.4524 - accuracy: 0.8612 - val_loss: 0.2372 - val_accuracy: 0.9375\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.3814 - accuracy: 0.8732 - val_loss: 0.1872 - val_accuracy: 0.9500\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.3077 - accuracy: 0.9023 - val_loss: 0.1793 - val_accuracy: 0.9500\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.2545 - accuracy: 0.9198 - val_loss: 0.1567 - val_accuracy: 0.9625\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.2216 - accuracy: 0.9328 - val_loss: 0.1544 - val_accuracy: 0.9688\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.1869 - accuracy: 0.9411 - val_loss: 0.1380 - val_accuracy: 0.9688\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.1744 - accuracy: 0.9445 - val_loss: 0.1308 - val_accuracy: 0.9625\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 29s 475ms/step - loss: 0.1555 - accuracy: 0.9508 - val_loss: 0.0994 - val_accuracy: 0.9875\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.1503 - accuracy: 0.9539 - val_loss: 0.1091 - val_accuracy: 0.9688\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1388 - accuracy: 0.9568 - val_loss: 0.1084 - val_accuracy: 0.9750\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.1177 - accuracy: 0.9604 - val_loss: 0.1077 - val_accuracy: 0.9750\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1248 - accuracy: 0.9607 - val_loss: 0.0909 - val_accuracy: 0.9812\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.1075 - accuracy: 0.9672 - val_loss: 0.0931 - val_accuracy: 0.9812\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0972 - accuracy: 0.9734 - val_loss: 0.0804 - val_accuracy: 0.9875\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0941 - accuracy: 0.9727 - val_loss: 0.0738 - val_accuracy: 0.9875\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1005 - accuracy: 0.9674 - val_loss: 0.0872 - val_accuracy: 0.9875\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0869 - accuracy: 0.9740 - val_loss: 0.0895 - val_accuracy: 0.9750\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0762 - accuracy: 0.9758 - val_loss: 0.1024 - val_accuracy: 0.9625\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0751 - accuracy: 0.9786 - val_loss: 0.1054 - val_accuracy: 0.9688\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0682 - accuracy: 0.9792 - val_loss: 0.0644 - val_accuracy: 0.9812\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0643 - accuracy: 0.9810 - val_loss: 0.0618 - val_accuracy: 0.9812\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0694 - accuracy: 0.9805 - val_loss: 0.0856 - val_accuracy: 0.9812\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0612 - accuracy: 0.9823 - val_loss: 0.0901 - val_accuracy: 0.9750\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0593 - accuracy: 0.9812 - val_loss: 0.0862 - val_accuracy: 0.9688\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0600 - accuracy: 0.9802 - val_loss: 0.0879 - val_accuracy: 0.9812\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0634 - accuracy: 0.9794 - val_loss: 0.0837 - val_accuracy: 0.9688\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.0510 - accuracy: 0.9852 - val_loss: 0.0848 - val_accuracy: 0.9812\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0486 - accuracy: 0.9854 - val_loss: 0.0812 - val_accuracy: 0.9875\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 28s 476ms/step - loss: 0.0515 - accuracy: 0.9833 - val_loss: 0.0824 - val_accuracy: 0.9812\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0528 - accuracy: 0.9833 - val_loss: 0.0835 - val_accuracy: 0.9750\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0467 - accuracy: 0.9872 - val_loss: 0.0879 - val_accuracy: 0.9812\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0452 - accuracy: 0.9857 - val_loss: 0.0644 - val_accuracy: 0.9937\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0480 - accuracy: 0.9875 - val_loss: 0.0657 - val_accuracy: 0.9875\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0452 - accuracy: 0.9857 - val_loss: 0.0569 - val_accuracy: 0.9937\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0353 - accuracy: 0.9891 - val_loss: 0.0620 - val_accuracy: 0.9875\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0385 - accuracy: 0.9875 - val_loss: 0.0776 - val_accuracy: 0.9750\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0355 - accuracy: 0.9896 - val_loss: 0.0552 - val_accuracy: 0.9875\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0327 - accuracy: 0.9898 - val_loss: 0.0591 - val_accuracy: 0.9875\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0330 - accuracy: 0.9898 - val_loss: 0.0587 - val_accuracy: 0.9875\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0294 - accuracy: 0.9932 - val_loss: 0.0485 - val_accuracy: 0.9937\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0341 - accuracy: 0.9904 - val_loss: 0.0568 - val_accuracy: 0.9875\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0312 - accuracy: 0.9904 - val_loss: 0.0550 - val_accuracy: 0.9875\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0323 - accuracy: 0.9898 - val_loss: 0.0631 - val_accuracy: 0.9812\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0298 - accuracy: 0.9919 - val_loss: 0.0403 - val_accuracy: 0.9875\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0280 - accuracy: 0.9935 - val_loss: 0.0679 - val_accuracy: 0.9750\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0258 - accuracy: 0.9935 - val_loss: 0.0516 - val_accuracy: 0.9875\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0313 - accuracy: 0.9906 - val_loss: 0.0617 - val_accuracy: 0.9875\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0291 - accuracy: 0.9898 - val_loss: 0.0753 - val_accuracy: 0.9812\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0247 - accuracy: 0.9940 - val_loss: 0.0532 - val_accuracy: 0.9937\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0238 - accuracy: 0.9932 - val_loss: 0.0812 - val_accuracy: 0.9750\n",
            "Epoch 55/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0251 - accuracy: 0.9927 - val_loss: 0.0411 - val_accuracy: 0.9875\n",
            "Epoch 56/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0263 - accuracy: 0.9927 - val_loss: 0.0617 - val_accuracy: 0.9812\n",
            "Epoch 57/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0215 - accuracy: 0.9948 - val_loss: 0.0544 - val_accuracy: 0.9937\n",
            "Epoch 58/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0245 - accuracy: 0.9927 - val_loss: 0.0754 - val_accuracy: 0.9812\n",
            "Epoch 59/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0231 - accuracy: 0.9940 - val_loss: 0.0717 - val_accuracy: 0.9812\n",
            "Epoch 60/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0227 - accuracy: 0.9937 - val_loss: 0.0570 - val_accuracy: 0.9875\n",
            "Epoch 61/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0212 - accuracy: 0.9943 - val_loss: 0.0332 - val_accuracy: 0.9937\n",
            "Epoch 62/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0193 - accuracy: 0.9953 - val_loss: 0.0545 - val_accuracy: 0.9875\n",
            "Epoch 63/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0198 - accuracy: 0.9937 - val_loss: 0.0449 - val_accuracy: 0.9937\n",
            "Epoch 64/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0194 - accuracy: 0.9940 - val_loss: 0.0581 - val_accuracy: 0.9875\n",
            "Epoch 65/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.0557 - val_accuracy: 0.9937\n",
            "Epoch 66/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.0501 - val_accuracy: 0.9937\n",
            "3/3 [==============================] - 1s 331ms/step - loss: 0.0644 - accuracy: 0.9937\n",
            "99.37 %\n",
            "Processing results for user 016... \n",
            "loading data for user 001 on the XY plane ... √\n",
            "loading data for user 002 on the XY plane ... √\n",
            "loading data for user 003 on the XY plane ... √\n",
            "loading data for user 004 on the XY plane ... √\n",
            "loading data for user 005 on the XY plane ... √\n",
            "loading data for user 006 on the XY plane ... √\n",
            "loading data for user 007 on the XY plane ... √\n",
            "loading data for user 008 on the XY plane ... √\n",
            "loading data for user 009 on the XY plane ... √\n",
            "loading data for user 010 on the XY plane ... √\n",
            "loading data for user 011 on the XY plane ... √\n",
            "loading data for user 012 on the XY plane ... √\n",
            "loading data for user 013 on the XY plane ... √\n",
            "loading data for user 014 on the XY plane ... √\n",
            "loading data for user 015 on the XY plane ... √\n",
            "loading data for user 016 on the XY plane ... √\n",
            "loading data for user 017 on the XY plane ... √\n",
            "loading data for user 018 on the XY plane ... √\n",
            "loading data for user 019 on the XY plane ... √\n",
            "loading data for user 020 on the XY plane ... √\n",
            "loading data for user 021 on the XY plane ... √\n",
            "loading data for user 022 on the XY plane ... √\n",
            "loading data for user 023 on the XY plane ... √\n",
            "loading data for user 024 on the XY plane ... √\n",
            "loading data for user 025 on the XY plane ... √\n",
            "loading data for user 001 on the YZ plane ... √\n",
            "loading data for user 002 on the YZ plane ... √\n",
            "loading data for user 003 on the YZ plane ... √\n",
            "loading data for user 004 on the YZ plane ... √\n",
            "loading data for user 005 on the YZ plane ... √\n",
            "loading data for user 006 on the YZ plane ... √\n",
            "loading data for user 007 on the YZ plane ... √\n",
            "loading data for user 008 on the YZ plane ... √\n",
            "loading data for user 009 on the YZ plane ... √\n",
            "loading data for user 010 on the YZ plane ... √\n",
            "loading data for user 011 on the YZ plane ... √\n",
            "loading data for user 012 on the YZ plane ... √\n",
            "loading data for user 013 on the YZ plane ... √\n",
            "loading data for user 014 on the YZ plane ... √\n",
            "loading data for user 015 on the YZ plane ... √\n",
            "loading data for user 016 on the YZ plane ... √\n",
            "loading data for user 017 on the YZ plane ... √\n",
            "loading data for user 018 on the YZ plane ... √\n",
            "loading data for user 019 on the YZ plane ... √\n",
            "loading data for user 020 on the YZ plane ... √\n",
            "loading data for user 021 on the YZ plane ... √\n",
            "loading data for user 022 on the YZ plane ... √\n",
            "loading data for user 023 on the YZ plane ... √\n",
            "loading data for user 024 on the YZ plane ... √\n",
            "loading data for user 025 on the YZ plane ... √\n",
            "loading data for user 001 on the ZX plane ... √\n",
            "loading data for user 002 on the ZX plane ... √\n",
            "loading data for user 003 on the ZX plane ... √\n",
            "loading data for user 004 on the ZX plane ... √\n",
            "loading data for user 005 on the ZX plane ... √\n",
            "loading data for user 006 on the ZX plane ... √\n",
            "loading data for user 007 on the ZX plane ... √\n",
            "loading data for user 008 on the ZX plane ... √\n",
            "loading data for user 009 on the ZX plane ... √\n",
            "loading data for user 010 on the ZX plane ... √\n",
            "loading data for user 011 on the ZX plane ... √\n",
            "loading data for user 012 on the ZX plane ... √\n",
            "loading data for user 013 on the ZX plane ... √\n",
            "loading data for user 014 on the ZX plane ... √\n",
            "loading data for user 015 on the ZX plane ... √\n",
            "loading data for user 016 on the ZX plane ... √\n",
            "loading data for user 017 on the ZX plane ... √\n",
            "loading data for user 018 on the ZX plane ... √\n",
            "loading data for user 019 on the ZX plane ... √\n",
            "loading data for user 020 on the ZX plane ... √\n",
            "loading data for user 021 on the ZX plane ... √\n",
            "loading data for user 022 on the ZX plane ... √\n",
            "loading data for user 023 on the ZX plane ... √\n",
            "loading data for user 024 on the ZX plane ... √\n",
            "loading data for user 025 on the ZX plane ... √\n",
            "8\n",
            "Epoch 1/300\n",
            "60/60 [==============================] - 43s 530ms/step - loss: 2.6578 - accuracy: 0.1932 - val_loss: 1.5855 - val_accuracy: 0.7250\n",
            "Epoch 2/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 1.6776 - accuracy: 0.4698 - val_loss: 0.7324 - val_accuracy: 0.9438\n",
            "Epoch 3/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 1.0794 - accuracy: 0.6643 - val_loss: 0.2640 - val_accuracy: 0.9688\n",
            "Epoch 4/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.6601 - accuracy: 0.7953 - val_loss: 0.1684 - val_accuracy: 0.9688\n",
            "Epoch 5/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.5037 - accuracy: 0.8365 - val_loss: 0.1115 - val_accuracy: 0.9812\n",
            "Epoch 6/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.3755 - accuracy: 0.8831 - val_loss: 0.0975 - val_accuracy: 0.9812\n",
            "Epoch 7/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.3128 - accuracy: 0.9023 - val_loss: 0.0863 - val_accuracy: 0.9812\n",
            "Epoch 8/300\n",
            "60/60 [==============================] - 29s 475ms/step - loss: 0.2733 - accuracy: 0.9094 - val_loss: 0.0880 - val_accuracy: 0.9750\n",
            "Epoch 9/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.2308 - accuracy: 0.9266 - val_loss: 0.0751 - val_accuracy: 0.9812\n",
            "Epoch 10/300\n",
            "60/60 [==============================] - 29s 486ms/step - loss: 0.2129 - accuracy: 0.9318 - val_loss: 0.0627 - val_accuracy: 0.9875\n",
            "Epoch 11/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.1882 - accuracy: 0.9372 - val_loss: 0.0576 - val_accuracy: 0.9875\n",
            "Epoch 12/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.1766 - accuracy: 0.9443 - val_loss: 0.0453 - val_accuracy: 0.9875\n",
            "Epoch 13/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.1560 - accuracy: 0.9523 - val_loss: 0.0469 - val_accuracy: 0.9875\n",
            "Epoch 14/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.1268 - accuracy: 0.9622 - val_loss: 0.0476 - val_accuracy: 0.9875\n",
            "Epoch 15/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.1260 - accuracy: 0.9581 - val_loss: 0.0376 - val_accuracy: 0.9875\n",
            "Epoch 16/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.1157 - accuracy: 0.9659 - val_loss: 0.0298 - val_accuracy: 0.9875\n",
            "Epoch 17/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.1059 - accuracy: 0.9648 - val_loss: 0.0312 - val_accuracy: 0.9875\n",
            "Epoch 18/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.1100 - accuracy: 0.9641 - val_loss: 0.0301 - val_accuracy: 0.9875\n",
            "Epoch 19/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.1003 - accuracy: 0.9674 - val_loss: 0.0368 - val_accuracy: 0.9875\n",
            "Epoch 20/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0949 - accuracy: 0.9693 - val_loss: 0.0258 - val_accuracy: 0.9937\n",
            "Epoch 21/300\n",
            "60/60 [==============================] - 28s 472ms/step - loss: 0.0864 - accuracy: 0.9719 - val_loss: 0.0263 - val_accuracy: 0.9937\n",
            "Epoch 22/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0762 - accuracy: 0.9776 - val_loss: 0.0325 - val_accuracy: 0.9875\n",
            "Epoch 23/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0772 - accuracy: 0.9763 - val_loss: 0.0224 - val_accuracy: 0.9875\n",
            "Epoch 24/300\n",
            "60/60 [==============================] - 29s 479ms/step - loss: 0.0720 - accuracy: 0.9771 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
            "Epoch 25/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0730 - accuracy: 0.9771 - val_loss: 0.0196 - val_accuracy: 0.9937\n",
            "Epoch 26/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0669 - accuracy: 0.9792 - val_loss: 0.0217 - val_accuracy: 0.9937\n",
            "Epoch 27/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0568 - accuracy: 0.9836 - val_loss: 0.0248 - val_accuracy: 0.9937\n",
            "Epoch 28/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0603 - accuracy: 0.9779 - val_loss: 0.0215 - val_accuracy: 0.9937\n",
            "Epoch 29/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0619 - accuracy: 0.9789 - val_loss: 0.0227 - val_accuracy: 0.9875\n",
            "Epoch 30/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0628 - accuracy: 0.9779 - val_loss: 0.0197 - val_accuracy: 0.9937\n",
            "Epoch 31/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0579 - accuracy: 0.9810 - val_loss: 0.0228 - val_accuracy: 0.9937\n",
            "Epoch 32/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0567 - accuracy: 0.9836 - val_loss: 0.0199 - val_accuracy: 0.9937\n",
            "Epoch 33/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.0514 - accuracy: 0.9854 - val_loss: 0.0195 - val_accuracy: 0.9937\n",
            "Epoch 34/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0452 - accuracy: 0.9875 - val_loss: 0.0162 - val_accuracy: 0.9937\n",
            "Epoch 35/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0469 - accuracy: 0.9852 - val_loss: 0.0149 - val_accuracy: 0.9937\n",
            "Epoch 36/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0447 - accuracy: 0.9872 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
            "Epoch 37/300\n",
            "60/60 [==============================] - 29s 478ms/step - loss: 0.0459 - accuracy: 0.9841 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
            "Epoch 38/300\n",
            "60/60 [==============================] - 29s 475ms/step - loss: 0.0399 - accuracy: 0.9880 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 39/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0351 - accuracy: 0.9906 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 40/300\n",
            "60/60 [==============================] - 29s 477ms/step - loss: 0.0505 - accuracy: 0.9839 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
            "Epoch 41/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0364 - accuracy: 0.9896 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
            "Epoch 42/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0431 - accuracy: 0.9867 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
            "Epoch 43/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0359 - accuracy: 0.9911 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
            "Epoch 44/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0382 - accuracy: 0.9862 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "Epoch 45/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0356 - accuracy: 0.9901 - val_loss: 0.0162 - val_accuracy: 0.9937\n",
            "Epoch 46/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0370 - accuracy: 0.9880 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "Epoch 47/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0316 - accuracy: 0.9901 - val_loss: 0.0078 - val_accuracy: 1.0000\n",
            "Epoch 48/300\n",
            "60/60 [==============================] - 29s 476ms/step - loss: 0.0332 - accuracy: 0.9893 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "Epoch 49/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.0301 - accuracy: 0.9922 - val_loss: 0.0130 - val_accuracy: 0.9937\n",
            "Epoch 50/300\n",
            "60/60 [==============================] - 28s 471ms/step - loss: 0.0312 - accuracy: 0.9909 - val_loss: 0.0127 - val_accuracy: 0.9937\n",
            "Epoch 51/300\n",
            "60/60 [==============================] - 28s 475ms/step - loss: 0.0285 - accuracy: 0.9909 - val_loss: 0.0155 - val_accuracy: 0.9937\n",
            "Epoch 52/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0292 - accuracy: 0.9906 - val_loss: 0.0147 - val_accuracy: 0.9937\n",
            "Epoch 53/300\n",
            "60/60 [==============================] - 28s 473ms/step - loss: 0.0273 - accuracy: 0.9911 - val_loss: 0.0210 - val_accuracy: 0.9937\n",
            "Epoch 54/300\n",
            "60/60 [==============================] - 28s 474ms/step - loss: 0.0259 - accuracy: 0.9930 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
            "3/3 [==============================] - 1s 341ms/step - loss: 0.0167 - accuracy: 1.0000\n",
            "100.00 %\n",
            "------------------------------------\n",
            "Average accuracy 99.14 +/- 0.93\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ACC = []\n",
        "CM = []\n",
        "HISTORY = []\n",
        "logs = ''\n",
        "\n",
        "# ... Change user limit\n",
        "users = USERS[8:16]\n",
        "\n",
        "for test_user in users:\n",
        "    print('Processing results for user ' + test_user, end='... \\n')\n",
        "    \n",
        "    X_train = []\n",
        "    X_test = []\n",
        "    y_train = []\n",
        "    y_test = []\n",
        "    \n",
        "    first_time_train = True\n",
        "    first_time_test = True\n",
        "\n",
        "    for user in USERS:\n",
        "        x_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_X.joblib')\n",
        "        y_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_y.joblib')\n",
        "        X = joblib.load(x_path)\n",
        "        y = joblib.load(y_path)\n",
        "\n",
        "        if user == test_user:\n",
        "            if first_time_train == True:\n",
        "                first_time_train = False\n",
        "                X_test = X\n",
        "                y_test = y\n",
        "                \n",
        "            else:\n",
        "                X_test = np.append(X_test, X, axis=0)\n",
        "                y_test = np.append(y_test, y, axis=0)\n",
        "                \n",
        "        else:\n",
        "            if first_time_test == True:\n",
        "                first_time_test = False\n",
        "                X_train = X\n",
        "                y_train = y\n",
        "                \n",
        "            else:\n",
        "                X_train = np.append(X_train, X, axis=0)\n",
        "                y_train = np.append(y_train, y, axis=0)\n",
        "\n",
        "\n",
        "    # X_train, y_train = shuffle(X_train, y_train)\n",
        "\n",
        "    X_train_xy, X_test_xy, y_train_xy, y_test_xy = load_data('XY', test_user)\n",
        "    X_train_yz, X_test_yz, y_train_yz, y_test_yz = load_data('YZ', test_user)\n",
        "    X_train_zx, X_test_zx, y_train_zx, y_test_zx = load_data('ZX', test_user)\n",
        "\n",
        "    X_train_xy, X_train_yz, X_train_zx, X_train, y_train = shuffle(\n",
        "        X_train_xy, X_train_yz, X_train_zx, X_train, y_train\n",
        "    )\n",
        "\n",
        "    X_train_combined = np.split(X_train, 8, axis=-1)[:5] + [X_train_xy, X_train_yz, X_train_zx]\n",
        "    X_test_combined = np.split(X_test, 8, axis=-1)[:5] + [X_test_xy, X_test_yz, X_test_zx]\n",
        "\n",
        "    del X_train_xy, X_test_xy, y_train_xy, y_test_xy\n",
        "    del X_train_yz, X_test_yz, y_train_yz, y_test_yz\n",
        "    del X_train_zx, X_test_zx, y_train_zx, y_test_zx\n",
        "    del X_train, X_test\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    print(len(X_train_combined))\n",
        "\n",
        "    callbacks = [\n",
        "                 tf.keras.callbacks.EarlyStopping(\n",
        "                    monitor='val_accuracy', \n",
        "                    patience=30, \n",
        "                    mode='max', \n",
        "                    restore_best_weights=True\n",
        "                ),\n",
        "    ]\n",
        "\n",
        "    model = get_stacked_model()\n",
        "    history = model.fit(\n",
        "        X_train_combined, \n",
        "        y_train, \n",
        "        validation_data=(X_test_combined, y_test),\n",
        "        epochs=300, \n",
        "        batch_size=64,\n",
        "        callbacks=[callbacks]\n",
        "    )\n",
        "    _, accuracy = model.evaluate(X_test_combined, y_test, batch_size=64)\n",
        "    y_pred = model.predict(X_test_combined)\n",
        "    cm = confusion_matrix(y_test.ravel(), np.argmax(y_pred, axis=-1))\n",
        "\n",
        "    CM.append(cm)\n",
        "    HISTORY.append(history)\n",
        "\n",
        "    accuracy = accuracy * 100\n",
        "    print(f'%.2f %%' %(accuracy))\n",
        "    logs = logs + 'Accuracy for user ' + str(test_user) + '... ' + str(accuracy) + '\\n'\n",
        "    ACC.append(accuracy)\n",
        "\n",
        "    del model, history\n",
        "    gc.collect()\n",
        "    \n",
        "AVG_ACC = np.mean(ACC)\n",
        "STD = np.std(ACC)\n",
        "print('------------------------------------')\n",
        "print(f'Average accuracy %.2f +/- %.2f' %(AVG_ACC, STD))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "data = {\n",
        "    \"cm\": CM,\n",
        "    \"accuracy\": ACC,\n",
        "    \"logs\": logs\n",
        "}\n",
        "\n",
        "joblib.dump(data, \"user8-16.joblib\")"
      ],
      "metadata": {
        "id": "HeOI8hw3zCXz",
        "outputId": "a24a6702-3779-49bf-91f6-a7fdaac603b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HeOI8hw3zCXz",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['user8-16.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    pass"
      ],
      "metadata": {
        "id": "3N94GWOk9KHE",
        "outputId": "d29ab1a4-7079-455e-87ff-fab386e82276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "id": "3N94GWOk9KHE",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b7133701d76c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92fd4333",
      "metadata": {
        "id": "92fd4333"
      },
      "outputs": [],
      "source": [
        "# model = get_stacked_model()\n",
        "# X_train_xy, X_train_yz, X_train_zx, y_train_xy = shuffle(\n",
        "#     X_train_xy, X_train_yz, X_train_zx, y_train_xy\n",
        "# )\n",
        "\n",
        "# history = model.fit(\n",
        "#     [X_train_xy, X_train_yz, X_train_zx],\n",
        "#     y_train_xy,\n",
        "#     validation_data=([X_test_xy, X_test_yz, X_test_zx], y_test_xy),\n",
        "#     batch_size=32,\n",
        "#     epochs=10,\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "distinct-appendix",
      "metadata": {
        "id": "distinct-appendix",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# model_xy = get_model()\n",
        "# X_train_xy, y_train_xy = shuffle(X_train_xy, y_train_xy)\n",
        "# history_xy = model_xy.fit(X_train_xy, y_train_xy, validation_data=(X_test_xy, y_test_xy), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "constitutional-genre",
      "metadata": {
        "id": "constitutional-genre"
      },
      "outputs": [],
      "source": [
        "# # prob_xy = tf.keras.Sequential([model_xy, tf.keras.layers.Softmax()])\n",
        "# # y_pred_xy = prob_xy.predict(X_test_xy)\n",
        "# y_pred_xy = model_xy.predict(X_test_xy)\n",
        "# y_pred = np.argmax(y_pred_xy, axis=1)\n",
        "# print(classification_report(y_test_xy.ravel(), y_pred, zero_division=0))\n",
        "# prc_xy = precision_score(y_test_xy.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "emotional-chrome",
      "metadata": {
        "id": "emotional-chrome"
      },
      "outputs": [],
      "source": [
        "# model_yz = get_model()\n",
        "# X_train_yz, y_train_yz = shuffle(X_train_yz, y_train_yz)\n",
        "# history_yz = model_yz.fit(X_train_yz, y_train_yz, validation_data=(X_test_yz, y_test_yz), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "boring-insurance",
      "metadata": {
        "id": "boring-insurance"
      },
      "outputs": [],
      "source": [
        "# # prob_yz = tf.keras.Sequential([model_yz, tf.keras.layers.Softmax()])\n",
        "# # y_pred_yz = prob_yz.predict(X_test_yz)\n",
        "# y_pred_yz = model_yz.predict(X_test_yz)\n",
        "# y_pred = np.argmax(y_pred_yz, axis=1)\n",
        "# print(classification_report(y_test_yz.ravel(), y_pred, zero_division=0))\n",
        "# prc_yz = precision_score(y_test_yz.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "joint-evaluation",
      "metadata": {
        "id": "joint-evaluation"
      },
      "outputs": [],
      "source": [
        "# model_zx = get_model()\n",
        "# X_train_zx, y_train_zx = shuffle(X_train_zx, y_train_zx)\n",
        "# history_zx = model_zx.fit(X_train_zx, y_train_zx, validation_data=(X_test_zx, y_test_zx), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "designed-still",
      "metadata": {
        "id": "designed-still"
      },
      "outputs": [],
      "source": [
        "# # prob_zx = tf.keras.Sequential([model_zx, tf.keras.layers.Softmax()])\n",
        "# # y_pred_zx = prob_zx.predict(X_test_zx)\n",
        "# y_pred_zx = model_zx.predict(X_test_zx)\n",
        "# y_pred = np.argmax(y_pred_zx, axis=1)\n",
        "# print(classification_report(y_test_zx.ravel(), y_pred, zero_division=0))\n",
        "# prc_zx = precision_score(y_test_zx.ravel(), y_pred, zero_division=0, average=None)\n",
        "# tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "selective-geography",
      "metadata": {
        "id": "selective-geography"
      },
      "outputs": [],
      "source": [
        "# y_total = y_pred_xy + y_pred_yz + y_pred_zx\n",
        "# y_pred = np.argmax(y_total, axis=1)\n",
        "# report = classification_report(y_test_xy.ravel(), y_pred, zero_division=0)\n",
        "# print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "collected-palmer",
      "metadata": {
        "id": "collected-palmer"
      },
      "outputs": [],
      "source": [
        "# config = '\\n\\nTEST_USER ' + TEST_USER + ' T: ' + str(int(time.time())) + '\\n'\n",
        "# underline = '=====================================\\n'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'logs_sptl_bw' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(underline)\n",
        "# f.write(report)\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intellectual-lunch",
      "metadata": {
        "id": "intellectual-lunch"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_xy' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_xy, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trying-thread",
      "metadata": {
        "id": "trying-thread"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_yz' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_yz, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "general-plant",
      "metadata": {
        "id": "general-plant"
      },
      "outputs": [],
      "source": [
        "# config = TEST_USER + ' :'\n",
        "# log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.mkdir(log_dir)\n",
        "# f = open(os.path.join(log_dir, 'prc_sptl_bw_zx' + CONFIG + '.txt'), 'a')\n",
        "# f.write(config)\n",
        "# f.write(np.array2string(prc_zx, precision=2, max_line_width=100) + '\\n')\n",
        "# f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Spatial_Path_Transfer_Learning_BW.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}