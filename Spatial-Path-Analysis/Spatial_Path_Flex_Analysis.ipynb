{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "planned-invitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electoral-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID   = '1cAJdvAZDolurN3KCZcYz_YJSMV-aIzWT'\n",
    "\n",
    "# -------- TEST USER ----------- #\n",
    "\n",
    "TEST_USER      = '001'\n",
    "\n",
    "# -------------BASE DIR (MODIFY THIS TO YOUR NEED) ------------ #\n",
    "BASE_DIR     = '../'\n",
    "\n",
    "DATA_DIR     = 'Sensor-Data/'\n",
    "\n",
    "USERS        = ['001', '002', '003', '004', '005', '006', '007']\n",
    "# ------------------------------- Only Dynalic Gestures ------------------------------ #\n",
    "GESTURES     = ['deaf', 'z', 'bad', 'deaf', 'fine', 'good', 'goodbye', 'hello', 'hungry',\n",
    "                'me', 'no', 'please', 'sorry', 'thankyou', 'yes', 'you']\n",
    "\n",
    "BATCH_SIZE     = 32\n",
    "\n",
    "# ------------- FOR THE GREATER GOOD :) ------------- #\n",
    "TRAIN_LEN      = 960\n",
    "TEST_LEN       = 160\n",
    "\n",
    "EPOCHS         = 25\n",
    "LEARNING_RATE  = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hungarian-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------- Download util for Google Drive ------------------- #\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "        \n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "        \n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(fid, destination):\n",
    "    print('cleaning already existing files ... ', end='')\n",
    "    try:\n",
    "        shutil.rmtree(destination)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "        \n",
    "    print('creating data directory ... ', end='')\n",
    "    os.mkdir(destination)\n",
    "    print('√')\n",
    "    \n",
    "    print('downloading dataset from the repository ... ', end='')\n",
    "    filename = os.path.join(destination, 'dataset.tar.xz')\n",
    "    try:\n",
    "        download_file_from_google_drive(fid, filename)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "        \n",
    "    print('extracting the dataset ... ', end='')\n",
    "    try:\n",
    "        tar = tarfile.open(filename)\n",
    "        tar.extractall(destination)\n",
    "        tar.close()\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comprehensive-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Comment This if already downloaded -------- #\n",
    "\n",
    "# destination = os.path.join(BASE_DIR, DATA_DIR)\n",
    "# download_data(DATASET_ID, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unlike-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X_train = np.zeros((TRAIN_LEN, 9, 1))\n",
    "    X_test = np.zeros((TEST_LEN, 9, 1))\n",
    "    y_train = np.zeros((TRAIN_LEN, 1))\n",
    "    y_test = np.zeros((TEST_LEN, 1))\n",
    "    \n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "        \n",
    "        \n",
    "    for user in USERS:\n",
    "        print('loading data for user ' + user + ' ... ', end='')\n",
    "        path = os.path.join(BASE_DIR, DATA_DIR, user)\n",
    "        for gesture in GESTURES:\n",
    "            filename = gesture + '.csv'\n",
    "            file_path = os.path.join(path, filename)\n",
    "            f1 = np.median(pd.read_csv(file_path)['flex_1'].to_numpy().reshape(-1, 150), axis=1, keepdims=True)\n",
    "            f2 = np.median(pd.read_csv(file_path)['flex_2'].to_numpy().reshape(-1, 150), axis=1, keepdims=True)\n",
    "            f3 = np.median(pd.read_csv(file_path)['flex_3'].to_numpy().reshape(-1, 150), axis=1, keepdims=True)\n",
    "            f4 = np.median(pd.read_csv(file_path)['flex_4'].to_numpy().reshape(-1, 150), axis=1, keepdims=True)\n",
    "            f5 = np.median(pd.read_csv(file_path)['flex_5'].to_numpy().reshape(-1, 150), axis=1, keepdims=True)\n",
    "            qw = np.median(pd.read_csv(file_path)['Qw'].to_numpy().reshape(-1, 150), axis=1, keepdims=True)\n",
    "            qx = np.median(pd.read_csv(file_path)['Qx'].to_numpy().reshape(-1, 150), axis=1, keepdims=True)\n",
    "            qy = np.median(pd.read_csv(file_path)['Qy'].to_numpy().reshape(-1, 150), axis=1, keepdims=True)\n",
    "            qz = np.median(pd.read_csv(file_path)['Qz'].to_numpy().reshape(-1, 150), axis=1, keepdims=True)\n",
    "            flex_data = np.concatenate([f1, f2, f3, f4, f5, qw, qx, qy, qz], axis=1)\n",
    "            \n",
    "            if user != TEST_USER:\n",
    "                X_train[train_count: train_count + 10, :] = np.expand_dims(flex_data, axis=-1)\n",
    "                y_train[train_count: train_count + 10, 0] = GESTURES.index(gesture)\n",
    "                train_count = train_count + 10\n",
    "                \n",
    "            else:\n",
    "                X_test[test_count: test_count + 10, :] = np.expand_dims(flex_data, axis=-1)\n",
    "                y_test[test_count: test_count + 10, 0] = GESTURES.index(gesture)\n",
    "                test_count = test_count + 10\n",
    "                \n",
    "        print('√')\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "democratic-control",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for user 001 ... √\n",
      "loading data for user 002 ... √\n",
      "loading data for user 003 ... √\n",
      "loading data for user 004 ... √\n",
      "loading data for user 005 ... √\n",
      "loading data for user 006 ... √\n",
      "loading data for user 007 ... √\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sustained-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.experimental.preprocessing.Normalization(input_shape=(9, 1))\n",
    "normalizer.adapt(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "humanitarian-stereo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "normalization (Normalization (None, 9, 1)              3         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                320       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 5,043\n",
      "Trainable params: 5,040\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Flatten(input_shape=(8, 1)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(16)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dirty-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "excessive-reflection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.8067 - accuracy: 0.0924\n",
      "Epoch 2/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.6233 - accuracy: 0.1017\n",
      "Epoch 3/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.4849 - accuracy: 0.1651\n",
      "Epoch 4/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 2.2377 - accuracy: 0.2470\n",
      "Epoch 5/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.9196 - accuracy: 0.3144\n",
      "Epoch 6/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.7495 - accuracy: 0.3620\n",
      "Epoch 7/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.5723 - accuracy: 0.4143\n",
      "Epoch 8/25\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.5129 - accuracy: 0.4065\n",
      "Epoch 9/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.4334 - accuracy: 0.4485\n",
      "Epoch 10/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.3897 - accuracy: 0.4495\n",
      "Epoch 11/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.3387 - accuracy: 0.4571\n",
      "Epoch 12/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.3804 - accuracy: 0.4655\n",
      "Epoch 13/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.3252 - accuracy: 0.4603\n",
      "Epoch 14/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.2647 - accuracy: 0.4723\n",
      "Epoch 15/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.2822 - accuracy: 0.5000\n",
      "Epoch 16/25\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.2428 - accuracy: 0.4795\n",
      "Epoch 17/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.2371 - accuracy: 0.4897\n",
      "Epoch 18/25\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.2237 - accuracy: 0.4953\n",
      "Epoch 19/25\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.2163 - accuracy: 0.5183\n",
      "Epoch 20/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.1449 - accuracy: 0.5238\n",
      "Epoch 21/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.1878 - accuracy: 0.4993\n",
      "Epoch 22/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.1300 - accuracy: 0.5094\n",
      "Epoch 23/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.1390 - accuracy: 0.5229\n",
      "Epoch 24/25\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 1.0722 - accuracy: 0.5528\n",
      "Epoch 25/25\n",
      "30/30 [==============================] - 0s 2ms/step - loss: 1.0830 - accuracy: 0.5464\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "figured-sweet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      1.00      0.57        20\n",
      "         1.0       0.00      0.00      0.00        10\n",
      "         2.0       0.15      0.30      0.20        10\n",
      "         4.0       0.00      0.00      0.00        10\n",
      "         5.0       0.00      0.00      0.00        10\n",
      "         6.0       0.03      0.10      0.05        10\n",
      "         7.0       1.00      1.00      1.00        10\n",
      "         8.0       0.00      0.00      0.00        10\n",
      "         9.0       0.00      0.00      0.00        10\n",
      "        10.0       1.00      1.00      1.00        10\n",
      "        11.0       0.00      0.00      0.00        10\n",
      "        12.0       0.91      1.00      0.95        10\n",
      "        13.0       0.00      0.00      0.00        10\n",
      "        14.0       0.53      1.00      0.69        10\n",
      "        15.0       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.40       160\n",
      "   macro avg       0.27      0.36      0.30       160\n",
      "weighted avg       0.28      0.40      0.31       160\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andromeda/Ai/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andromeda/Ai/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/andromeda/Ai/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "prob = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "y_pred = prob.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test.ravel(), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-colon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
