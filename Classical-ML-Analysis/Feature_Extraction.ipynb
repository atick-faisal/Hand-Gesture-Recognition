{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conditional-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import shutil\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import librosa as lb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "documented-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "certain-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID      = '1cAJdvAZDolurN3KCZcYz_YJSMV-aIzWT'\n",
    "\n",
    "# -------------BASE DIR (MODIFY THIS TO YOUR NEED) ------------ #\n",
    "BASE_DIR        = '../'\n",
    "# BASE_DIR     = '/content/drive/MyDrive/Research/Hand Gesture/GitHub/'\n",
    "\n",
    "DATA_DIR        = 'Sensor-Data/'\n",
    "FEATURES_DIR    = 'Features/'\n",
    "ALL_USERS_DIR   = 'All/'\n",
    "IND_USER_DIR    = 'Individuals/'\n",
    "\n",
    "\n",
    "USERS           = ['001', '002', '003', '004', '005', '006', '007']\n",
    "GESTURES        = [ 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \n",
    "                    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "                    'bad', 'deaf', 'fine', 'good', 'goodbye', 'hello', 'hungry',\n",
    "                    'me', 'no', 'please', 'sorry', 'thankyou', 'yes', 'you' ]\n",
    "\n",
    "WINDOW_LEN      = 150\n",
    "\n",
    "#--------- Trim -----------\n",
    "TRIM_AMOUNT     = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subjective-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------- Download util for Google Drive ------------------- #\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "        \n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "        \n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(fid, destination):\n",
    "    print('cleaning already existing files ... ', end='')\n",
    "    try:\n",
    "        shutil.rmtree(destination)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "        \n",
    "    print('creating data directory ... ', end='')\n",
    "    os.mkdir(destination)\n",
    "    print('√')\n",
    "    \n",
    "    print('downloading dataset from the repository ... ', end='')\n",
    "    filename = os.path.join(destination, 'dataset.tar.xz')\n",
    "    try:\n",
    "        download_file_from_google_drive(fid, filename)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "        \n",
    "    print('extracting the dataset ... ', end='')\n",
    "    try:\n",
    "        tar = tarfile.open(filename)\n",
    "        tar.extractall(destination)\n",
    "        tar.close()\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "senior-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Comment This if already downloaded -------- #\n",
    "\n",
    "# destination = os.path.join(BASE_DIR, DATA_DIR)\n",
    "# download_data(DATASET_ID, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "electric-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(x):\n",
    "    cor = []\n",
    "    for n in range(x.shape[0]):\n",
    "        cor.append(np.correlate(x[n, :], x[n, :])[0])\n",
    "    return np.array(cor)\n",
    "\n",
    "\n",
    "def mean_crossing_rate(x):\n",
    "    mcr = []\n",
    "    for n in range(x.shape[0]):\n",
    "        mcr.append(lb.feature.zero_crossing_rate(x[n, :] - np.mean(x[n, :]))[0, 0])\n",
    "    return np.array(mcr)\n",
    "\n",
    "\n",
    "def get_entropy(x, axis = 1):\n",
    "    x = x / np.sum(x, axis = axis, keepdims=True)\n",
    "    entropy = np.sum(sp.special.entr(x), axis = axis)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def number_of_peaks(x):\n",
    "    npk = []\n",
    "    for n in range(x.shape[0]):\n",
    "        thres = (np.max(x[n, :]) / 3)\n",
    "        peaks, _ = sp.signal.find_peaks(x[n, :], thres)\n",
    "        npk.append(len(peaks))\n",
    "    return np.array(npk, dtype=float)\n",
    "\n",
    "\n",
    "def get_stat_features(x, axis=1, prefix=''):\n",
    "    min = np.min(x, axis = axis)\n",
    "    max = np.max(x, axis = axis)\n",
    "    std = np.std(x, axis = axis)\n",
    "    avg = np.mean(x, axis = axis)\n",
    "    var = np.var(x, axis = axis)\n",
    "    ptp = np.ptp(x, axis = axis)\n",
    "    mrc = np.max(np.diff(x, axis = axis), axis = axis)\n",
    "    arc = np.mean(np.diff(x, axis = axis), axis = axis)\n",
    "    src = np.std(np.diff(x, axis = axis), axis = axis)\n",
    "    mad = sp.stats.median_abs_deviation(x, axis = axis)\n",
    "    iqr = sp.stats.iqr(x, axis = axis)\n",
    "    cor = correlation(x)\n",
    "    mcr = mean_crossing_rate(x)\n",
    "    rms = np.sum(np.square(x), axis = axis)\n",
    "\n",
    "    feature_names = ['min', 'max', 'std', 'avg', 'var', \n",
    "                   'ptp', 'mrc', 'arc', 'src', 'mad', \n",
    "                   'iqr', 'cor', 'mcr', 'rms']\n",
    "    columnName = [prefix + '_' + sub for sub in feature_names]\n",
    "\n",
    "    stat_features = pd.DataFrame(np.stack((min, max, std, avg, \n",
    "                                         var, ptp, mrc, arc, \n",
    "                                         src, mad, iqr, cor, \n",
    "                                         mcr, rms), axis=1), columns=columnName)\n",
    "\n",
    "    return stat_features\n",
    " \n",
    "\n",
    "def get_freq_features(x, axis=1, fs=100, nperseg=150, prefix=''):\n",
    "    freq, psd = sp.signal.welch(x, fs, nperseg = nperseg, axis = axis)\n",
    "    mpw = np.max(psd, axis = axis)\n",
    "    ent = get_entropy(psd, axis = axis)\n",
    "    ctf = np.divide(np.sum((freq * psd), axis = axis), np.sum(psd, axis = axis))\n",
    "    mxf = np.argmax(psd, axis = axis)\n",
    "    enr = np.sum(np.square(psd), axis = axis) / nperseg\n",
    "    skw = sp.stats.skew(x, axis = axis)\n",
    "    kut = sp.stats.kurtosis(x, axis = axis)\n",
    "    npk = number_of_peaks(psd)\n",
    "\n",
    "    feature_names = ['mpw', 'ent', 'ctf', 'mxf', 'enr', 'skw', 'kut', 'npk']\n",
    "    columnName = [prefix + '_' + sub for sub in feature_names]\n",
    "\n",
    "    freq_features = pd.DataFrame(np.stack((mpw, ent, ctf, mxf, enr, skw, \n",
    "                                         kut, npk), axis=1), columns=columnName)\n",
    "\n",
    "    return freq_features\n",
    "\n",
    "\n",
    "def get_mutual_features(x, y, z, axis=1, nperseg=150, prefix=''):\n",
    "    cxy = []\n",
    "    cxz = []\n",
    "    cyz = []\n",
    "    vxy = []\n",
    "    vxz = []\n",
    "    vyz = []\n",
    "    for n in range(x.shape[0]):\n",
    "        cxy.append(np.corrcoef(x[n, :].ravel(), y[n, :].ravel())[0, 1])\n",
    "        cxz.append(np.corrcoef(x[n, :].ravel(), z[n, :].ravel())[0, 1])\n",
    "        cyz.append(np.corrcoef(y[n, :].ravel(), z[n, :].ravel())[0, 1])\n",
    "        vxy.append(np.cov(x[n, :].ravel(), y[n, :].ravel())[0, 1])\n",
    "        vxz.append(np.cov(x[n, :].ravel(), z[n, :].ravel())[0, 1])\n",
    "        vyz.append(np.cov(y[n, :].ravel(), z[n, :].ravel())[0, 1])\n",
    "    cxy = np.array(cxy)\n",
    "    cxz = np.array(cxz)\n",
    "    cyz = np.array(cyz)\n",
    "    vxy = np.array(vxy)\n",
    "    vxz = np.array(vxz)\n",
    "    vyz = np.array(vyz)\n",
    "    sma = (np.trapz(x, axis = axis) + np.trapz(x, axis = axis) + np.trapz(x, axis = axis)) / nperseg\n",
    "\n",
    "    feature_names = ['cxy', 'cxz', 'cyz', 'vxy', 'vxz', 'vyz', 'sma']\n",
    "    columnName = [prefix + '_' + sub for sub in feature_names]\n",
    "\n",
    "    mutual_features = pd.DataFrame(np.stack((cxy, cxz, cyz, vxy, vxz, vyz, sma), \n",
    "                                        axis=1), columns=columnName)\n",
    "\n",
    "    return mutual_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "local-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dir(path):\n",
    "    print('cleaning already existing files ... ', end='')\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "    \n",
    "    print('creating ' + path + ' directory ... ', end='')\n",
    "    os.mkdir(path)\n",
    "    print('√')\n",
    "\n",
    "def generate_features():\n",
    "    features_dir = os.path.join(BASE_DIR, FEATURES_DIR, IND_USER_DIR)\n",
    "    clean_dir(features_dir)\n",
    "        \n",
    "    for user in USERS:\n",
    "        print('Processing data for user ' + user, end=' ... ')\n",
    "        \n",
    "        X = pd.DataFrame()\n",
    "        \n",
    "        for gesture in GESTURES:\n",
    "              \n",
    "            user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
    "            gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
    "\n",
    "            dataset = pd.read_csv(gesture_dir)\n",
    "            \n",
    "            flx1 = dataset['flex_1'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            flx2 = dataset['flex_2'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            flx3 = dataset['flex_3'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            flx4 = dataset['flex_4'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            flx5 = dataset['flex_5'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            \n",
    "            accx = dataset['ACCx'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            accy = dataset['ACCy'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            accz = dataset['ACCz'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            \n",
    "            gyrx = dataset['GYRx'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            gyry = dataset['GYRy'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            gyrz = dataset['GYRz'].to_numpy().reshape(-1, WINDOW_LEN)[:, TRIM_AMOUNT:]\n",
    "            \n",
    "            accm = np.sqrt(accx ** 2 + accy ** 2 + accz ** 2)\n",
    "            gyrm = np.sqrt(gyrx ** 2 + gyry ** 2 + gyrz ** 2)\n",
    "            \n",
    "            stat_accx = get_stat_features(accx, prefix=\"accx\")\n",
    "            stat_accy = get_stat_features(accy, prefix=\"accy\")\n",
    "            stat_accz = get_stat_features(accz, prefix=\"accz\")\n",
    "            stat_accm = get_stat_features(accm, prefix=\"accm\")\n",
    "            stat_gyrx = get_stat_features(gyrx, prefix=\"gyrx\")\n",
    "            stat_gyry = get_stat_features(gyry, prefix=\"gyry\")\n",
    "            stat_gyrz = get_stat_features(gyrz, prefix=\"gyrz\")\n",
    "            stat_gyrm = get_stat_features(gyrm, prefix=\"gyrm\")\n",
    "            stat_flx1 = get_stat_features(flx1, prefix=\"flx1\")\n",
    "            stat_flx2 = get_stat_features(flx2, prefix=\"flx2\")\n",
    "            stat_flx3 = get_stat_features(flx3, prefix=\"flx3\")\n",
    "            stat_flx4 = get_stat_features(flx4, prefix=\"flx4\")\n",
    "            stat_flx5 = get_stat_features(flx5, prefix=\"flx5\")\n",
    "\n",
    "            freq_accx = get_freq_features(accx, prefix=\"accx\")\n",
    "            freq_accy = get_freq_features(accy, prefix=\"accy\")\n",
    "            freq_accz = get_freq_features(accz, prefix=\"accz\")\n",
    "            freq_accm = get_freq_features(accm, prefix=\"accm\")\n",
    "            freq_gyrx = get_freq_features(gyrx, prefix=\"gyrx\")\n",
    "            freq_gyry = get_freq_features(gyry, prefix=\"gyry\")\n",
    "            freq_gyrz = get_freq_features(gyrz, prefix=\"gyrz\")\n",
    "            freq_gyrm = get_freq_features(gyrm, prefix=\"gyrm\")\n",
    "            freq_flx1 = get_freq_features(flx1, prefix=\"flx1\")\n",
    "            freq_flx2 = get_freq_features(flx2, prefix=\"flx2\")\n",
    "            freq_flx3 = get_freq_features(flx3, prefix=\"flx3\")\n",
    "            freq_flx4 = get_freq_features(flx4, prefix=\"flx4\")\n",
    "            freq_flx5 = get_freq_features(flx5, prefix=\"flx5\")\n",
    "\n",
    "            mutual_acc = get_mutual_features(accx, accy, accz, prefix='acc')\n",
    "            mutual_gyr = get_mutual_features(gyrx, gyry, gyrz, prefix='gyr')\n",
    "            \n",
    "            g_idx = GESTURES.index(gesture)\n",
    "            y = pd.DataFrame(np.ones((accx.shape[0], 1)) * g_idx, columns=['y'])\n",
    "            \n",
    "#             X_gesture = pd.concat([ stat_accx, stat_accy, stat_accz, stat_accm, \n",
    "#                                     freq_accx, freq_accy, freq_accz, freq_accm, \n",
    "#                                     mutual_acc, y], axis=1  )\n",
    "\n",
    "            X_gesture = pd.concat([ stat_accx, stat_accy, stat_accz, stat_accm, \n",
    "                            stat_gyrx, stat_gyry, stat_gyrz, stat_gyrm,\n",
    "                            stat_flx1, stat_flx2, stat_flx3, stat_flx4, stat_flx5,\n",
    "                            freq_accx, freq_accy, freq_accz, freq_accm,\n",
    "                            freq_gyrx, freq_gyry, freq_gyrz, freq_gyrm,\n",
    "                            freq_flx1, freq_flx2, freq_flx3, freq_flx4, freq_flx5,\n",
    "                            mutual_acc, mutual_gyr, y], axis=1 )\n",
    "            \n",
    "            X = pd.concat([X, X_gesture], ignore_index=True)\n",
    "            \n",
    "        feature_path = os.path.join(features_dir, user + '.joblib')\n",
    "        joblib.dump(X, feature_path)\n",
    "            \n",
    "        print('√')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "complicated-assistant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning already existing files ... √\n",
      "creating ../Features/Individuals/ directory ... √\n",
      "Processing data for user 001 ... √\n",
      "Processing data for user 002 ... √\n",
      "Processing data for user 003 ... √\n",
      "Processing data for user 004 ... √\n",
      "Processing data for user 005 ... √\n",
      "Processing data for user 006 ... √\n",
      "Processing data for user 007 ... √\n"
     ]
    }
   ],
   "source": [
    "generate_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
