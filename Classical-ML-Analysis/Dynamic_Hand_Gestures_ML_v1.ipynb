{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/atick-faisal/Hand-Gesture-Recognition/blob/main/Classical-ML-Analysis/Dynamic_Hand_Gestures_ML_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0Hd-t-h5nXE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import shutil\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import librosa as lb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.special import entr\n",
    "from scipy.signal import butter, lfilter, find_peaks\n",
    "from scipy.stats import median_absolute_deviation, skew, kurtosis\n",
    "from scipy.stats import iqr as inter_quartile_range\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGSWepcd5suh"
   },
   "outputs": [],
   "source": [
    "\n",
    "DATASET_ID      = '1p0CSRb9gax0sKqdyzOYVt-BXvZ4GtrBv'\n",
    "\n",
    "# -------------BASE DIR (MODIFY THIS TO YOUR NEED) ------------ #\n",
    "# BASE_DIR        = '../'\n",
    "BASE_DIR     = '/content/drive/MyDrive/Research/Hand Gesture/GitHub/'\n",
    "\n",
    "DATA_DIR        = 'Sensor-Data/'\n",
    "FEATURES_DIR    = 'Features/'\n",
    "FIGURE_DIR      = 'Figures/'\n",
    "LOG_DIR         = 'Logs/'\n",
    "\n",
    "USERS           = ['001', '002', '003', '004', '005', '006', '007', '008', '009',\n",
    "                   '010', '011', '012', '013', '014', '015', '016', '017', '018',\n",
    "                   '019', '020', '021', '022', '023', '024', '025']\n",
    "GESTURES        = ['j', 'z', 'bad', 'deaf', 'fine', 'good', 'goodbye', 'hello', 'hungry',\n",
    "                   'me', 'no', 'please', 'sorry', 'thankyou', 'yes', 'you']\n",
    "\n",
    "WINDOW_LEN      = 150\n",
    "\n",
    "# ------------- FOR THE GREATER GOOD :) ------------- #\n",
    "DATASET_LEN     = 1120\n",
    "TRAIN_LEN       = 960\n",
    "TEST_LEN        = 160\n",
    "\n",
    "TEST_USER       = '001'\n",
    "EPOCHS          = 5\n",
    "\n",
    "FEATURE_GROUP   = 'DYNAMIC_ACC_ONLY_'\n",
    "CUT_OFF         = 3.0\n",
    "ORDER           = 4\n",
    "FS              = 100\n",
    "\n",
    "CONFIG          = FEATURE_GROUP + 'CUT_OFF_' + str(CUT_OFF) + '_ORDER_' + str(ORDER) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFpyBb3X5-it"
   },
   "outputs": [],
   "source": [
    "\n",
    "#--------------------- Download util for Google Drive ------------------- #\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "        \n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "        \n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(fid, destination):\n",
    "    print('cleaning already existing files ... ', end='')\n",
    "    try:\n",
    "        shutil.rmtree(destination)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "        \n",
    "    print('creating data directory ... ', end='')\n",
    "    os.mkdir(destination)\n",
    "    print('√')\n",
    "    \n",
    "    print('downloading dataset from the repository ... ', end='')\n",
    "    filename = os.path.join(destination, 'dataset.tar.xz')\n",
    "    try:\n",
    "        download_file_from_google_drive(fid, filename)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "        \n",
    "    print('extracting the dataset ... ', end='')\n",
    "    try:\n",
    "        tar = tarfile.open(filename)\n",
    "        tar.extractall(destination)\n",
    "        tar.close()\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuFTBWVU-p_g"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------- Comment This if already downloaded -------- #\n",
    "\n",
    "# destination = os.path.join(BASE_DIR, DATA_DIR)\n",
    "# download_data(DATASET_ID, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZymwGm0c6DLO"
   },
   "outputs": [],
   "source": [
    "class LowPassFilter(object): \n",
    "    def butter_lowpass(cutoff, fs, order):\n",
    "        nyq = 0.5 * fs\n",
    "        normal_cutoff = cutoff / nyq\n",
    "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "        return b, a\n",
    "\n",
    "    def apply(data, cutoff=CUT_OFF, fs=FS, order=ORDER):\n",
    "        b, a = LowPassFilter.butter_lowpass(cutoff, fs, order=order)\n",
    "        y = lfilter(b, a, data)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpkbftUw6GRp"
   },
   "outputs": [],
   "source": [
    "\n",
    "def correlation(x):\n",
    "    cor = []\n",
    "    for n in range(x.shape[0]):\n",
    "        cor.append(np.correlate(x[n, :], x[n, :])[0])\n",
    "    return np.array(cor)\n",
    "\n",
    "\n",
    "def mean_crossing_rate(x):\n",
    "    mcr = []\n",
    "    for n in range(x.shape[0]):\n",
    "        mcr.append(lb.feature.zero_crossing_rate(x[n, :] - np.mean(x[n, :]))[0, 0])\n",
    "    return np.array(mcr)\n",
    "\n",
    "\n",
    "def get_entropy(x, axis = 1):\n",
    "    x = x / np.sum(x, axis = axis, keepdims=True)\n",
    "    entropy = np.sum(entr(x), axis = axis)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def number_of_peaks(x):\n",
    "    npk = []\n",
    "    for n in range(x.shape[0]):\n",
    "        thres = (np.max(x[n, :]) / 3)\n",
    "        peaks, _ = find_peaks(x[n, :], thres)\n",
    "        npk.append(len(peaks))\n",
    "    return np.array(npk, dtype=float)\n",
    "\n",
    "\n",
    "def get_stat_features(x, axis=1, prefix=''):\n",
    "    min = np.min(x, axis = axis)\n",
    "    max = np.max(x, axis = axis)\n",
    "    std = np.std(x, axis = axis)\n",
    "    avg = np.mean(x, axis = axis)\n",
    "    var = np.var(x, axis = axis)\n",
    "    ptp = np.ptp(x, axis = axis)\n",
    "    mrc = np.max(np.diff(x, axis = axis), axis = axis)\n",
    "    arc = np.mean(np.diff(x, axis = axis), axis = axis)\n",
    "    src = np.std(np.diff(x, axis = axis), axis = axis)\n",
    "    mad = median_absolute_deviation(x, axis = axis)\n",
    "    iqr = inter_quartile_range(x, axis = axis)\n",
    "    cor = correlation(x)\n",
    "    mcr = mean_crossing_rate(x)\n",
    "    rms = np.sum(np.square(x), axis = axis)\n",
    "\n",
    "    feature_names = ['min', 'max', 'std', 'avg', 'var', \n",
    "                   'ptp', 'mrc', 'arc', 'src', 'mad', \n",
    "                   'iqr', 'cor', 'mcr', 'rms']\n",
    "    columnName = [prefix + '_' + sub for sub in feature_names]\n",
    "\n",
    "    stat_features = pd.DataFrame(np.stack((min, max, std, avg, \n",
    "                                         var, ptp, mrc, arc, \n",
    "                                         src, mad, iqr, cor, \n",
    "                                         mcr, rms), axis=1), columns=columnName)\n",
    "\n",
    "    return stat_features\n",
    " \n",
    "\n",
    "def get_freq_features(x, axis=1, fs=100, nperseg=150, prefix=''):\n",
    "    freq, psd = sp.signal.welch(x, fs, nperseg = nperseg, axis = axis)\n",
    "    mpw = np.max(psd, axis = axis)\n",
    "    ent = get_entropy(psd, axis = axis)\n",
    "    ctf = np.divide(np.sum((freq * psd), axis = axis), np.sum(psd, axis = axis))\n",
    "    mxf = np.argmax(psd, axis = axis)\n",
    "    enr = np.sum(np.square(psd), axis = axis) / nperseg\n",
    "    skw = skew(x, axis = axis)\n",
    "    kut = kurtosis(x, axis = axis)\n",
    "    npk = number_of_peaks(psd)\n",
    "\n",
    "    feature_names = ['mpw', 'ent', 'ctf', 'mxf', 'enr', 'skw', 'kut', 'npk']\n",
    "    columnName = [prefix + '_' + sub for sub in feature_names]\n",
    "\n",
    "    freq_features = pd.DataFrame(np.stack((mpw, ent, ctf, mxf, enr, skw, \n",
    "                                         kut, npk), axis=1), columns=columnName)\n",
    "\n",
    "    return freq_features\n",
    "\n",
    "\n",
    "def get_mutual_features(x, y, z, axis=1, nperseg=150, prefix=''):\n",
    "    cxy = []\n",
    "    cxz = []\n",
    "    cyz = []\n",
    "    vxy = []\n",
    "    vxz = []\n",
    "    vyz = []\n",
    "    for n in range(x.shape[0]):\n",
    "        cxy.append(np.corrcoef(x[n, :].ravel(), y[n, :].ravel())[0, 1])\n",
    "        cxz.append(np.corrcoef(x[n, :].ravel(), z[n, :].ravel())[0, 1])\n",
    "        cyz.append(np.corrcoef(y[n, :].ravel(), z[n, :].ravel())[0, 1])\n",
    "        vxy.append(np.cov(x[n, :].ravel(), y[n, :].ravel())[0, 1])\n",
    "        vxz.append(np.cov(x[n, :].ravel(), z[n, :].ravel())[0, 1])\n",
    "        vyz.append(np.cov(y[n, :].ravel(), z[n, :].ravel())[0, 1])\n",
    "    cxy = np.array(cxy)\n",
    "    cxz = np.array(cxz)\n",
    "    cyz = np.array(cyz)\n",
    "    vxy = np.array(vxy)\n",
    "    vxz = np.array(vxz)\n",
    "    vyz = np.array(vyz)\n",
    "    sma = (np.trapz(x, axis = axis) + np.trapz(x, axis = axis) + np.trapz(x, axis = axis)) / nperseg\n",
    "\n",
    "    feature_names = ['cxy', 'cxz', 'cyz', 'vxy', 'vxz', 'vyz', 'sma']\n",
    "    columnName = [prefix + '_' + sub for sub in feature_names]\n",
    "\n",
    "    mutual_features = pd.DataFrame(np.stack((cxy, cxz, cyz, vxy, vxz, vyz, sma), \n",
    "                                        axis=1), columns=columnName)\n",
    "\n",
    "    return mutual_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKvTwyDa6LGD"
   },
   "outputs": [],
   "source": [
    "def clean_dir(path):\n",
    "    print('cleaning already existing files ... ', end='')\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "    \n",
    "    print('creating ' + path + ' directory ... ', end='')\n",
    "    os.mkdir(path)\n",
    "    print('√')\n",
    "\n",
    "def generate_features():\n",
    "    features_dir = os.path.join(BASE_DIR, FEATURES_DIR)\n",
    "    clean_dir(features_dir)\n",
    "        \n",
    "    for user in USERS:\n",
    "        print('Processing data for user ' + user, end=' ')\n",
    "        \n",
    "        X = pd.DataFrame()\n",
    "        \n",
    "        for gesture in GESTURES:\n",
    "              \n",
    "            user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
    "            gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
    "\n",
    "            dataset = pd.read_csv(gesture_dir)\n",
    "            \n",
    "            flx1 = dataset['flex_1'].to_numpy().reshape(-1, WINDOW_LEN)\n",
    "            flx2 = dataset['flex_2'].to_numpy().reshape(-1, WINDOW_LEN)\n",
    "            flx3 = dataset['flex_3'].to_numpy().reshape(-1, WINDOW_LEN)\n",
    "            flx4 = dataset['flex_4'].to_numpy().reshape(-1, WINDOW_LEN)\n",
    "            flx5 = dataset['flex_5'].to_numpy().reshape(-1, WINDOW_LEN)\n",
    "            \n",
    "            accx = dataset['ACCx'].to_numpy()\n",
    "            accy = dataset['ACCy'].to_numpy()\n",
    "            accz = dataset['ACCz'].to_numpy()\n",
    "            \n",
    "            accx = LowPassFilter.apply(accx).reshape(-1, WINDOW_LEN)\n",
    "            accy = LowPassFilter.apply(accy).reshape(-1, WINDOW_LEN)\n",
    "            accz = LowPassFilter.apply(accz).reshape(-1, WINDOW_LEN)\n",
    "            \n",
    "            gyrx = dataset['GYRx'].to_numpy()\n",
    "            gyry = dataset['GYRy'].to_numpy()\n",
    "            gyrz = dataset['GYRz'].to_numpy()\n",
    "            \n",
    "            gyrx = LowPassFilter.apply(gyrx).reshape(-1, WINDOW_LEN)\n",
    "            gyry = LowPassFilter.apply(gyry).reshape(-1, WINDOW_LEN)\n",
    "            gyrz = LowPassFilter.apply(gyrz).reshape(-1, WINDOW_LEN)\n",
    "            \n",
    "            accm = np.sqrt(accx ** 2 + accy ** 2 + accz ** 2)\n",
    "            gyrm = np.sqrt(gyrx ** 2 + gyry ** 2 + gyrz ** 2)\n",
    "            \n",
    "            stat_accx = get_stat_features(accx, prefix=\"accx\")\n",
    "            stat_accy = get_stat_features(accy, prefix=\"accy\")\n",
    "            stat_accz = get_stat_features(accz, prefix=\"accz\")\n",
    "            stat_accm = get_stat_features(accm, prefix=\"accm\")\n",
    "            stat_gyrx = get_stat_features(gyrx, prefix=\"gyrx\")\n",
    "            stat_gyry = get_stat_features(gyry, prefix=\"gyry\")\n",
    "            stat_gyrz = get_stat_features(gyrz, prefix=\"gyrz\")\n",
    "            stat_gyrm = get_stat_features(gyrm, prefix=\"gyrm\")\n",
    "            stat_flx1 = get_stat_features(flx1, prefix=\"flx1\")\n",
    "            stat_flx2 = get_stat_features(flx2, prefix=\"flx2\")\n",
    "            stat_flx3 = get_stat_features(flx3, prefix=\"flx3\")\n",
    "            stat_flx4 = get_stat_features(flx4, prefix=\"flx4\")\n",
    "            stat_flx5 = get_stat_features(flx5, prefix=\"flx5\")\n",
    "\n",
    "            freq_accx = get_freq_features(accx, prefix=\"accx\")\n",
    "            freq_accy = get_freq_features(accy, prefix=\"accy\")\n",
    "            freq_accz = get_freq_features(accz, prefix=\"accz\")\n",
    "            freq_accm = get_freq_features(accm, prefix=\"accm\")\n",
    "            freq_gyrx = get_freq_features(gyrx, prefix=\"gyrx\")\n",
    "            freq_gyry = get_freq_features(gyry, prefix=\"gyry\")\n",
    "            freq_gyrz = get_freq_features(gyrz, prefix=\"gyrz\")\n",
    "            freq_gyrm = get_freq_features(gyrm, prefix=\"gyrm\")\n",
    "            # freq_flx1 = get_freq_features(flx1, prefix=\"flx1\")\n",
    "            # freq_flx2 = get_freq_features(flx2, prefix=\"flx2\")\n",
    "            # freq_flx3 = get_freq_features(flx3, prefix=\"flx3\")\n",
    "            # freq_flx4 = get_freq_features(flx4, prefix=\"flx4\")\n",
    "            # freq_flx5 = get_freq_features(flx5, prefix=\"flx5\")\n",
    "\n",
    "            mutual_acc = get_mutual_features(accx, accy, accz, prefix='acc')\n",
    "            mutual_gyr = get_mutual_features(gyrx, gyry, gyrz, prefix='gyr')\n",
    "            \n",
    "            g_idx = GESTURES.index(gesture)\n",
    "            y = pd.DataFrame(np.ones((accx.shape[0], 1)) * g_idx, columns=['y'])\n",
    "            \n",
    "            # X_gesture = pd.concat([ stat_accx, stat_accy, stat_accz, stat_accm, \n",
    "            #                         freq_accx, freq_accy, freq_accz, freq_accm, \n",
    "            #                         mutual_acc, y], axis=1  )\n",
    "\n",
    "            X_gesture = pd.concat([ stat_accx, stat_accy, stat_accz, stat_accm, \n",
    "                            stat_gyrx, stat_gyry, stat_gyrz, stat_gyrm,\n",
    "                            stat_flx1, stat_flx2, stat_flx3, stat_flx4, stat_flx5,\n",
    "                            freq_accx, freq_accy, freq_accz, freq_accm,\n",
    "                            freq_gyrx, freq_gyry, freq_gyrz, freq_gyrm,\n",
    "                            # freq_flx1, freq_flx2, freq_flx3, freq_flx4, freq_flx5,\n",
    "                            mutual_acc, mutual_gyr, y], axis=1 )\n",
    "            \n",
    "            X = pd.concat([X, X_gesture], ignore_index=True)\n",
    "            \n",
    "        feature_path = os.path.join(BASE_DIR, FEATURES_DIR, FEATURE_GROUP + user + '.joblib')\n",
    "        joblib.dump(X, feature_path)\n",
    "            \n",
    "        print('√')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aH_KZ9Vk6ZyG",
    "outputId": "36e6fc9c-e20b-4fd2-9597-549626190013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning already existing files ... √\n",
      "creating /content/drive/MyDrive/Research/Hand Gesture/GitHub/Features/ directory ... √\n",
      "Processing data for user 001 √\n",
      "Processing data for user 002 √\n",
      "Processing data for user 003 √\n",
      "Processing data for user 004 √\n",
      "Processing data for user 005 √\n",
      "Processing data for user 006 √\n",
      "Processing data for user 007 √\n",
      "Processing data for user 008 √\n",
      "Processing data for user 009 √\n",
      "Processing data for user 010 √\n",
      "Processing data for user 011 √\n",
      "Processing data for user 012 √\n",
      "Processing data for user 013 √\n",
      "Processing data for user 014 √\n",
      "Processing data for user 015 √\n",
      "Processing data for user 016 √\n",
      "Processing data for user 017 √\n",
      "Processing data for user 018 √\n",
      "Processing data for user 019 √\n",
      "Processing data for user 020 √\n",
      "Processing data for user 021 √\n",
      "Processing data for user 022 √\n",
      "Processing data for user 023 √\n",
      "Processing data for user 024 √\n",
      "Processing data for user 025 √\n"
     ]
    }
   ],
   "source": [
    "generate_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_ox63Cv-kZ0",
    "outputId": "a2ce7cb5-5c31-4249-f191-f77ac3f21420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing results for user 001... 95.62 %\n",
      "Processing results for user 002... 96.25 %\n",
      "Processing results for user 003... 100.00 %\n",
      "Processing results for user 004... 52.50 %\n",
      "Processing results for user 005... 89.38 %\n",
      "Processing results for user 006... 89.38 %\n",
      "Processing results for user 007... 98.75 %\n",
      "Processing results for user 008... 98.75 %\n",
      "Processing results for user 009... 100.00 %\n",
      "Processing results for user 010... 96.25 %\n",
      "Processing results for user 011... 100.00 %\n",
      "Processing results for user 012... 99.38 %\n",
      "Processing results for user 013... 100.00 %\n",
      "Processing results for user 014... 100.00 %\n",
      "Processing results for user 015... 93.75 %\n",
      "Processing results for user 016... 100.00 %\n",
      "Processing results for user 017... 100.00 %\n",
      "Processing results for user 018... 100.00 %\n",
      "Processing results for user 019... 100.00 %\n",
      "Processing results for user 020... 100.00 %\n",
      "Processing results for user 021... 100.00 %\n",
      "Processing results for user 022... 96.88 %\n",
      "Processing results for user 023... 98.75 %\n",
      "Processing results for user 024... 100.00 %\n",
      "Processing results for user 025... 100.00 %\n",
      "------------------------------------\n",
      "Average accuracy 96.22 +/- 9.44\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ACC = []\n",
    "logs = ''\n",
    "\n",
    "for test_user in USERS:\n",
    "    print('Processing results for user ' + test_user, end='... ')\n",
    "    \n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "\n",
    "    for user in USERS:\n",
    "        feature_path = os.path.join(BASE_DIR, FEATURES_DIR, FEATURE_GROUP + user + '.joblib')\n",
    "        features = joblib.load(feature_path)\n",
    "\n",
    "        if user == test_user:\n",
    "            test = pd.concat([test, features], ignore_index=True)\n",
    "        else:\n",
    "            train = pd.concat([train, features], ignore_index=True)\n",
    "\n",
    "    X_train = train.drop(columns=['y']).to_numpy()\n",
    "    X_test = test.drop(columns=['y']).to_numpy()\n",
    "\n",
    "    y_train = train['y'].to_numpy()\n",
    "    y_test = test['y'].to_numpy()\n",
    "\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators      = 40,\n",
    "        criterion         = 'gini',\n",
    "        max_depth         = None,\n",
    "        random_state      = 42,\n",
    "        verbose           = 0\n",
    "    )\n",
    "\n",
    "    acc = 0\n",
    "    for i in range(EPOCHS):\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = acc + accuracy_score(y_test, y_pred)\n",
    "\n",
    "    avg_acc = (acc / EPOCHS) * 100\n",
    "    print(f'%.2f %%' %(avg_acc))\n",
    "    \n",
    "    logs = logs + 'Average accuracy for user ' + str(test_user) + '... ' + str(avg_acc) + '\\n'\n",
    "\n",
    "    ACC.append(avg_acc)\n",
    "    \n",
    "AVG_ACC = np.mean(ACC)\n",
    "STD = np.std(ACC)\n",
    "print('------------------------------------')\n",
    "print(f'Average accuracy %.2f +/- %.2f' %(AVG_ACC, STD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6SpEF4uDLOs"
   },
   "outputs": [],
   "source": [
    "line = '---------------------------------------\\n'\n",
    "log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.mkdir(log_dir)\n",
    "f = open(os.path.join(log_dir, 'logs_classical_ml.txt'), 'a')\n",
    "f.write(CONFIG)\n",
    "f.write(logs)\n",
    "f.write(line)\n",
    "f.write(f'Average accuracy %.2f +/- %.2f' %(AVG_ACC, STD))\n",
    "f.write('\\n\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzOFHUq3UPqs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPklOD1zGoUkP4JyRmv1ydv",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1KAO4IMeALBgLsBqf-u20Whj5AXGzSUfC",
   "name": "Dynamic_Hand_Gestures_ML_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
