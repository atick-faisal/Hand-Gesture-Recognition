{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atick-faisal/Hand-Gesture-Recognition/blob/main/Deep-Learning-Analysis/Dynamic_Hand_Gestures_DL_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkE25oMeBzt6"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSI8PsLeyijV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import joblib\n",
        "import shutil\n",
        "import tarfile\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnTuZP2MA69B"
      },
      "outputs": [],
      "source": [
        "\n",
        "DATASET_ID      = '1p0CSRb9gax0sKqdyzOYVt-BXvZ4GtrBv'\n",
        "\n",
        "# -------------BASE DIR (MODIFY THIS TO YOUR NEED) ------------ #\n",
        "BASE_DIR        = '../Dataset/'\n",
        "# BASE_DIR     = '/content/drive/MyDrive/Research/Hand Gesture/GitHub/'\n",
        "\n",
        "DATA_DIR        = 'Sensor-Data/'\n",
        "CHANNELS_DIR    = 'Channels/'\n",
        "FEATURES_DIR    = 'Features/'\n",
        "FIGURE_DIR      = 'Figures/'\n",
        "LOG_DIR         = 'Logs/'\n",
        "\n",
        "USERS           = ['001', '002', '003', '004', '005', '006', '007', '008', '009',\n",
        "                   '010', '011', '012', '013', '014', '015', '016', '017', '018',\n",
        "                   '019', '020', '021', '022', '023', '024', '025']\n",
        "GESTURES        = ['j', 'z', 'bad', 'deaf', 'fine', 'good', 'goodbye', 'hello', 'hungry',\n",
        "                   'me', 'no', 'please', 'sorry', 'thankyou', 'yes', 'you']\n",
        "\n",
        "WINDOW_LEN      = 150\n",
        "\n",
        "# ------------- FOR THE GREATER GOOD :) ------------- #\n",
        "DATASET_LEN     = 1120\n",
        "TRAIN_LEN       = 960\n",
        "TEST_LEN        = 160\n",
        "\n",
        "TEST_USER       = '001'\n",
        "EPOCHS          = 5\n",
        "\n",
        "CHANNELS_GROUP  = 'DYNAMIC_ACC_ONLY_'\n",
        "CUT_OFF         = 3.0\n",
        "ORDER           = 4\n",
        "FS              = 100\n",
        "\n",
        "CONFIG          = \"Rolling median filter for flex, LPF for IMU, Stacked CNN, epochs 20, lr 0.001\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNEMKhZ2A69C"
      },
      "outputs": [],
      "source": [
        "\n",
        "#--------------------- Download util for Google Drive ------------------- #\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "        \n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "        \n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "\n",
        "def download_data(fid, destination):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(destination)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "        \n",
        "    print('creating data directory ... ', end='')\n",
        "    os.mkdir(destination)\n",
        "    print('√')\n",
        "    \n",
        "    print('downloading dataset from the repository ... ', end='')\n",
        "    filename = os.path.join(destination, 'dataset.tar.xz')\n",
        "    try:\n",
        "        download_file_from_google_drive(fid, filename)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "        \n",
        "    print('extracting the dataset ... ', end='')\n",
        "    try:\n",
        "        tar = tarfile.open(filename)\n",
        "        tar.extractall(destination)\n",
        "        tar.close()\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6fUrhiFA69E",
        "outputId": "9d9d84dc-1129-4f0b-c9ff-31db8007d169",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# ------- Comment This if already downloaded -------- #\n",
        "\n",
        "# destination = os.path.join(BASE_DIR, DATA_DIR)\n",
        "# download_data(DATASET_ID, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYml41AQA69E"
      },
      "outputs": [],
      "source": [
        "class LowPassFilter(object): \n",
        "    def butter_lowpass(cutoff, fs, order):\n",
        "        nyq = 0.5 * fs\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
        "        return b, a\n",
        "\n",
        "    def apply(data, cutoff=CUT_OFF, fs=FS, order=ORDER):\n",
        "        b, a = LowPassFilter.butter_lowpass(cutoff, fs, order=order)\n",
        "        y = lfilter(b, a, data)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FgIg6L0A69F"
      },
      "outputs": [],
      "source": [
        "def clean_dir(path):\n",
        "    print('cleaning already existing files ... ', end='')\n",
        "    try:\n",
        "        shutil.rmtree(path)\n",
        "        print('√')\n",
        "    except:\n",
        "        print('✕')\n",
        "    \n",
        "    print('creating ' + path + ' directory ... ', end='')\n",
        "    os.mkdir(path)\n",
        "    print('√')\n",
        "\n",
        "def extract_channels():\n",
        "    channels_dir = os.path.join(BASE_DIR, CHANNELS_DIR)\n",
        "    clean_dir(channels_dir)\n",
        "        \n",
        "    for user in USERS:\n",
        "        print('Processing data for user ' + user, end=' ')\n",
        "        \n",
        "        X = []\n",
        "        y = []\n",
        "        first_time = True\n",
        "        \n",
        "        for gesture in GESTURES:\n",
        "              \n",
        "            user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
        "            gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
        "\n",
        "            dataset = pd.read_csv(gesture_dir)\n",
        "\n",
        "            dataset['flex_1'] = dataset['flex_1'].rolling(3).median()\n",
        "            dataset['flex_2'] = dataset['flex_2'].rolling(3).median()\n",
        "            dataset['flex_3'] = dataset['flex_3'].rolling(3).median()\n",
        "            dataset['flex_4'] = dataset['flex_4'].rolling(3).median()\n",
        "            dataset['flex_5'] = dataset['flex_5'].rolling(3).median()\n",
        "\n",
        "            dataset.fillna(0, inplace=True)\n",
        "\n",
        "            # flex = ['flex_1', 'flex_2', 'flex_3', 'flex_4', 'flex_5']\n",
        "            # max_flex = dataset[flex].max(axis=1)\n",
        "            # max_flex.replace(0, 1, inplace=True)\n",
        "            # dataset[flex] = dataset[flex].divide(max_flex, axis=0)\n",
        "            \n",
        "            flx1 = dataset['flex_1'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx2 = dataset['flex_2'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx3 = dataset['flex_3'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx4 = dataset['flex_4'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            flx5 = dataset['flex_5'].to_numpy().reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            accx = dataset['ACCx'].to_numpy()\n",
        "            accy = dataset['ACCy'].to_numpy()\n",
        "            accz = dataset['ACCz'].to_numpy()\n",
        "            \n",
        "            accx = LowPassFilter.apply(accx).reshape(-1, WINDOW_LEN)\n",
        "            accy = LowPassFilter.apply(accy).reshape(-1, WINDOW_LEN)\n",
        "            accz = LowPassFilter.apply(accz).reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            gyrx = dataset['GYRx'].to_numpy()\n",
        "            gyry = dataset['GYRy'].to_numpy()\n",
        "            gyrz = dataset['GYRz'].to_numpy()\n",
        "            \n",
        "            gyrx = LowPassFilter.apply(gyrx).reshape(-1, WINDOW_LEN)\n",
        "            gyry = LowPassFilter.apply(gyry).reshape(-1, WINDOW_LEN)\n",
        "            gyrz = LowPassFilter.apply(gyrz).reshape(-1, WINDOW_LEN)\n",
        "            \n",
        "            accm = np.sqrt(accx ** 2 + accy ** 2 + accz ** 2)\n",
        "            gyrm = np.sqrt(gyrx ** 2 + gyry ** 2 + gyrz ** 2)\n",
        "            \n",
        "            g_idx = GESTURES.index(gesture)\n",
        "            labels = np.ones((accx.shape[0], 1)) * g_idx\n",
        "            \n",
        "            channels = np.stack([\n",
        "                flx1, flx2, flx3, flx4, flx5,\n",
        "                accx, accy, accz\n",
        "            ], axis=-1)\n",
        "            \n",
        "            if first_time == True:\n",
        "                X = channels\n",
        "                y = labels\n",
        "                first_time = False\n",
        "            else:\n",
        "                X = np.append(X, channels, axis=0)\n",
        "                y = np.append(y, labels, axis=0)\n",
        "            \n",
        "        \n",
        "        x_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_X.joblib')\n",
        "        y_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_y.joblib')\n",
        "        joblib.dump(X, x_path)\n",
        "        joblib.dump(y, y_path)\n",
        "        \n",
        "        print('√')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JNhoZxMA69H",
        "outputId": "b7734758-2b38-4d88-d276-abac24b1feb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# extract_channels()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_nMIkDOA69I"
      },
      "outputs": [],
      "source": [
        "def get_model(input_shape = (150, 8)):\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(0.0001)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(BatchNormalization(input_shape=input_shape))\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(GESTURES), activation='softmax'))\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=optimizer,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyDNvK-B7zTS",
        "outputId": "8f5dbe3a-c974-41d6-987d-69693fc9634c"
      },
      "outputs": [],
      "source": [
        "def get_conv_block():\n",
        "    input = Input(shape=(150, 1))\n",
        "    x = BatchNormalization()(input)\n",
        "    x = Conv1D(filters=8, kernel_size=3, activation='selu', padding='valid')(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='selu', padding='valid')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='selu', padding='valid')(x)\n",
        "    x = Conv1D(filters=16, kernel_size=3, activation='selu', padding='valid')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(50, activation='elu')(x)\n",
        "\n",
        "    return input, x\n",
        "\n",
        "def get_stacked_model(n=8):\n",
        "    inputs = []\n",
        "    CNNs = []\n",
        "\n",
        "    for i in range(n):\n",
        "        input_i, CNN_i = get_conv_block()\n",
        "        inputs.append(input_i)\n",
        "        CNNs.append(CNN_i)\n",
        "\n",
        "    x = concatenate(CNNs, axis=-1)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(100, activation='selu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    # x = Dense(20, activation='selu')(x)\n",
        "    # x = Dropout(0.5)(x)\n",
        "    output = Dense(len(GESTURES), activation='softmax')(x)\n",
        "    model = Model(inputs, output)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a_A_E2lA69J",
        "outputId": "04e2ea7b-4350-4e98-ecd4-339f32ec5674"
      },
      "outputs": [],
      "source": [
        "\n",
        "ACC = []\n",
        "logs = ''\n",
        "\n",
        "for test_user in ['004']:\n",
        "    print('Processing results for user ' + test_user, end='... \\n')\n",
        "    \n",
        "    X_train = []\n",
        "    X_test = []\n",
        "    y_train = []\n",
        "    y_test = []\n",
        "    \n",
        "    first_time_train = True\n",
        "    first_time_test = True\n",
        "\n",
        "    for user in USERS:\n",
        "        x_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_X.joblib')\n",
        "        y_path = os.path.join(BASE_DIR, CHANNELS_DIR, CHANNELS_GROUP + user + '_y.joblib')\n",
        "        X = joblib.load(x_path)\n",
        "        y = joblib.load(y_path)\n",
        "\n",
        "        if user == test_user:\n",
        "            if first_time_train == True:\n",
        "                first_time_train = False\n",
        "                X_test = X\n",
        "                y_test = y\n",
        "                \n",
        "            else:\n",
        "                X_test = np.append(X_test, X, axis=0)\n",
        "                y_test = np.append(y_test, y, axis=0)\n",
        "                \n",
        "        else:\n",
        "            if first_time_test == True:\n",
        "                first_time_test = False\n",
        "                X_train = X\n",
        "                y_train = y\n",
        "                \n",
        "            else:\n",
        "                X_train = np.append(X_train, X, axis=0)\n",
        "                y_train = np.append(y_train, y, axis=0)\n",
        "\n",
        "\n",
        "    X_train, y_train = shuffle(X_train, y_train)\n",
        "    \n",
        "    # y_train = to_categorical(y_train)\n",
        "    # y_test = to_categorical(y_test)\n",
        "\n",
        "    callback = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=30, \n",
        "        mode='min',\n",
        "        restore_best_weights=True,\n",
        "    ) \n",
        "\n",
        "    model = get_stacked_model()\n",
        "    model.fit(\n",
        "        np.split(X_train, 8, axis=-1), \n",
        "        y_train, \n",
        "        epochs=300, \n",
        "        validation_data=(np.split(X_test, 8, axis=-1), y_test),\n",
        "        batch_size=32, \n",
        "        callbacks=[callback]\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict(np.split(X_test, 8, axis=-1))\n",
        "    _, accuracy = model.evaluate(np.split(X_test, 8, axis=-1), y_test, batch_size=32)\n",
        "\n",
        "    accuracy = accuracy * 100\n",
        "    print(f'%.2f %%' %(accuracy))\n",
        "    logs = logs + 'Accuracy for user ' + str(test_user) + '... ' + str(accuracy) + '\\n'\n",
        "    ACC.append(accuracy)\n",
        "    \n",
        "AVG_ACC = np.mean(ACC)\n",
        "STD = np.std(ACC)\n",
        "print('------------------------------------')\n",
        "print(f'Average accuracy %.2f +/- %.2f' %(AVG_ACC, STD))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNsNXgnEA69K"
      },
      "outputs": [],
      "source": [
        "line = '---------------------------------------\\n'\n",
        "log_dir = os.path.join(BASE_DIR, LOG_DIR)\n",
        "if not os.path.exists(log_dir):\n",
        "    os.mkdir(log_dir)\n",
        "f = open(os.path.join(log_dir, 'logs_dl_basic_cnn.txt'), 'a')\n",
        "f.write(CONFIG)\n",
        "f.write(logs)\n",
        "f.write(line)\n",
        "f.write(f'Average accuracy %.2f +/- %.2f' %(AVG_ACC, STD))\n",
        "f.write('\\n\\n')\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugVrCD9qD1Xv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "Dynamic_Hand_Gestures_DL_v1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "toc-autonumbering": false,
    "toc-showcode": true,
    "toc-showmarkdowntxt": false
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
