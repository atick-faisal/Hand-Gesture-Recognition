{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3503e833-12b3-4666-988a-8857fa2b6ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import joblib\n",
    "import shutil\n",
    "import tarfile\n",
    "import requests\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82afa666-0f98-4d53-999c-2e5c590efc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_USER    = '003'\n",
    "\n",
    "DATASET_ID   = '1p0CSRb9gax0sKqdyzOYVt-BXvZ4GtrBv'\n",
    "\n",
    "# -------------BASE DIR (MODIFY THIS TO YOUR NEED) ------------ #\n",
    "BASE_DIR     = '../'\n",
    "# BASE_DIR     = '/content/drive/MyDrive/Research/Hand Gesture/GitHub/'\n",
    "\n",
    "DATA_DIR     = 'Sensor-Data/'\n",
    "BW_IMG_DIR   = 'BW-Spatial-Path-Images/'\n",
    "RGB_IMG_DIR  = 'RGB-Spatial-Path-Images/'\n",
    "IMG_SIZE     = (3, 3) # INCHES\n",
    "\n",
    "IMG_DIR      = 'BW-Spatial-Path-Images/'\n",
    "LOG_DIR      = 'Logs/'\n",
    "\n",
    "USERS        = ['001', '002', '003', '004', '005', '006', '007', '008', '009',\n",
    "                '010', '011', '012', '013', '014', '015', '016', '017', '018',\n",
    "                '019', '020', '021', '022', '023', '024', '025']\n",
    "\n",
    "# ------------------------------- Only Dynalic Gestures ------------------------------ #\n",
    "GESTURES     = ['j', 'z', 'bad', 'deaf', 'fine', 'good', 'goodbye', 'hello', 'hungry',\n",
    "                'me', 'no', 'please', 'sorry', 'thankyou', 'yes', 'you']\n",
    "\n",
    "PLANES       = ['VH']\n",
    "\n",
    "DT           = 0.01\n",
    "LINEWIDTH    = 7\n",
    "\n",
    "BATCH_SIZE   = 32\n",
    "IMG_LEN      = 96\n",
    "IMG_SIZE     = (IMG_LEN, IMG_LEN)\n",
    "CHANNELS     = 3\n",
    "IMG_SHAPE    = IMG_SIZE + (CHANNELS,)\n",
    "\n",
    "# ------------- FOR THE GREATER GOOD :) ------------- #\n",
    "DATASET_LEN  = 4000\n",
    "TRAIN_LEN    = 3840\n",
    "TEST_LEN     = 160\n",
    "\n",
    "EPOCHS       = 15\n",
    "LR           = 0.001\n",
    "DECAY        = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0915139-f6a5-4688-b470-d7f96ea66824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------- Download util for Google Drive ------------------- #\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "        \n",
    "    save_response_content(response, destination)    \n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "        \n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_data(fid, destination):\n",
    "    print('cleaning already existing files ... ', end='')\n",
    "    try:\n",
    "        shutil.rmtree(destination)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "        \n",
    "    print('creating data directory ... ', end='')\n",
    "    os.mkdir(destination)\n",
    "    print('√')\n",
    "    \n",
    "    print('downloading dataset from the repository ... ', end='')\n",
    "    filename = os.path.join(destination, 'dataset.tar.xz')\n",
    "    try:\n",
    "        download_file_from_google_drive(fid, filename)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "        \n",
    "    print('extracting the dataset ... ', end='')\n",
    "    try:\n",
    "        tar = tarfile.open(filename)\n",
    "        tar.extractall(destination)\n",
    "        tar.close()\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b25287-7ee0-450d-a021-387539401c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Comment This if already downloaded -------- #\n",
    "\n",
    "# destination = os.path.join(BASE_DIR, DATA_DIR)\n",
    "# download_data(DATASET_ID, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae84acc-46dc-4389-aa4b-e24bfec908e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- Spatial Path Image Generation ----------- #\n",
    "\n",
    "def clean_dir(path):\n",
    "    print('cleaning already existing files ... ', end='')\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "        print('√')\n",
    "    except:\n",
    "        print('✕')\n",
    "    \n",
    "    print('creating ' + path + ' directory ... ', end='')\n",
    "    os.mkdir(path)\n",
    "    print('√')\n",
    "    \n",
    "def get_user_acceleration_angle(gx, gy, gz, lx, ly, lz):\n",
    "\n",
    "    # Calculates angle between linear acceleration and gravity\n",
    "    # Argumnet(s):\n",
    "    #    - gx, gy, gz : Gravity vector components\n",
    "    #    - lx, ly, lz : Linear acceleration components\n",
    "\n",
    "    n = gx * lx + gy * ly + gz * lz\n",
    "    d = np.sqrt(gx ** 2 + gy ** 2 + gz ** 2) * np.sqrt(lx ** 2 + ly ** 2 + lz ** 2)\n",
    "\n",
    "    # Division by zero is not allowed\n",
    "    # Replacing all 0s with 0.000001\n",
    "    #d = np.where(d == 0, 0.000001, d)\n",
    "\n",
    "    theta = np.arccos(np.divide(n, d))\n",
    "    \n",
    "    return np.nan_to_num(theta)\n",
    "    \n",
    "# ----------- Spatial Path Vector Calculation ----------- #\n",
    "\n",
    "def get_displacement(acc):\n",
    "    v = np.zeros(acc.shape)\n",
    "    d = np.zeros(acc.shape)\n",
    "    for i in range(acc.shape[0] - 1):\n",
    "        v[i + 1] = v[i] + acc[i] * DT\n",
    "        d[i + 1] = v[i] * DT + 0.5 * acc[i] * DT * DT\n",
    "        \n",
    "    return d\n",
    "\n",
    "def write_image(x, y, path):\n",
    "    fig, ax = plt.subplots(frameon=True, figsize=(3, 3))\n",
    "    ax.axis('off')\n",
    "    plt.plot(x, y, '-k', linewidth=LINEWIDTH)\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "def generate_bw_images():\n",
    "    count = 0\n",
    "    image_dir = os.path.join(BASE_DIR, BW_IMG_DIR)\n",
    "    clean_dir(image_dir)\n",
    "    \n",
    "    for plane in PLANES:\n",
    "        print('processing spatial path images for ' + plane + ' plane ... ', end='')\n",
    "        plane_dir = os.path.join(image_dir, plane)\n",
    "        os.mkdir(plane_dir)\n",
    "        \n",
    "        for gesture in GESTURES:\n",
    "            os.mkdir(os.path.join(plane_dir, gesture))\n",
    "    \n",
    "            for user in USERS:\n",
    "                user_dir = os.path.join(BASE_DIR, DATA_DIR, user)\n",
    "                gesture_dir = os.path.join(user_dir, gesture + '.csv')\n",
    "                \n",
    "                accx = pd.read_csv(gesture_dir)['ACCx'].to_numpy()\n",
    "                accy = pd.read_csv(gesture_dir)['ACCy'].to_numpy()\n",
    "                accz = pd.read_csv(gesture_dir)['ACCz'].to_numpy()\n",
    "                \n",
    "                lacx = pd.read_csv(gesture_dir)['ACCx_body'].to_numpy()\n",
    "                lacy = pd.read_csv(gesture_dir)['ACCy_body'].to_numpy()\n",
    "                lacz = pd.read_csv(gesture_dir)['ACCz_body'].to_numpy()\n",
    "                \n",
    "                grax = accx - lacx\n",
    "                gray = accy - lacy\n",
    "                graz = accz - lacz\n",
    "                \n",
    "                theta = get_user_acceleration_angle(grax, gray, graz, lacx, lacy, lacz)\n",
    "                lm = np.sqrt(lacx ** 2 + lacy ** 2 + lacz ** 2)\n",
    "\n",
    "                accv = lm * np.cos(theta)\n",
    "                acch = lm * np.sin(theta)\n",
    "\n",
    "                v = get_displacement(accv).reshape(-1, 150)\n",
    "                h = get_displacement(acch).reshape(-1, 150)\n",
    "\n",
    "                for i in range(v.shape[0]):\n",
    "                    image_name = 'u' + user + '_g' + '{:0>2d}'.format(GESTURES.index(gesture)) + \\\n",
    "                                 '_s' + '{:0>7d}'.format(count) + '_p' + plane + '.jpg'\n",
    "                    \n",
    "                    path = os.path.join(BASE_DIR, BW_IMG_DIR, plane, gesture, image_name)\n",
    "                    write_image(h[i, :], v[i, :], path)\n",
    "                    count = count + 1\n",
    "            \n",
    "        print('√')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e227de8-b903-43a0-9a30-8628eb630b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning already existing files ... √\n",
      "creating ../BW-Spatial-Path-Images/ directory ... √\n",
      "processing spatial path images for VH plane ... √\n"
     ]
    }
   ],
   "source": [
    "# generate_bw_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abd85aa-2917-47f9-8d76-80f02203c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(plane):\n",
    "    X_train = np.zeros((TRAIN_LEN, IMG_LEN, IMG_LEN, CHANNELS))\n",
    "    X_test = np.zeros((TEST_LEN, IMG_LEN, IMG_LEN, CHANNELS))\n",
    "    y_train = np.zeros((TRAIN_LEN, 1))\n",
    "    y_test = np.zeros((TEST_LEN, 1))\n",
    "    \n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "        \n",
    "    for gesture in GESTURES:\n",
    "        print('loading data for ' + gesture + ' gesture on the ' + plane + ' plane ... ', end='')\n",
    "        path = os.path.join(BASE_DIR, IMG_DIR, plane, gesture)\n",
    "        for filename in os.listdir(path):\n",
    "            img = cv2.imread(\n",
    "                os.path.join(path, filename),\n",
    "                cv2.IMREAD_GRAYSCALE\n",
    "            )\n",
    "            resized = cv2.resize(img, IMG_SIZE)\n",
    "\n",
    "            if resized.ndim < 3:\n",
    "                resized = np.expand_dims(resized, axis=-1)\n",
    "                \n",
    "            if filename[1:4] != TEST_USER:\n",
    "                X_train[train_count, :] = resized\n",
    "                y_train[train_count, 0] = GESTURES.index(gesture)\n",
    "                train_count = train_count + 1\n",
    "            else:\n",
    "                X_test[test_count, :] = resized\n",
    "                y_test[test_count, 0] = GESTURES.index(gesture)\n",
    "                test_count = test_count + 1\n",
    "                \n",
    "        print('√')\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def load_and_save_data(plane):\n",
    "    X = np.zeros((DATASET_LEN, IMG_LEN, IMG_LEN, CHANNELS))\n",
    "    y = np.zeros((DATASET_LEN, 1))\n",
    "    \n",
    "    train_count = 0\n",
    "    test_count  = TRAIN_LEN\n",
    "        \n",
    "    for gesture in GESTURES:\n",
    "        print('loading data for ' + gesture + ' gesture on the ' + plane + ' plane ... ', end='')\n",
    "        path = os.path.join(BASE_DIR, IMG_DIR, plane, gesture)\n",
    "        for filename in os.listdir(path):\n",
    "            img = cv2.imread(os.path.join(path, filename))\n",
    "            resized = cv2.resize(img, IMG_SIZE)\n",
    "            if filename[1:4] != TEST_USER:\n",
    "                X[train_count, :] = resized\n",
    "                y[train_count, 0] = GESTURES.index(gesture)\n",
    "                train_count = train_count + 1\n",
    "            else:\n",
    "                X[test_count, :] = resized\n",
    "                y[test_count, 0] = GESTURES.index(gesture)\n",
    "                test_count = test_count + 1\n",
    "                \n",
    "        print('√')\n",
    "\n",
    "    joblib.dump(X, BASE_DIR + 'X_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
    "    joblib.dump(y, BASE_DIR + 'Y_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
    "\n",
    "def load_data_from_joblib(plane):\n",
    "    print('Loading data for ' + plane + ' plane ... ', end='')\n",
    "    X = joblib.load(BASE_DIR + 'X_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
    "    y = joblib.load(BASE_DIR + 'Y_BW_' + plane + str(IMG_SIZE) + '.joblib')\n",
    "    test_user = int(TEST_USER)\n",
    "    X_train = X[:TRAIN_LEN, :, :, :]\n",
    "    y_train = y[:TRAIN_LEN, :]\n",
    "    X_test = X[TRAIN_LEN:, :, :, :]\n",
    "    y_test = y[TRAIN_LEN:, :]\n",
    "\n",
    "    print('√')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d00fb2-4063-412b-8e1c-3d211bd5752a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
      "9412608/9406464 [==============================] - 30s 3us/step\n"
     ]
    }
   ],
   "source": [
    "preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n",
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.3),\n",
    "])\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')\n",
    "base_model.trainable = False\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "prediction_layer = tf.keras.layers.Dense(len(GESTURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfbcbdc3-c1df-4aef-9196-8948b0cf20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "    x = data_augmentation(inputs)\n",
    "    x = preprocess_input(inputs)\n",
    "    x = base_model(x, training=False)\n",
    "    x = global_average_layer(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a541e0e3-a3f6-46d3-b4b9-dcdf8eb6fd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data for j gesture on the VH plane ... √\n",
      "loading data for z gesture on the VH plane ... √\n",
      "loading data for bad gesture on the VH plane ... √\n",
      "loading data for deaf gesture on the VH plane ... √\n",
      "loading data for fine gesture on the VH plane ... √\n",
      "loading data for good gesture on the VH plane ... √\n",
      "loading data for goodbye gesture on the VH plane ... √\n",
      "loading data for hello gesture on the VH plane ... √\n",
      "loading data for hungry gesture on the VH plane ... √\n",
      "loading data for me gesture on the VH plane ... √\n",
      "loading data for no gesture on the VH plane ... √\n",
      "loading data for please gesture on the VH plane ... √\n",
      "loading data for sorry gesture on the VH plane ... √\n",
      "loading data for thankyou gesture on the VH plane ... √\n",
      "loading data for yes gesture on the VH plane ... √\n",
      "loading data for you gesture on the VH plane ... √\n",
      "Epoch 1/15\n",
      "120/120 [==============================] - 22s 150ms/step - loss: 2.4209 - accuracy: 0.2559 - val_loss: 1.2218 - val_accuracy: 0.5938\n",
      "Epoch 2/15\n",
      "120/120 [==============================] - 17s 142ms/step - loss: 1.1894 - accuracy: 0.6073 - val_loss: 0.9482 - val_accuracy: 0.6875\n",
      "Epoch 3/15\n",
      "120/120 [==============================] - 16s 134ms/step - loss: 0.9619 - accuracy: 0.6748 - val_loss: 0.8650 - val_accuracy: 0.6812\n",
      "Epoch 4/15\n",
      "120/120 [==============================] - 16s 135ms/step - loss: 0.7788 - accuracy: 0.7326 - val_loss: 0.8612 - val_accuracy: 0.7188\n",
      "Epoch 5/15\n",
      "120/120 [==============================] - 16s 132ms/step - loss: 0.6962 - accuracy: 0.7583 - val_loss: 0.7838 - val_accuracy: 0.6938\n",
      "Epoch 6/15\n",
      "120/120 [==============================] - 16s 135ms/step - loss: 0.6563 - accuracy: 0.7681 - val_loss: 0.8000 - val_accuracy: 0.6875\n",
      "Epoch 7/15\n",
      "120/120 [==============================] - 16s 131ms/step - loss: 0.6142 - accuracy: 0.7854 - val_loss: 0.7893 - val_accuracy: 0.7188\n",
      "Epoch 8/15\n",
      "120/120 [==============================] - 16s 136ms/step - loss: 0.5909 - accuracy: 0.7955 - val_loss: 0.8434 - val_accuracy: 0.6687\n",
      "Epoch 9/15\n",
      "120/120 [==============================] - 17s 138ms/step - loss: 0.5486 - accuracy: 0.8081 - val_loss: 0.8340 - val_accuracy: 0.7125\n",
      "Epoch 10/15\n",
      "120/120 [==============================] - 16s 133ms/step - loss: 0.5258 - accuracy: 0.8189 - val_loss: 0.8051 - val_accuracy: 0.7188\n",
      "Epoch 11/15\n",
      "120/120 [==============================] - 17s 139ms/step - loss: 0.5128 - accuracy: 0.8211 - val_loss: 0.7830 - val_accuracy: 0.7000\n",
      "Epoch 12/15\n",
      "120/120 [==============================] - 16s 132ms/step - loss: 0.4767 - accuracy: 0.8294 - val_loss: 0.8171 - val_accuracy: 0.6875\n",
      "Epoch 13/15\n",
      "120/120 [==============================] - 16s 132ms/step - loss: 0.4828 - accuracy: 0.8253 - val_loss: 0.8049 - val_accuracy: 0.7000\n",
      "Epoch 14/15\n",
      "120/120 [==============================] - 16s 132ms/step - loss: 0.4807 - accuracy: 0.8316 - val_loss: 0.8376 - val_accuracy: 0.6687\n",
      "Epoch 15/15\n",
      "120/120 [==============================] - 16s 133ms/step - loss: 0.4489 - accuracy: 0.8391 - val_loss: 0.9015 - val_accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "X_train, X_test, y_train, y_test = load_data('VH')\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18dd71-4731-478d-b260-6a4e79c46ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
